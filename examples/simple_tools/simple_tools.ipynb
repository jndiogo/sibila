{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10913b9-6281-4fbb-920f-f16952284d37",
   "metadata": {},
   "source": [
    "In this example we'll look at a simple way to have the model choose among tools or give a straight answer, using structured data extraction. This can be advantageous to keep tool usage independent of model provider.\n",
    "\n",
    "We'll use a Llama-3 8B model. Please make sure you have its file in the folder \"../../models\". You can use any GGUF format model, don't forget to set its filename in the name variable below, after the text \"llamacpp:\". Or you could likewise use any other remote model from any provider.\n",
    "\n",
    "Jupyter notebook and Python script versions are available in the example's folder.\n",
    "\n",
    "Let's create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac034358-b5c2-4bb9-b2a9-790876e648ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sibila import Models\n",
    "\n",
    "# load env variables like OPENAI_API_KEY from a .env file (if available)\n",
    "try: from dotenv import load_dotenv; load_dotenv()\n",
    "except: ...\n",
    "\n",
    "# delete any live model\n",
    "try: model.close(); del model\n",
    "except: pass\n",
    "\n",
    "Models.setup(\"../../models\")\n",
    "name = \"llamacpp:Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"\n",
    "# name = \"openai:gpt-4o\"\n",
    "\n",
    "model = Models.create(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224fd44-44c2-4898-baca-bee41f8b4a51",
   "metadata": {},
   "source": [
    "We'll use an enumerator to choose which tool to use and a generic field for the arguments of the tool call.\n",
    "\n",
    "A special value of NO_TOOL signals that the model is giving a straight answer and no tool is being used.\n",
    "\n",
    "It's important to explain what each tool does in the instructions text, so the model \"knows\" what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa90b2a7-5984-4453-9fe4-9810654359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import (\n",
    "    BaseModel,\n",
    "    Field\n",
    ")\n",
    "\n",
    "# which tool to use?\n",
    "AnswerType = Literal[\"NO_TOOL\", \"WEB_SEARCH\", \"CALCULATOR\", \"NEW_NOTE\"]\n",
    "\n",
    "class AnswerOrTool(BaseModel):\n",
    "    answer_type: AnswerType\n",
    "    argument: str\n",
    "\n",
    "inst = \"\"\"\\\n",
    "If user requests live information, answer_type should be WEB_SEARCH and the argument field should be the query.\n",
    "If the user requests a calculation, don't do the math, instead answer_type should be CALCULATOR and the argument field should be the math expression.\n",
    "If the user asks to create a new note, answer_type should be NEW_NOTE and the argument field should be note's subject.\n",
    "Otherwise, answer_type should be \"NO_TOOL\" and the answer should be given in the argument field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6b454-0df7-4bb6-8a85-7d1b86007fed",
   "metadata": {},
   "source": [
    "Let's now try a few queries to see how the model behaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70a8ca0-4871-45c8-a092-fb3b9bf808e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you write a simple poem?\n",
      "answer_type='NO_TOOL' argument='Here is a simple poem, with words so sweet,\\nA gentle breeze that whispers at your feet.\\nThe sun shines bright, the birds sing their song,\\nAnd all around, life is strong.'\n",
      "\n",
      "What's the current NVIDIA stock market value?\n",
      "answer_type='WEB_SEARCH' argument='NVIDIA stock market value'\n",
      "\n",
      "How much is 78*891?\n",
      "answer_type='CALCULATOR' argument='78*891'\n",
      "\n",
      "Create a new note to call Manuel to invite him to come over and visit me\n",
      "answer_type='NEW_NOTE' argument='Call Manuel to invite him to come over and visit me'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Can you write a simple poem?\",\n",
    "    \"What's the current NVIDIA stock market value?\",\n",
    "    \"How much is 78*891?\",\n",
    "    \"Create a new note to call Manuel to invite him to come over and visit me\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    res = model.extract(AnswerOrTool, q, inst=inst)\n",
    "    print(q)\n",
    "    print(res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd333da-a45c-4d4a-bd0e-24a4e1f7a8a0",
   "metadata": {},
   "source": [
    "Some of these tools should have their output feed back into the model for further queries. In this case we'd be better off using a message Thread, where the next IN message would contain the tool output and a query for the results we're looking for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
