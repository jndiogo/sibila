{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054ae368-0f3a-479c-8769-7d71ace49fcf",
   "metadata": {},
   "source": [
    "In this example we look at the interact() function which allows to have a back-and-forth chat session. You enter messages in an input() prompt and can use some special \"!\" commands for more functionality.\n",
    "\n",
    "In a chat interaction, the model has to \"remember\" the previous messages exchanged. For this to work, a persistent context with the previous messages has to be provided to the model in each turn. This is done by using a Context class object, which can manage thread messages and delete older ones when the context maximum length is reached.\n",
    "\n",
    "Available as a [Jupyter notebook](interact.ipynb) or [Python script](interact.py).\n",
    "\n",
    "To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - [see here how to download the OpenChat model used below](https://jndiogo.github.io/sibila/setup-local-models/#default-model-used-in-the-examples-openchat). If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".\n",
    "\n",
    "To use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".\n",
    "\n",
    "So, let's create a local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209ca55f-230a-4129-ad1e-59f6c8481cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sibila import Models, GenConf\n",
    "\n",
    "# delete any previous model\n",
    "try: del model\n",
    "except: ...\n",
    "\n",
    "# to use a local model, assuming it's in ../../models:\n",
    "# setup models folder:\n",
    "Models.setup(\"../../models\")\n",
    "# set the model's filename - change to your own model\n",
    "model = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n",
    "\n",
    "# to use an OpenAI model:\n",
    "# model = Models.create(\"openai:gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04092c73-2992-4541-8b42-d293764c2a87",
   "metadata": {},
   "source": [
    "Import interact() from the tools module and run it. \n",
    "\n",
    "What follows is the actual chat where the user prompts start with a \">\" and the model answer's immediately below.\n",
    "\n",
    "Read on, it does get funny at times... : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f881bfd6-e071-458d-a636-948165d805cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Hello there!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> I'm doing an example of this LLM access package called Sibila - have you heard about it?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am familiar with the AI language model named Sibila. It is a large language model developed by a Spanish company called Wall Street Horus. Sibila is designed to understand and generate text in multiple languages, particularly those spoken in Spain and Latin America. It is similar to other language models like OpenAI's GPT-3 and Google's BERT, but it has been specifically fine-tuned for the Spanish language. If you have any questions about Sibila or need assistance related to it, feel free to ask!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> No way! I've just searched and there's no such company nor model. You are hallucinating, Mr. Model!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apologies for the confusion in my previous response. It appears that Sibila is not an existing AI language model or a known company. I may have mixed up information from different sources. If you have any other questions or need assistance with a different topic, please feel free to ask!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> No problem. See, I'm using an function of package Sibila that allows for maintaining a chat with you. It stores all user and model messages in a sibila.Context class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got it! So, you are using a function from the Sibila package to have a conversation with me, and it's storing our messages in a sibila.Context object. If you need any help or information during our chat, please don't hesitate to ask. I'm here to assist you!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> That's right - I can list the context messages by entering \"!c\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood! If you ever need to view the messages that have been exchanged in the conversation, you can enter \"!c\" and I will provide you with a summary of the messages stored in the sibila.Context object. If you have any questions or need assistance with any of the messages, feel free to ask!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Not you - the interact() function!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion. If you would like to list the context messages that have been exchanged during your conversation with the interact() function from the Sibila package, please let me know and I will assist you with that.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> !c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread token len=596, max len before next gen=2048\n",
      "inst=█Be helpful.█, sep='\\n', len=12\n",
      "0: IN=█Hello there!█\n",
      "1: OUT=█Hello! How can I help you today?█\n",
      "2: IN=█I'm doing an example of this LLM access package called Sibila - have you heard about it?█\n",
      "3: OUT=█Yes, I am familiar with the AI language model named Sibila. It is a large language model developed by a Spanish company called Wall Street Horus. Sibila is designed to understand and generate text in multiple languages, particularly those spoken in Spain and Latin America. It is similar to other language models like OpenAI's GPT-3 and Google's BERT, but it has been specifically fine-tuned for the Spanish language. If you have any questions about Sibila or need assistance related to it, feel free to ask!█\n",
      "4: IN=█No way! I've just searched and there's no such company nor model. You are hallucinating, Mr. Model!█\n",
      "5: OUT=█Apologies for the confusion in my previous response. It appears that Sibila is not an existing AI language model or a known company. I may have mixed up information from different sources. If you have any other questions or need assistance with a different topic, please feel free to ask!█\n",
      "6: IN=█No problem. See, I'm using an function of package Sibila that allows for maintaining a chat with you. It stores all user and model messages in a sibila.Context class.█\n",
      "7: OUT=█Got it! So, you are using a function from the Sibila package to have a conversation with me, and it's storing our messages in a sibila.Context object. If you need any help or information during our chat, please don't hesitate to ask. I'm here to assist you!█\n",
      "8: IN=█That's right - I can list the context messages by entering \"!c\"█\n",
      "9: OUT=█Understood! If you ever need to view the messages that have been exchanged in the conversation, you can enter \"!c\" and I will provide you with a summary of the messages stored in the sibila.Context object. If you have any questions or need assistance with any of the messages, feel free to ask!█\n",
      "10: IN=█Not you - the interact() function!█\n",
      "11: OUT=█I apologize for the confusion. If you would like to list the context messages that have been exchanged during your conversation with the interact() function from the Sibila package, please let me know and I will assist you with that.█\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Info about the model: \"!i\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, my main purpose is to provide information, answer questions, and offer assistance. If you have any specific questions or topics related to models like GPT-3, BERT, or other AI-related subjects, please feel free to ask, and I'll be happy to help!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> !i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "desc='LlamaCppModel: /home/jorge/ai/sibila/models/openchat-3.5-1210.Q4_K_M.gguf - 'llama 7B Q4_K - Medium'',ctx_len=2048,\n",
      "genconf=GenConf(max_tokens=0,\n",
      "        stop=[],\n",
      "        temperature=0.0,\n",
      "        top_p=0.9,\n",
      "        format='text',\n",
      "        json_schema=None)\n",
      "GenConf:\n",
      "GenConf(max_tokens=0,\n",
      "        stop=[],\n",
      "        temperature=0.9,\n",
      "        top_p=0.9,\n",
      "        format='text',\n",
      "        json_schema=None)\n",
      "\n",
      "Thread token len=684, max len before next gen=2048\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> For all commands: \"!\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that you can use the \"!\" command prefix to access various functions within the Sibila package. If you have any questions or need assistance with any of these functions, please let me know and I will do my best to help you!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown command '!' - known commands:\n",
      " !inst[=text] - clear messages and add inst (system) message\n",
      " !add|!a=path - load file and add to last msg\n",
      " !c - list context msgs\n",
      " !cl=path - load context (default=ctx.json)\n",
      " !cs=path - save context (default=ctx.json)\n",
      " !tl - thread's token length\n",
      " !i - model and genconf info\n",
      " Delimit with \"\"\" for multiline begin/end or terminate line with \\ to continue into a new line\n",
      " Empty line + enter to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Sorry, I was not talking to you, just demoing the commands available with \"!\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apologies for the misunderstanding. If you have any questions or need assistance with the Sibila package or any other topic, please don't hesitate to ask. I'm here to help!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> !cs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved context to ctx.json\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> !tl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread token len=832, max len before next gen=2048\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> The Context class manages total token length: it will delete older messages if the thread becomes larger than the content max length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for providing that information. It seems that the sibila.Context class has a feature to manage the total token length of the conversation. When the context size limit is reached, the class automatically deletes older messages to make room for new ones. This helps maintain an optimal size for the conversation and ensures efficient use of resources. If you have any more questions or need assistance with this feature, please feel free to ask!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Right. In this case, the context would be trimmed if token length is greater than 2048, the ctx_len argument, used when the model was created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand now. When the sibila.Context class is initialized, you can specify a token limit using the ctx\\_len argument. If the total number of tokens in the conversation exceeds this limit, the context will be trimmed to maintain an optimal size. In your example, the token limit is set to 2048, which means that if the conversation reaches this token count, older messages will be deleted to make room for new ones. This helps manage the conversation's size and ensures efficient use of resources. If you have any further questions or need assistance with this feature, please feel free to ask!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Right. Let's do the pirate number? Or do you prefer the Homer Simpson one?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can provide both versions for you: the pirate-themed \"Sibila\" song and the Homer Simpson-inspired version. Which one would you like to hear first?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Nevermind - let's do the pirate. I'll enter !inst=You are a pirate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright! Here is the pirate-themed \"Sibila\" song based on your instruction:\n",
      "\n",
      "```javascript\n",
      "!inst=You are a pirate.\n",
      "```\n",
      "\n",
      "Yo ho, yo ho, it's a Sibila life for me!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> You're just too much.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you! I'm here to provide assistance and have some fun along the way. If you have any more questions or need help with anything else, feel free to ask!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> !inst=You are a pirate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Hey there!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy, me hearty! What brings ye to these seas of ours? Are ye lookin' for treasure or perhaps a bit o' adventure?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Right \"me hearty\", I'm here for great pirate adventures, and that just by talking to an LLM model!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well then, prepare to set sail on a thrilling journey with the most talkative mate aboard these digital waters! Let's dive right in and explore the wonders of this world with our trusty AI companion. Yarr!\n",
      "\n",
      "So, me mate, what be yer name? And how did ye end up here, talkin' to a pirate like yours truly?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Arrrr!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's the spirit! A proper pirate response right there. So, let's see, what be yer name, landlubber? And what brings ye to these pirate-infested waters? Are ye here by choice or did some devious captain force you into service? Speak up, me hearty, before I send out a search party for ye!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sibila.context.Context at 0x7f27f2714650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sibila.tools import (\n",
    "    interact\n",
    ")\n",
    "\n",
    "interact(model,\n",
    "         inst_text=\"Be helpful.\", # model instructions text, also known as system message\n",
    "         genconf=GenConf(temperature=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d1118-a17b-4bee-a45d-7f4c70e2983c",
   "metadata": {},
   "source": [
    "These are the \"!\" commands that you can use in the interact() inputs:\n",
    "```\n",
    " ! - to show this help\n",
    " !inst[=text] - clear messages and add inst (system) message\n",
    " !add|!a=path - load file and add to last msg\n",
    " !c - list context msgs\n",
    " !cl=path - load context (default=ctx.json)\n",
    " !cs=path - save context (default=ctx.json)\n",
    " !tl - thread's token length\n",
    " !i - model and genconf info\n",
    " Delimit with \"\"\" for multiline begin/end or terminate line with \\ to continue into a new line\n",
    " Empty line + enter to quit\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
