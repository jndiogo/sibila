{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f6614c-aa35-4a07-8b03-92ba952b7f47",
   "metadata": {},
   "source": [
    "In this example we'll look at how to do multiple parallel requests to remote models by using Python's asyncio capabilities.\n",
    "\n",
    "Generating from local llama.cpp models does not benefit from async functionality, because the local models must already be loaded in memory and can't benefit from asynchronous IO loading. When the async methods are used with a LlamaCppModel, inference will end up being made sequentially.\n",
    "\n",
    "So we'll be using a remote OpenAI model. Make sure you defined the env variable OPENAI_API_KEY with a valid token.\n",
    "\n",
    "This example is available as a Jupyter notebook or a Python script in this folder.\n",
    "\n",
    "As usual, let's start by creating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f4e441b-f72f-4fdd-b81f-e656b87b5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variables like OPENAI_API_KEY from a .env file (if available)\n",
    "try: from dotenv import load_dotenv; load_dotenv()\n",
    "except: ...\n",
    "\n",
    "import time, asyncio\n",
    "\n",
    "from sibila import Models\n",
    "\n",
    "# delete any previous model\n",
    "try: del model\n",
    "except: ...\n",
    "\n",
    "# to use a local model, assuming it's in ../../models:\n",
    "# setup models folder:\n",
    "# Models.setup(\"../../models\")\n",
    "# model = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\", ctx_len=3072)\n",
    "\n",
    "# to use an OpenAI model:\n",
    "model = Models.create(\"openai:gpt-4\")\n",
    "\n",
    "# convenience time-counting functions:\n",
    "start_time = None\n",
    "def start_secs():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "def secs(): \n",
    "    return f\"{time.time() - start_time:.1f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bbf6e-284c-4231-8872-40e0d838c275",
   "metadata": {},
   "source": [
    "We'll create two tasks that will run in parallel:\n",
    "1. Ask the model to generate 20 names\n",
    "2. Classify a phrase as spam\n",
    "\n",
    "This example is running in a Jupyter notebook, so we can directly call the function with an await. In a python script we'd use asyncio.run() instead.\n",
    "\n",
    "Note that we're using the _async suffix methods: extract_async() and classify_async(), instead of the normal functions.\n",
    "\n",
    "The first task, generate 20 names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e806c42d-a5e2-4ba7-b378-6d5450bb5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_names begin... 0.0\n",
      "...extract_names done 4.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['John Smith',\n",
       " 'Emily Johnson',\n",
       " 'Michael Brown',\n",
       " 'Jessica Williams',\n",
       " 'David Jones',\n",
       " 'Sarah Davis',\n",
       " 'Daniel Miller',\n",
       " 'Laura Wilson',\n",
       " 'James Taylor',\n",
       " 'Sophia Anderson',\n",
       " 'Christopher Thomas',\n",
       " 'Emma Moore',\n",
       " 'Joseph Jackson',\n",
       " 'Olivia Martin',\n",
       " 'Andrew White',\n",
       " 'Isabella Thompson',\n",
       " 'Matthew Harris',\n",
       " 'Ava Garcia',\n",
       " 'Ethan Martin',\n",
       " 'Mia Clark']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def extract_names():    \n",
    "    print(\"extract_names begin...\", secs())\n",
    "    \n",
    "    names = await model.extract_async(list[str],\n",
    "                                      \"Generate 20 English names with first name and surname\")\n",
    "    \n",
    "    print(\"...extract_names done\", secs())\n",
    "    \n",
    "    return names\n",
    "\n",
    "start_secs()\n",
    "await extract_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c59656-b428-4891-b5ba-7df271495ffe",
   "metadata": {},
   "source": [
    "The second task will classify a phrase as \"spam\"/\"not spam\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08916ce4-4077-43f5-81bd-e132255d4252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify_spam begin... 0.0\n",
      "...classify_spam done 1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def classify_spam():\n",
    "    print(\"classify_spam begin...\", secs())\n",
    "    \n",
    "    classification = await model.classify_async([\"spam\", \"not spam\"],\n",
    "                                                \"I am a Nigerian prince and will make you very rich!\")\n",
    "    \n",
    "    print(\"...classify_spam done\", secs())\n",
    "    \n",
    "    return classification\n",
    "\n",
    "start_secs()\n",
    "await classify_spam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4cde1-eed7-4019-bcce-3668ca5bb130",
   "metadata": {},
   "source": [
    "Let's use asyncio.as_completed(), to receive each task output, as soon as it's ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c27554d-bbe3-4a42-9ea5-44080442620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as_complete begin--- 0.0\n",
      "extract_names begin... 0.0\n",
      "classify_spam begin... 0.0\n",
      "...classify_spam done 1.0\n",
      "Result: spam\n",
      "...extract_names done 4.8\n",
      "Result: ['John Smith', 'Emily Johnson', 'Michael Brown', 'Jessica Williams', 'David Jones', 'Sarah Davis', 'Daniel Miller', 'Laura Wilson', 'James Taylor', 'Sophia Anderson', 'Christopher Thomas', 'Emma Moore', 'Joseph Jackson', 'Olivia Martin', 'Andrew White', 'Isabella Thompson', 'Matthew Harris', 'Ava Garcia', 'Ethan Martin', 'Mia Clark']\n",
      "---as_complete done 4.8\n"
     ]
    }
   ],
   "source": [
    "async def run_tasks():\n",
    "    print(\"as_complete begin---\", secs())\n",
    "    \n",
    "    tasks = [extract_names(), classify_spam()]\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        res = await task\n",
    "        print(\"Result:\", res)\n",
    "        \n",
    "    print(\"---as_complete done\", secs())\n",
    "\n",
    "start_secs()\n",
    "await run_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55b71c-8904-48b7-beee-01a05800919c",
   "metadata": {},
   "source": [
    "Follow the above begin/done print statements and the listed time in seconds, as they are printed.\n",
    "\n",
    "Both tasks were started at the same time and classify_spam() terminated first (at the 1.0s mark), because it's a shorter task that simply outputs \"spam\"/\"not spam\".\n",
    "\n",
    "On the meanwhile, the model worked on generating the 20 names that we requested with extract_names(), a longer operation which terminates later (at 4.8s).\n",
    "\n",
    "In the same manner any other tasks could be run in parallel by using the *_async() methods of the model classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
