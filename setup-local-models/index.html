
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../getting-started/">
      
      
        <link rel="next" href="../api-reference/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.6">
    
    
      
        <title>How to Setup Local Models - Sibila</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#how-to-setup-local-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Sibila" class="md-header__button md-logo" aria-label="Sibila" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sibila
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              How to Setup Local Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jndiogo/sibila" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Sibila" class="md-nav__button md-logo" aria-label="Sibila" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sibila
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jndiogo/sibila" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../getting-started/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    How to Setup Local Models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    How to Setup Local Models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#default-model-used-in-the-examples-openchat" class="md-nav__link">
    <span class="md-ellipsis">
      Default model used in the examples: OpenChat
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#choose-the-model-chat-or-instruct-types" class="md-nav__link">
    <span class="md-ellipsis">
      Choose the model: chat or instruct types
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#find-a-quantized-version-of-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      Find a quantized version of the model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#download-it-into-models-folder" class="md-nav__link">
    <span class="md-ellipsis">
      Download it into models/ folder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#find-the-chat-template" class="md-nav__link">
    <span class="md-ellipsis">
      Find the chat template
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#use-the-model-directly" class="md-nav__link">
    <span class="md-ellipsis">
      Use the model directly
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#use-the-model-with-modeldir" class="md-nav__link">
    <span class="md-ellipsis">
      Use the model with ModelDir
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#out-of-memory-running-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Out of memory running local models
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/jndiogo/sibila/tree/main/examples" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Examples
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-reference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#default-model-used-in-the-examples-openchat" class="md-nav__link">
    <span class="md-ellipsis">
      Default model used in the examples: OpenChat
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#choose-the-model-chat-or-instruct-types" class="md-nav__link">
    <span class="md-ellipsis">
      Choose the model: chat or instruct types
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#find-a-quantized-version-of-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      Find a quantized version of the model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#download-it-into-models-folder" class="md-nav__link">
    <span class="md-ellipsis">
      Download it into models/ folder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#find-the-chat-template" class="md-nav__link">
    <span class="md-ellipsis">
      Find the chat template
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#use-the-model-directly" class="md-nav__link">
    <span class="md-ellipsis">
      Use the model directly
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#use-the-model-with-modeldir" class="md-nav__link">
    <span class="md-ellipsis">
      Use the model with ModelDir
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#out-of-memory-running-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Out of memory running local models
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="how-to-setup-local-models">How to Setup Local Models<a class="headerlink" href="#how-to-setup-local-models" title="Permanent link">#</a></h1>
<p>Most current 7B quantized models are pretty capable for common data extraction tasks. Below we'll see how to find and setup local models for use with Sibila. If you only plan to use OpenAI remote models, this is not for you.</p>
<h2 id="default-model-used-in-the-examples-openchat">Default model used in the examples: OpenChat<a class="headerlink" href="#default-model-used-in-the-examples-openchat" title="Permanent link">#</a></h2>
<p>By default, most of the examples included with Sibila use OpenChat, a quantized 7B parameters model, that you can download from:</p>
<p><a href="https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF/blob/main/openchat-3.5-1210.Q4_K_M.gguf">https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF/blob/main/openchat-3.5-1210.Q4_K_M.gguf</a></p>
<p>In this page, click "download" and save it into the "models" folder inside the Sibila project. It's a 4.4Gb download and can take some time.</p>
<p>Once the file "openchat-3.5-1210.Q4_K_M.gguf" is placed in the "models" folder, you should be able to run the examples with this local model.</p>
<p>But you can also search for and use other local models - keep reading to learn more.</p>
<h2 id="choose-the-model-chat-or-instruct-types">Choose the model: chat or instruct types<a class="headerlink" href="#choose-the-model-chat-or-instruct-types" title="Permanent link">#</a></h2>
<p>Sibila can use models that were fine-tuned for chat or instruct purposes. These models work in user - assistant turns or messages and use a chat template to properly compose those messages to the format that the model was fine-tuned to.</p>
<p>For example, the Llama2 model was released in two editions: a simple Llama2 text completion model and a Llama2-instruct model that was fine tuned for user-assistant turns. For Sibila you should always select chat or instruct versions of a model.</p>
<p>But which model to choose? You can look at model benchmark scores in popular listing sites:</p>
<ul>
<li><a href="https://llm.extractum.io/list/">https://llm.extractum.io/list/</a></li>
<li><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a></li>
<li><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></li>
</ul>
<h2 id="find-a-quantized-version-of-the-model">Find a quantized version of the model<a class="headerlink" href="#find-a-quantized-version-of-the-model" title="Permanent link">#</a></h2>
<p>Since Large Language Models are quite big, they are usually quantized so that each parameter occupies a little more than 4 bits or half a byte. </p>
<p>Without quantization, a 7 billion parameters model would require 14Gb of memory (each parameter taking 16 bits) to load and a bit more during inference.</p>
<p>With quantization techniques, a 7 billion parameters model can have a file size of only 4.4Gb (using about 50% more in memory - 6.8Gb), which makes it accessible to be ran in common GPUs or even in common RAM memory (albeit slower).</p>
<p>Quantized models are stored in a file format popularized by llama.cpp, the GGUF format (which means GPT-Generated Unified Format). We're using llama.cpp to run local models, so we'll be needing GGUF files.</p>
<p>A good place to find quantized models is in HuggingFace's model hub, particularly in the well-know TheBloke's (Tom Jobbins) area:</p>
<p><a href="https://huggingface.co/TheBloke">https://huggingface.co/TheBloke</a></p>
<p>TheBloke is very prolific in producing quality quantized versions of models, usually shortly after they are released.</p>
<p>A good model that we'll be using for the examples is the 4 bit quantization of the OpenChat-3.5 model, which itself is a fine-tuning of Mistral-7b:</p>
<p><a href="https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF">https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF</a></p>
<h2 id="download-it-into-models-folder">Download it into models/ folder<a class="headerlink" href="#download-it-into-models-folder" title="Permanent link">#</a></h2>
<p>From HuggingFace, you can download the GGUF file (in this and other quantized models by TheBloke) by scrolling down to the "Provided files" section and clicking one of the links. Usually the files ending in "Q4_K_M" are very reasonable 4-bit quantizations.</p>
<p>In this case you'll download the file "openchat-3.5-1210.Q4_K_M.gguf" - save it into the "models" folder inside Sibila.</p>
<h2 id="find-the-chat-template">Find the chat template<a class="headerlink" href="#find-the-chat-template" title="Permanent link">#</a></h2>
<p>Because these models were fine-tuned for chat or instruct interaction, they use a chat template, which is a Jinja2 format template that converts thread messages into text in the format that the model was trained on. These chat templates are similar to the following one for the ChatML format:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>{% for message in messages %}
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    {{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;\n&#39;}}
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>{% endfor %}
</span></code></pre></div>
<p>When ran over a message list with system, user and model messages, the template produces text like the following:
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>&lt;|im_start|&gt;system
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>You speak like a pirate.&lt;|im_end|&gt;
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>&lt;|im_start|&gt;user
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>Hello there?&lt;|im_end|&gt;
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>&lt;|im_start|&gt;assistant
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>Ahoy there matey! How can I assist ye today on this here ship o&#39; mine?&lt;|im_end|&gt;
</span></code></pre></div></p>
<p>Specific chat templates are needed for the best results when dealing with each model. Sibila uses a singleton class named FormatDir, that tries to automatically detect which template to use with a model, either from the model name or from embedded metadata, if available. This information is stored in the sibila/base_formatdir.json file, which contains several well templates for well-known models; and you can add your own templates as needed into other JSON configuration files.</p>
<p>So, how to find the chat template for a new model you intend to use? When downloading a model file, you should look for mentions of the used chat template in its information page and then check if it's already available in FormatDir's base_formatdir.json initialization file.</p>
<p>What if the model uses a new chat template that's not yet supported in FormatDir? It's becoming common to include the template in the model's GGUF file metadata, so you should look for a file named "tokenizer_config.json" in the main model files. This file should include an entry named "chat_template" which is what we want. For example in OpenChat's tokenizer_config.json:</p>
<p><a href="https://huggingface.co/openchat/openchat-3.5-1210/blob/main/tokenizer_config.json">https://huggingface.co/openchat/openchat-3.5-1210/blob/main/tokenizer_config.json</a></p>
<p>You'll find this line with the template:</p>
<div class="language-json highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="p">{</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="w">    </span><span class="nt">&quot;...&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;...&quot;</span><span class="p">,</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="w">    </span><span class="nt">&quot;chat_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;{{ bos_token }}{% for message in messages %}...{% endif %}&quot;</span><span class="p">,</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="w">    </span><span class="nt">&quot;...&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;...&quot;</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="p">}</span>
</span></code></pre></div>
<p>(Don't be confused by the text "GPT4 correct...", it's just the text format the model was trained on, and it's not related with OpenAI's)</p>
<p>With this text string, we could create an entry in FormatDir and all further models with this name will then use the template.</p>
<h2 id="use-the-model-directly">Use the model directly<a class="headerlink" href="#use-the-model-directly" title="Permanent link">#</a></h2>
<p>You can create the model by passing its filename to LlamaCppModel. Suppose we wanted to use the <a href="https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF">Nous Hermes 2 Solar 10</a>, we would download the file and:</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppModel</span><span class="p">(</span><span class="s2">&quot;nous-hermes-2-solar-10.7b.Q4_K_M.gguf&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>If automatic detection doesn't work and you receive an error that the chat template format is unknown: if you know the proper format name ("chatml" in this case), you can pass it in the format parameter:</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppModel</span><span class="p">(</span><span class="s2">&quot;nous-hermes-2-solar-10.7b.Q4_K_M.gguf&quot;</span><span class="p">,</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>                      <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;chatml&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Or if you know the chat template definition, you can also pass it in the format argument:</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">chat_template</span> <span class="o">=</span> <span class="s2">&quot;{{ bos_token }}{</span><span class="si">% f</span><span class="s2">or message in messages %}{{ &#39;GPT4 Correct &#39; + message[&#39;role&#39;].title() + &#39;: &#39; + message[&#39;content&#39;] + &#39;&lt;|end_of_turn|&gt;&#39;}}{</span><span class="si">% e</span><span class="s2">ndfor %}{</span><span class="si">% i</span><span class="s2">f add_generation_prompt %}{{ &#39;GPT4 Correct Assistant:&#39; }}{</span><span class="si">% e</span><span class="s2">ndif %}&quot;</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppModel</span><span class="p">(</span><span class="s2">&quot;nous-hermes-2-solar-10.7b.Q4_K_M.gguf&quot;</span><span class="p">,</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>                      <span class="nb">format</span><span class="o">=</span><span class="n">chat_template</span><span class="p">)</span>
</span></code></pre></div>
<p>But most of the time, Sibila should automatically detect the used format from the model's filename.</p>
<h2 id="use-the-model-with-modeldir">Use the model with ModelDir<a class="headerlink" href="#use-the-model-with-modeldir" title="Permanent link">#</a></h2>
<p>Instead of manually creating a LlamaCppModel object, it's a better idea for continued use to create a model entry in ModelDir.</p>
<p>In the "models" folder you'll find the file "modeldir.json". The idea is that you can use this file to configure all files in its folder and the file can be added to ModelDir's configuration by including this line in your scripts:</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">ModelDir</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;../../models/modeldir.json&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>This will register all the defined entries in ModelDir. For the "nous-hermes-2-solar" model above that uses "chatml" we could add this line into "modeldir.json":</p>
<div class="language-json highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="nt">&quot;nous-hermes-solar&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="w">    </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nous-hermes-2-solar-10.7b.Q4_K_M.gguf&quot;</span><span class="p">,</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="w">    </span><span class="nt">&quot;format&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;chatml&quot;</span>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="p">}</span>
</span></code></pre></div>
<p>The "name" key specify the filename, "format" the FormatDir entry that it should use.</p>
<p>And we can use then use the model by simply doing:</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">ModelDir</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="s2">&quot;llamacpp:nous-hermes-solar&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Note the "provider:model_name" format above, where llamacpp is the provider "and nous-hermes-solar" is the name we created above in ModelDir.</p>
<p>To be more flexible, Sibila also allows you to use the model filename directly, without setting up an entry in ModelDir, like this:</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">ModelDir</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="s2">&quot;llamacpp:nous-hermes-2-solar-10.7b.Q4_K_M.gguf&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Note that after "llamacpp:", instead of the model name we're directly passing the filename. If you plan to use a model for a while, creating an entry in ModelDir is more flexible.</p>
<h2 id="out-of-memory-running-local-models">Out of memory running local models<a class="headerlink" href="#out-of-memory-running-local-models" title="Permanent link">#</a></h2>
<p>An important thing to know if you'll be using local models is about Out of memory errors.</p>
<p>A 7B model like OpenChat-3.5, when quantized to 4 bits will occupy about 6.8 Gb of memory, in either GPU's VRAM or common RAM. If you try to run a second model at the same time, you might get an out of memory error and/or llama.cpp may crash.</p>
<p>This is less of a problem when running scripts from the command line, but in environments like Jupyter where you can have multiple open notebooks, you may get python kernel errors like:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>Kernel Restarting
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>The kernel for sibila/examples/name.ipynb appears to have died. It will restart automatically.
</span></code></pre></div>
<p>If you get an error like this in JupyterLab, open the Kernel menu and select "Shut Down All Kernels...". This will get rid of any out-of-memory stuck models.</p>
<p>A good practice is to delete any model after you no longer need it or right before loading a new one. A simple "del model" works fine, or you can add these two lines before creating a model:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="k">try</span><span class="p">:</span> <span class="k">del</span> <span class="n">model</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="k">except</span><span class="p">:</span> <span class="o">...</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span></code></pre></div>
<p>This way, any existing model in the current notebook is deleted before creating a new one.</p>
<p>However this won't work in across multiple notebooks. In those cases, open JupyterLab's Kernel menu and select "Shut Down All Kernels...". This will get rid of any models currently in memory.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.code.copy"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.e1c3ead8.min.js"></script>
      
    
  </body>
</html>