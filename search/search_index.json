{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>Extract structured information from LLM models, using a common API to access remote models like GPT-4 or local GGUF quantized models with llama.cpp.</p> <ul> <li>Query structured information into Pydantic BaseModel objects or typed Python dicts.</li> <li>Use the same API for local and remote models.</li> <li>Thread-based interaction with chat/instruct fine-tuned models.</li> <li>Compare output across local/remote models with included utilities, text or CSV output.</li> <li>Model management directory: manage models and their configurations and quickly switch between models.</li> <li>Automatic chat templates: identifies and uses the right templates for each model.</li> </ul> <p>With Sibila you can extract structured data from a small local model like OpenChat-3.5:</p> <pre><code>from sibila import LlamaCppModel, OpenAIModel\nfrom pydantic import BaseModel, Field\n\nclass Info(BaseModel):\n    event_year: int\n    first_name: str\n    last_name: str\n    age_at_the_time: int\n    nationality: str\n\nopenchat = LlamaCppModel(\"models/openchat-3.5-1210.Q5_K_M.gguf\")\n\nopenchat.query_pydantic(Info,\n                       \"Just be helpful.\",\n                       \"Who was the first man in the moon?\")\n</code></pre> <p>Outputs an object of class Info, initialized with the model's output:</p> <pre><code>Info(event_year=1969, \n     first_name='Neil',\n     last_name='Armstrong',\n     age_at_the_time=38,\n     nationality='American')\n</code></pre>"},{"location":"#getting-started","title":"Getting started","text":"<p>Installation, accessing OpenAI, getting local models - how to get started.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Reference for the Sibila API.</p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#models","title":"Models","text":""},{"location":"api-reference/#sibila.LlamaCppModel","title":"LlamaCppModel","text":"<pre><code>LlamaCppModel(\n    path,\n    format=None,\n    format_search_order=[\"name\", \"meta_template\"],\n    *,\n    genconf=None,\n    tokenizer=None,\n    ctx_len=2048,\n    n_gpu_layers=-1,\n    main_gpu=0,\n    n_batch=512,\n    seed=4294967295,\n    verbose=False,\n    **llamacpp_kwargs\n)\n</code></pre> <p>Use local GGUF format models via llama.cpp engine.</p> <p>Supports grammar-constrained JSON output, possibly following a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path to the GGUF file.</p> required <code>format</code> <code>Union[str, dict, None]</code> <p>Chat template format to use with model. Leave as None for auto-detection.</p> <code>None</code> <code>format_search_order</code> <code>Optional[list[str]]</code> <p>Search order for auto-detecting format, \"name\" searches in the filename, \"meta_template\" looks in the model's metadata. Defaults to [\"name\",\"meta_template\"].</p> <code>['name', 'meta_template']</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Default generation configuration, which can be used in gen() and related. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>int</code> <p>Maximum context length to be used (shared for input and output). Defaults to 2048.</p> <code>2048</code> <code>n_gpu_layers</code> <code>int</code> <p>Number of model layers to run in a GPU. Defaults to -1 for all.</p> <code>-1</code> <code>main_gpu</code> <code>int</code> <p>Index of the GPU to use. Defaults to 0.</p> <code>0</code> <code>n_batch</code> <code>int</code> <p>Prompt processing batch size. Defaults to 512.</p> <code>512</code> <code>seed</code> <code>int</code> <p>Random number generation seed, for non zero temperature inference. Defaults to 4294967295.</p> <code>4294967295</code> <code>verbose</code> <code>bool</code> <p>Emit (very) verbose output. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If llama-cpp-python is not installed.</p> <code>ValueError</code> <p>If ctx_len is 0 or larger than the values supported by model.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def __init__(self,\n             path: str,\n\n             format: Union[str,dict,None] = None,                 \n             format_search_order: Optional[list[str]] = [\"name\",\"meta_template\"],\n\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             tokenizer: Optional[Tokenizer] = None,\n             ctx_len: int = 2048,\n\n             # important LlamaCpp-specific args\n             n_gpu_layers: int = -1,\n             main_gpu: int = 0,\n             n_batch: int = 512,\n             seed: int = 4294967295,\n             verbose: bool = False,\n\n             # other LlamaCpp-specific args\n             **llamacpp_kwargs\n             ):\n    \"\"\"\n    Args:\n        path: File path to the GGUF file.\n        format: Chat template format to use with model. Leave as None for auto-detection.\n        format_search_order: Search order for auto-detecting format, \"name\" searches in the filename, \"meta_template\" looks in the model's metadata. Defaults to [\"name\",\"meta_template\"].\n        genconf: Default generation configuration, which can be used in gen() and related. Defaults to None.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). Defaults to 2048.\n        n_gpu_layers: Number of model layers to run in a GPU. Defaults to -1 for all.\n        main_gpu: Index of the GPU to use. Defaults to 0.\n        n_batch: Prompt processing batch size. Defaults to 512.\n        seed: Random number generation seed, for non zero temperature inference. Defaults to 4294967295.\n        verbose: Emit (very) verbose output. Defaults to False.\n\n    Raises:\n        ImportError: If llama-cpp-python is not installed.\n        ValueError: If ctx_len is 0 or larger than the values supported by model.\n    \"\"\"\n\n    self._llama = None\n    self.tokenizer = None\n\n    if not has_llama_cpp:\n        raise ImportError(\"Please install llama-cpp-python by running: pip install llama-cpp-python\")\n\n    if ctx_len == 0:\n        raise ValueError(\"LlamaCppModel doesn't support ctx_len=0\")\n\n    super().__init__(True,\n                     genconf,\n                     tokenizer\n                     )\n\n    # update kwargs from important args\n    llamacpp_kwargs.update(n_ctx=ctx_len,\n                           n_batch=n_batch,\n                           n_gpu_layers=n_gpu_layers,\n                           main_gpu=main_gpu,\n                           seed=seed,\n                           verbose=verbose\n                           )\n\n    logger.debug(f\"Creating Llama with model_path='{path}', llamacpp_kwargs={llamacpp_kwargs}\")\n\n    with normalize_notebook_stdout_stderr(not verbose):\n        self._llama = Llama(model_path=path, **llamacpp_kwargs)\n\n    self._model_path = path\n\n    # correct super __init__ values\n    self._ctx_len = self._llama.n_ctx()\n\n    n_ctx_train = self._llama._model.n_ctx_train()        \n    if self.ctx_len &gt; n_ctx_train:\n        raise ValueError(f\"ctx_len ({self.ctx_len}) is greater than n_ctx_train ({n_ctx_train})\")\n\n\n    if self.tokenizer is None:\n        self.tokenizer = LlamaCppTokenizer(self._llama)\n\n    try:\n        self.init_format(format,\n                         format_search_order,\n                         {\"name\": os.path.basename(self._model_path),\n                          \"meta_template_name\": \"tokenizer.chat_template\"}\n                         )\n    except Exception as e:\n        del self.tokenizer\n        del self._llama\n        self._llama = self.tokenizer = None\n        raise e\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None, ok_length_is_error=False)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Like the gen_() method but will raise a GenError exception if an error occurs.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>ok_length_is_error</code> <code>Optional[bool]</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen(self,\n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ok_length_is_error: Optional[bool] = False\n        ) -&gt; str:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Like the gen_() method but will raise a GenError exception if an error occurs.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    out = self.gen_(thread, \n                    genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.json","title":"json","text":"<pre><code>json(\n    thread,\n    genconf=None,\n    json_schema=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>json_schema</code> <code>Union[dict, str]</code> <p>An optional JSON schema describing the dict fields that will be output. Defaults to None.</p> <code>None</code> <code>massage_schema</code> <code>Optional[bool]</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n\n         thread: Thread,\n         genconf: Optional[GenConf] = None,\n\n         json_schema: Union[dict,str] = None,\n\n         massage_schema: Optional[bool] = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        json_schema: An optional JSON schema describing the dict fields that will be output. Defaults to None.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n    out = self.json_(thread,\n                     genconf,\n\n                     json_schema,\n                     massage_schema,\n                     schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.dictype","title":"dictype","text":"<pre><code>dictype(dictype, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values,  Raises GenError if unable to get a valid dict that follows the dictype definition.</p> <p>Parameters:</p> Name Type Description Default <code>dictype</code> <code>dict</code> <p>description</p> required <code>thread</code> <code>Thread</code> <p>description</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>description. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict following the dictype definition.</p> Source code in <code>sibila/model.py</code> <pre><code>def dictype(self,\n            dictype: dict,\n            thread: Thread,\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; dict:\n    \"\"\"Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values, \n    Raises GenError if unable to get a valid dict that follows the dictype definition.\n\n    Args:\n        dictype: _description_\n        thread: _description_\n        genconf: _description_. Defaults to None.\n        schemaconf: _description_. Defaults to None.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n\n    Returns:\n        A dict following the dictype definition.\n    \"\"\"\n\n    out = self.dictype_(dictype,\n                        thread,\n                        genconf,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n             thread: Thread,\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Grammar-constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    out = self.pydantic_(cls,\n                         thread,\n                         genconf,\n                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.obj\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.gen_","title":"gen_","text":"<pre><code>gen_(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns a result dict.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_(self, \n         thread: Thread,\n         genconf: Optional[GenConf] = None,\n         ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns a result dict.\n\n    Args:\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc. \n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    thread = self._prepare_gen_in(thread, genconf)\n\n    prompt = self.text_from_thread(thread)\n\n    text,finish = self._text_gen(prompt, genconf)\n\n    out = self._prepare_gen_out(text, finish, genconf)\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.json_","title":"json_","text":"<pre><code>json_(\n    thread,\n    genconf=None,\n    json_schema=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>json_schema</code> <code>Union[dict, str]</code> <p>An optional JSON schema describing the dict fields that will be output. Defaults to None.</p> <code>None</code> <code>massage_schema</code> <code>Optional[bool]</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def json_(self,\n\n          thread: Thread,\n          genconf: Optional[GenConf] = None,\n\n          json_schema: Union[dict,str] = None,\n\n          massage_schema: Optional[bool] = True,\n          schemaconf: Optional[JSchemaConf] = None,\n          ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        json_schema: An optional JSON schema describing the dict fields that will be output. Defaults to None.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    if genconf.json_schema is not None and json_schema is not None:\n        logger.warn(\"Both json_schema and genconf.json_schema are set: using json_schema\")\n\n    if json_schema is not None and massage_schema:\n        if schemaconf is None:\n            schemaconf = JSchemaConf()\n        json_schema = json_schema_massage(json_schema, schemaconf)\n\n    out = self.gen_(thread, \n                    genconf(format=\"json\", \n                            json_schema=json_schema))\n\n    return out        \n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.dictype_","title":"dictype_","text":"<pre><code>dictype_(dictype, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values,  Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>dictype</code> <code>dict</code> <p>A dictype defining the layout of the returned dict.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc, with the output dict in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def dictype_(self,\n             dictype: dict,\n             thread: Thread,\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; GenOut:\n    \"\"\"Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values, \n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        dictype: A dictype defining the layout of the returned dict.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc, with the output dict in GenOut.dic.\n    \"\"\"\n\n    if schemaconf is None:\n        schemaconf = JSchemaConf()\n\n    params_scheme = dictype_get_json_schema(dictype, schemaconf)\n\n    if genconf is None:\n        genconf = self.genconf\n\n    out = self.json_(thread,\n                     genconf,\n                     params_scheme,\n                     massage_schema=False, # already done in dictype_get_json_schema()\n                     schemaconf=schemaconf) \n    return out\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.pydantic_","title":"pydantic_","text":"<pre><code>pydantic_(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.obj.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic_(self,\n              cls: Any, # a Pydantic BaseModel class\n              thread: Thread,\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; GenOut:\n    \"\"\"Grammar-constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.obj.\n    \"\"\"\n\n    if schemaconf is None:\n        schemaconf = JSchemaConf()\n\n    params_scheme = pydantic_get_class_parameters(cls, schemaconf)\n\n    if genconf is None:\n        genconf = self.genconf\n\n    out = self.json_(thread,\n                     genconf,\n                     params_scheme,\n                     massage_schema=False, # already done in dictype_get_json_schema()\n                     schemaconf=schemaconf)\n\n    if out.dic is not None:\n        try:\n            obj = pydantic_obj_from_json(cls, \n                                         out.dic,\n                                         schemaconf=schemaconf)\n            out.obj = obj\n\n        except TypeError as e:\n            out.res = GenRes.ERROR_JSON_SCHEMA_VAL # error validating for object (by Pydantic), but JSON is valid for its schema\n            out.text += f\"\\nJSON Schema error: {e}\"\n    else:\n        # out.res already holds the right error\n        ...\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.token_len","title":"token_len","text":"<pre><code>token_len(thread, _=None)\n</code></pre> <p>Calculate token length for a Thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>For token length calculation.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens the thread will use.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self,\n              thread: Thread,\n              _: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Calculate token length for a Thread.\n\n    Args:\n        thread: For token length calculation.\n\n    Returns:\n        Number of tokens the thread will use.\n    \"\"\"\n\n    text = self.text_from_thread(thread)\n    return self.tokenizer.token_len(text)\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = None\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.ctx_len","title":"ctx_len  <code>property</code>","text":"<pre><code>ctx_len\n</code></pre> <p>Maximum context length, shared for input + output. We assume a common in+out context where total token length must always be less than this number.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.desc","title":"desc  <code>property</code>","text":"<pre><code>desc\n</code></pre> <p>Model description.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.n_embd","title":"n_embd  <code>property</code>","text":"<pre><code>n_embd\n</code></pre> <p>Embedding size of model.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.n_params","title":"n_params  <code>property</code>","text":"<pre><code>n_params\n</code></pre> <p>Total number of model parameters.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata()\n</code></pre> <p>Returns model metadata.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def get_metadata(self):\n    \"\"\"Returns model metadata.\"\"\"\n    out = {}\n    buf = bytes(16 * 1024)\n    lmodel = self._llama.model\n    count = llama_cpp.llama_model_meta_count(lmodel)\n    for i in range(count):\n        res = llama_cpp.llama_model_meta_key_by_index(lmodel, i, buf,len(buf))\n        if res &gt;= 0:\n            key = buf[:res].decode('utf-8')\n            res = llama_cpp.llama_model_meta_val_str_by_index(lmodel, i, buf,len(buf))\n            if res &gt;= 0:\n                value = buf[:res].decode('utf-8')\n                out[key] = value\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel","title":"OpenAIModel","text":"<pre><code>OpenAIModel(\n    name,\n    unknown_name_mask=0,\n    *,\n    genconf=None,\n    tokenizer=None,\n    ctx_len=0,\n    api_key=None,\n    base_url=None,\n    openai_init_kwargs={}\n)\n</code></pre> <p>Access an OpenAI model via OpenAI API.</p> <p>Supports grammar-constrained JSON output, via the OpenAI API tools mechanism. Ref: https://platform.openai.com/docs/api-reference/chat/create</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name.</p> required <code>unknown_name_mask</code> <code>bool</code> <p>How to deal with unmatched names, a mask of: 2: Raise NameError if exact name not found. 1: Only allow versioned names - raise NameError if generic non-versioned model name used. 0 (Default): Accept any name, use first in list if necessary. </p> <code>0</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>int</code> <p>Maximum context length to be used (shared for input and output). Defaults to 0 which means model's maximum.</p> <code>0</code> <code>api_key</code> <code>Optional[str]</code> <p>OpenAI API key. Defaults to None, which will use env variable OPENAI_API_KEY.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>description. Defaults to None, which will use en variable OPENAI_BASE_URL.</p> <code>None</code> <code>openai_init_kwargs</code> <code>Optional[dict]</code> <p>Extra args for OpenAI.OpenAI() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If OpenAI API is not installed.</p> Source code in <code>sibila/openai.py</code> <pre><code>def __init__(self,\n             name: str,\n             unknown_name_mask: bool = 0,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             tokenizer: Optional[Tokenizer] = None,\n             ctx_len: int = 0,\n\n             # most important OpenAI-specific args\n             api_key: Optional[str] = None,\n             base_url: Optional[str] = None,\n\n             # OpenAI-specific args\n             openai_init_kwargs: Optional[dict] = {},\n             ):\n    \"\"\"\n    Args:\n        name: Model name.\n        unknown_name_mask: How to deal with unmatched names, a mask of:\n            2: Raise NameError if exact name not found.\n            1: Only allow versioned names - raise NameError if generic non-versioned model name used.\n            0 (Default): Accept any name, use first in list if necessary. \n        genconf: Model generation configuration. Defaults to None.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). Defaults to 0 which means model's maximum.\n        api_key: OpenAI API key. Defaults to None, which will use env variable OPENAI_API_KEY.\n        base_url: _description_. Defaults to None, which will use en variable OPENAI_BASE_URL.\n        openai_init_kwargs: Extra args for OpenAI.OpenAI() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If OpenAI API is not installed.\n    \"\"\"\n\n\n    if not has_openai:\n        raise ImportError(\"Please install openai by running: pip install openai\")\n\n    self._model_name, max_ctx_len, self._tokens_per_message, self._tokens_per_name = resolve_model(\n        name,\n        unknown_name_mask\n    )\n\n\n    super().__init__(False,\n                     genconf,\n                     tokenizer\n                     )\n\n    # only check for \"json\" text presence as json schema is requested with the tools facility.\n    self.json_format_instructors[\"json_schema\"] = self.json_format_instructors[\"json\"]\n\n    logger.debug(f\"Creating OpenAI with base_url={base_url}, openai_init_kwargs={openai_init_kwargs}\")\n\n    self._client = openai.OpenAI(api_key=api_key,\n                                 base_url=base_url,\n\n                                 **openai_init_kwargs\n                                 )\n\n\n    # correct super __init__ values\n    if self.tokenizer is None:\n        self.tokenizer = OpenAITokenizer(self._model_name)\n\n    if ctx_len == 0:\n        self._ctx_len = max_ctx_len\n    else:\n        self._ctx_len = ctx_len\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None, ok_length_is_error=False)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Like the gen_() method but will raise a GenError exception if an error occurs.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>ok_length_is_error</code> <code>Optional[bool]</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen(self,\n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ok_length_is_error: Optional[bool] = False\n        ) -&gt; str:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Like the gen_() method but will raise a GenError exception if an error occurs.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    out = self.gen_(thread, \n                    genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.json","title":"json","text":"<pre><code>json(\n    thread,\n    genconf=None,\n    json_schema=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>json_schema</code> <code>Union[dict, str]</code> <p>An optional JSON schema describing the dict fields that will be output. Defaults to None.</p> <code>None</code> <code>massage_schema</code> <code>Optional[bool]</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n\n         thread: Thread,\n         genconf: Optional[GenConf] = None,\n\n         json_schema: Union[dict,str] = None,\n\n         massage_schema: Optional[bool] = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        json_schema: An optional JSON schema describing the dict fields that will be output. Defaults to None.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n    out = self.json_(thread,\n                     genconf,\n\n                     json_schema,\n                     massage_schema,\n                     schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.dictype","title":"dictype","text":"<pre><code>dictype(dictype, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values,  Raises GenError if unable to get a valid dict that follows the dictype definition.</p> <p>Parameters:</p> Name Type Description Default <code>dictype</code> <code>dict</code> <p>description</p> required <code>thread</code> <code>Thread</code> <p>description</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>description. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict following the dictype definition.</p> Source code in <code>sibila/model.py</code> <pre><code>def dictype(self,\n            dictype: dict,\n            thread: Thread,\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; dict:\n    \"\"\"Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values, \n    Raises GenError if unable to get a valid dict that follows the dictype definition.\n\n    Args:\n        dictype: _description_\n        thread: _description_\n        genconf: _description_. Defaults to None.\n        schemaconf: _description_. Defaults to None.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n\n    Returns:\n        A dict following the dictype definition.\n    \"\"\"\n\n    out = self.dictype_(dictype,\n                        thread,\n                        genconf,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n             thread: Thread,\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Grammar-constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    out = self.pydantic_(cls,\n                         thread,\n                         genconf,\n                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.obj\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.gen_","title":"gen_","text":"<pre><code>gen_(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If method was not defined by a derived class.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def gen_(self, \n         thread: Thread,\n         genconf: Optional[GenConf] = None,\n         ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        NotImplementedError: If method was not defined by a derived class.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    token_len = self.token_len(thread, genconf)\n    if genconf.max_tokens == 0:\n        genconf = genconf(max_tokens=self.ctx_len - token_len)\n\n    elif token_len + genconf.max_tokens &gt; self.ctx_len:\n        # this is not true for all models: 1106 models have 128k max input and 4k max output (in and out ctx are not shared)\n        # so we assume the smaller max ctx length for the model\n        logger.warn(f\"Token length + genconf.max_tokens ({token_len + genconf.max_tokens}) is greater than model's context window length ({self.ctx_len})\")\n\n\n    thread = self._prepare_gen_in(thread, genconf)\n\n    fn_name = \"json_out\"\n\n    json_kwargs = {}\n    format = genconf.format\n    if format == \"json\":\n\n        if genconf.json_schema is None:\n            json_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        else:\n            # use json_schema in OpenAi's tool api\n            json_kwargs[\"tool_choice\"] = {\n                \"type\": \"function\",\n                \"function\": {\"name\": fn_name},\n            }\n\n            if isinstance(genconf.json_schema, str):\n                params = json.loads(genconf.json_schema)\n            else:\n                params = genconf.json_schema\n\n            json_kwargs[\"tools\"] = [\n                {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": fn_name,\n                        \"parameters\": params\n                    }\n                }\n            ]\n\n    logger.debug(f\"OpenAI json args: {json_kwargs}\")\n\n    msgs = thread.as_chatml()\n\n    # https://platform.openai.com/docs/api-reference/chat/create\n    response = self._client.chat.completions.create(model=self._model_name,\n                                                    messages=msgs,\n\n                                                    max_tokens=genconf.max_tokens,\n                                                    stop=genconf.stop,\n                                                    temperature=genconf.temperature,\n                                                    top_p=genconf.top_p,\n                                                    **json_kwargs,\n\n                                                    n=1\n                                                    )\n\n    logger.debug(f\"OpenAI response: {response}\")\n\n    choice = response.choices[0]\n    finish = choice.finish_reason\n    message = choice.message\n\n    if \"tool_choice\" in json_kwargs:\n\n        # json schema generation via the tools API:\n        if message.tool_calls is not None:\n            fn = message.tool_calls[0].function\n            if fn.name != fn_name:\n                logger.debug(f\"OpenAIModel: different returned JSON function name ({fn.name})\")\n\n            text = fn.arguments\n        else: # use content instead\n            text = message.content\n\n    else:\n        # text or simple json format\n        text = message.content\n\n    out = self._prepare_gen_out(text, finish, genconf)\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.json_","title":"json_","text":"<pre><code>json_(\n    thread,\n    genconf=None,\n    json_schema=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>json_schema</code> <code>Union[dict, str]</code> <p>An optional JSON schema describing the dict fields that will be output. Defaults to None.</p> <code>None</code> <code>massage_schema</code> <code>Optional[bool]</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def json_(self,\n\n          thread: Thread,\n          genconf: Optional[GenConf] = None,\n\n          json_schema: Union[dict,str] = None,\n\n          massage_schema: Optional[bool] = True,\n          schemaconf: Optional[JSchemaConf] = None,\n          ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema grammar-constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        json_schema: An optional JSON schema describing the dict fields that will be output. Defaults to None.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    if genconf.json_schema is not None and json_schema is not None:\n        logger.warn(\"Both json_schema and genconf.json_schema are set: using json_schema\")\n\n    if json_schema is not None and massage_schema:\n        if schemaconf is None:\n            schemaconf = JSchemaConf()\n        json_schema = json_schema_massage(json_schema, schemaconf)\n\n    out = self.gen_(thread, \n                    genconf(format=\"json\", \n                            json_schema=json_schema))\n\n    return out        \n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.dictype_","title":"dictype_","text":"<pre><code>dictype_(dictype, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values,  Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>dictype</code> <code>dict</code> <p>A dictype defining the layout of the returned dict.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc, with the output dict in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def dictype_(self,\n             dictype: dict,\n             thread: Thread,\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; GenOut:\n    \"\"\"Grammar-constrained generation after a dictype definition (see dictype.py), returning a Python dict of values, \n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        dictype: A dictype defining the layout of the returned dict.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc, with the output dict in GenOut.dic.\n    \"\"\"\n\n    if schemaconf is None:\n        schemaconf = JSchemaConf()\n\n    params_scheme = dictype_get_json_schema(dictype, schemaconf)\n\n    if genconf is None:\n        genconf = self.genconf\n\n    out = self.json_(thread,\n                     genconf,\n                     params_scheme,\n                     massage_schema=False, # already done in dictype_get_json_schema()\n                     schemaconf=schemaconf) \n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.pydantic_","title":"pydantic_","text":"<pre><code>pydantic_(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Grammar-constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.obj.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic_(self,\n              cls: Any, # a Pydantic BaseModel class\n              thread: Thread,\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; GenOut:\n    \"\"\"Grammar-constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"obj\" field of return dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.obj.\n    \"\"\"\n\n    if schemaconf is None:\n        schemaconf = JSchemaConf()\n\n    params_scheme = pydantic_get_class_parameters(cls, schemaconf)\n\n    if genconf is None:\n        genconf = self.genconf\n\n    out = self.json_(thread,\n                     genconf,\n                     params_scheme,\n                     massage_schema=False, # already done in dictype_get_json_schema()\n                     schemaconf=schemaconf)\n\n    if out.dic is not None:\n        try:\n            obj = pydantic_obj_from_json(cls, \n                                         out.dic,\n                                         schemaconf=schemaconf)\n            out.obj = obj\n\n        except TypeError as e:\n            out.res = GenRes.ERROR_JSON_SCHEMA_VAL # error validating for object (by Pydantic), but JSON is valid for its schema\n            out.text += f\"\\nJSON Schema error: {e}\"\n    else:\n        # out.res already holds the right error\n        ...\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.token_len","title":"token_len","text":"<pre><code>token_len(thread, genconf=None)\n</code></pre> <p>Calculate the number of tokens used by a list of messages. If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.</p> <p>From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p> <p>More info on calculating function_call (and tools?) tokens:</p> <p>https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24</p> <p>https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens the thread will use.</p> Source code in <code>sibila/openai.py</code> <pre><code>def token_len(self,\n              thread: Thread,\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Calculate the number of tokens used by a list of messages.\n    If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.\n\n    From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    More info on calculating function_call (and tools?) tokens:\n\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24\n\n    https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5\n\n    Args:\n        thread: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens the thread will use.\n    \"\"\"\n\n    # name = self._model_name\n\n    num_tokens = 0\n    for index in range(-1, len(thread)): # -1 for system message\n        message = thread.msg_as_chatml(index)\n        num_tokens += self._tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(self.tokenizer.encode(value))\n            # if key == \"name\":\n            #     num_tokens += self._tokens_per_name\n\n    num_tokens += 3  # every reply is primed with &lt;|start|&gt;assistant&lt;|message|&gt;\n\n    if genconf is not None and genconf.json_schema is not None:\n        if isinstance(genconf.json_schema, str):\n            js_str = genconf.json_schema\n        else:\n            js_str = json.dumps(genconf.json_schema)\n        # this is an upper bound, as empirically tested with the api.\n        num_tokens += self.tokenizer.token_len(js_str)                \n\n    return num_tokens\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = OpenAITokenizer(_model_name)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.ctx_len","title":"ctx_len  <code>property</code>","text":"<pre><code>ctx_len\n</code></pre> <p>Maximum context length, shared for input + output. We assume a common in+out context where total token length must always be less than this number.</p>"},{"location":"api-reference/#sibila.OpenAIModel.desc","title":"desc  <code>property</code>","text":"<pre><code>desc\n</code></pre> <p>Model description.</p>"},{"location":"api-reference/#messages-threads-context","title":"Messages, Threads, Context","text":""},{"location":"api-reference/#sibila.MsgKind","title":"MsgKind","text":"<p>Enumeration for kinds of messages in a Thread.</p>"},{"location":"api-reference/#sibila.MsgKind.IN","title":"IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN = 0\n</code></pre> <p>Input message, from user.</p>"},{"location":"api-reference/#sibila.MsgKind.OUT","title":"OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUT = 1\n</code></pre> <p>Model output message.</p>"},{"location":"api-reference/#sibila.MsgKind.INST","title":"INST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INST = 2\n</code></pre> <p>Initial instructions for model.</p>"},{"location":"api-reference/#sibila.Thread","title":"Thread","text":"<pre><code>Thread(t=None, inst='', join_sep='\\n')\n</code></pre> <p>A sequence of messages alternating between IN (\"user\" role) and OUT (\"assistant\" role).</p> <p>Stores a special initial INST information (known as \"system\" role in ChatML) providing instructions to the model. Some models don't use system instructions - in those cases it's prepended to first IN message.</p> <p>Messages are kept in a strict IN,OUT,IN,OUT,... order. To enforce this, if two IN messages are added, the second just appends to the text of the first.</p> <p>Examples:</p> <p>Creation with message list</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")],\n...             inst=\"Be helpful.\")\n&gt;&gt;&gt; print(th)\ninst=\u2588Be helpful.\u2588, sep='\\n', len=2\n0: IN=\u2588Hello model!\u2588\n1: OUT=\u2588Hello there human!\u2588\n</code></pre> <p>Adding messages</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread(inst=\"Be helpful.\")\n&gt;&gt;&gt; th.add(MsgKind.IN, \"Can you teach me how to cook?\")\n&gt;&gt;&gt; th.add_IN(\"I mean really cook as a chef?\") # gets appended\n&gt;&gt;&gt; print(th)\ninst=\u2588Be helpful.\u2588, sep='\\n', len=1\n0: IN=\u2588Can you teach me how to cook?\\nI mean really cook as a chef?\u2588\n</code></pre> <p>Another way to add a message</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread(inst=\"Be informative.\")\n&gt;&gt;&gt; th.add_IN(\"Tell me about kangaroos, please?\")\n&gt;&gt;&gt; th += \"They are so impressive.\" # appends text to last message\n&gt;&gt;&gt; print(th)\ninst=\u2588Be informative.\u2588, sep='\\n', len=1\n0: IN=\u2588Tell me about kangaroos, please?\\nThey are so impressive.\u2588\n</code></pre> <p>As a ChatML message list</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")], \n...             inst=\"Be helpful.\")\n&gt;&gt;&gt; th.as_chatml()\n[{'role': 'system', 'content': 'Be helpful.'},\n {'role': 'user', 'content': 'Hello model!'},\n {'role': 'assistant', 'content': 'Hello there human!'}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Self, list, str, dict, tuple]]</code> <p>Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.</p> <code>None</code> <code>join_sep</code> <code>Optional[str]</code> <p>Separator used when message text needs to be joined. Defaults to \"\\n\".</p> <code>'\\n'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>On invalid args passed.</p> Source code in <code>sibila/thread.py</code> <pre><code>def __init__(self,\n             t: Optional[Union[Self,list,str,dict,tuple]] = None,\n             inst: Optional[str] = '',\n             join_sep: Optional[str] = \"\\n\"):\n    \"\"\"\n    Examples:\n        Creation with message list\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")],\n        ...             inst=\"Be helpful.\")\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be helpful.\u2588, sep='\\\\n', len=2\n        0: IN=\u2588Hello model!\u2588\n        1: OUT=\u2588Hello there human!\u2588\n\n        Adding messages\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread(inst=\"Be helpful.\")\n        &gt;&gt;&gt; th.add(MsgKind.IN, \"Can you teach me how to cook?\")\n        &gt;&gt;&gt; th.add_IN(\"I mean really cook as a chef?\") # gets appended\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be helpful.\u2588, sep='\\\\n', len=1\n        0: IN=\u2588Can you teach me how to cook?\\\\nI mean really cook as a chef?\u2588\n\n        Another way to add a message\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread(inst=\"Be informative.\")\n        &gt;&gt;&gt; th.add_IN(\"Tell me about kangaroos, please?\")\n        &gt;&gt;&gt; th += \"They are so impressive.\" # appends text to last message\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be informative.\u2588, sep='\\\\n', len=1\n        0: IN=\u2588Tell me about kangaroos, please?\\\\nThey are so impressive.\u2588\n\n        As a ChatML message list\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")], \n        ...             inst=\"Be helpful.\")\n        &gt;&gt;&gt; th.as_chatml()\n        [{'role': 'system', 'content': 'Be helpful.'},\n         {'role': 'user', 'content': 'Hello model!'},\n         {'role': 'assistant', 'content': 'Hello there human!'}]\n\n    Args:\n        t: Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.\n        join_sep: Separator used when message text needs to be joined. Defaults to \"\\\\n\".\n\n    Raises:\n        TypeError: On invalid args passed.\n    \"\"\"\n\n    if isinstance(t, Thread):\n        self._msgs = t._msgs.copy()\n        self.inst = t.inst\n        self.join_sep = t.join_sep\n    else:\n        self._msgs = []\n        self.inst = inst\n        self.join_sep = join_sep\n\n        if t is not None:\n            self.concat(t)\n</code></pre>"},{"location":"api-reference/#sibila.Thread.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Delete all messages and clear inst.</p> Source code in <code>sibila/thread.py</code> <pre><code>def clear(self):\n    \"\"\"Delete all messages and clear inst.\"\"\"\n    self.inst = \"\"\n    self._msgs = []\n</code></pre>"},{"location":"api-reference/#sibila.Thread.last_kind","title":"last_kind  <code>property</code>","text":"<pre><code>last_kind\n</code></pre> <p>Get kind of last message in thread .</p> <p>Returns:</p> Type Description <code>MsgKind</code> <p>Kind of last message or MsgKind.IN if empty.</p>"},{"location":"api-reference/#sibila.Thread.inst","title":"inst  <code>instance-attribute</code>","text":"<pre><code>inst = inst\n</code></pre>"},{"location":"api-reference/#sibila.Thread.add","title":"add","text":"<pre><code>add(t, text=None)\n</code></pre> <p>Add a message to Thread by parsing a mix of types.</p> <p>Accepts these combinations of arg types:</p> <pre><code>t:MsgKind, text:str\n\nt:str, text=None -&gt; uses last thread message's MsgKind\n\n(MsgKind, text)\n\n{\"kind\": \"...\", text: \"...\"}\n\n{\"role\": \"...\", content: \"...\"} - ChatML format\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[str, list, dict]]</code> <p>One of the accepted types listed above.</p> required <code>text</code> <code>Optional[str]</code> <p>Message text if first type is MsgKind. Defaults to None.</p> <code>None</code> Source code in <code>sibila/thread.py</code> <pre><code>def add(self, \n        t: Optional[Union[str,list,dict]],\n        text: Optional[str] = None):\n    \"\"\"Add a message to Thread by parsing a mix of types.\n\n    Accepts these combinations of arg types:\n\n        t:MsgKind, text:str\n\n        t:str, text=None -&gt; uses last thread message's MsgKind\n\n        (MsgKind, text)\n\n        {\"kind\": \"...\", text: \"...\"}\n\n        {\"role\": \"...\", content: \"...\"} - ChatML format\n\n    Args:\n        t: One of the accepted types listed above.\n        text: Message text if first type is MsgKind. Defaults to None.\n    \"\"\"\n\n    kind, text = self._parse_msg(t, text)\n\n    if kind == MsgKind.INST:\n        self.inst = self.join_text(self.inst, text)\n    else:\n        if kind == self.last_kind and len(self._msgs):\n            self._msgs[-1] = self.join_text(self._msgs[-1], text)\n        else:\n            self._msgs.append(text) # in new kind\n</code></pre>"},{"location":"api-reference/#sibila.Thread.addx","title":"addx","text":"<pre><code>addx(path=None, text=None, kind=None)\n</code></pre> <p>Add message with text from a supplied arg or loaded from a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>If given, text is loaded from an UTF-8 file in this path. Defaults to None.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>If given, text is added. Defaults to None.</p> <code>None</code> <code>kind</code> <code>Optional[MsgKind]</code> <p>MsgKind of message. If not given or the same as last thread message, it's appended to it. Defaults to None.</p> <code>None</code> Source code in <code>sibila/thread.py</code> <pre><code>def addx(self, \n         path: Optional[str] = None, \n         text: Optional[str] = None,\n         kind: Optional[MsgKind] = None):\n    \"\"\"Add message with text from a supplied arg or loaded from a path.\n\n    Args:\n        path: If given, text is loaded from an UTF-8 file in this path. Defaults to None.\n        text: If given, text is added. Defaults to None.\n        kind: MsgKind of message. If not given or the same as last thread message, it's appended to it. Defaults to None.\n    \"\"\"\n\n    assert (path is not None) ^ (text is not None), \"Only one of path or text\"\n\n    if path is not None:\n        with open(path, 'r', encoding=\"utf-8\") as f:\n            text = f.read()\n\n    if kind is None: # use last message role, so that it gets appended\n        kind = self.last_kind\n\n    self.add(kind, text)\n</code></pre>"},{"location":"api-reference/#sibila.Thread.get_text","title":"get_text","text":"<pre><code>get_text(index)\n</code></pre> <p>Return text for message at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Message index. Use -1 to get inst value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Message text at index.</p> Source code in <code>sibila/thread.py</code> <pre><code>def get_text(self,\n             index: int) -&gt; str:\n    \"\"\"Return text for message at index.\n\n    Args:\n        index: Message index. Use -1 to get inst value.\n\n    Returns:\n        Message text at index.\n    \"\"\"        \n    if index == -1:\n        return self.inst\n    else:\n        return self._msgs[index]\n</code></pre>"},{"location":"api-reference/#sibila.Thread.set_text","title":"set_text","text":"<pre><code>set_text(index, text)\n</code></pre> <p>Set text for message at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Message index. Use -1 to set inst value.</p> required <code>text</code> <code>str</code> <p>Text to replace in message at index.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def set_text(self,\n             index: int,\n             text: str):        \n    \"\"\"Set text for message at index.\n\n    Args:\n        index: Message index. Use -1 to set inst value.\n        text: Text to replace in message at index.\n    \"\"\"\n    if index == -1:\n        self.inst = text\n    else:\n        self._msgs[index] = text\n</code></pre>"},{"location":"api-reference/#sibila.Thread.concat","title":"concat","text":"<pre><code>concat(t)\n</code></pre> <p>Concatenate a Thread or list of messages to the current Thread.</p> <p>Take care that the other list starts with an IN message, therefore,  if last message in self is also an IN kind, their text will be joined as in add().</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Self, list, str, dict, tuple]]</code> <p>A Thread or a list of messages. Otherwise a single message as in add().</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If bad arg types provided.</p> Source code in <code>sibila/thread.py</code> <pre><code>def concat(self,\n           t: Optional[Union[Self,list,str,dict,tuple]]):\n    \"\"\"Concatenate a Thread or list of messages to the current Thread.\n\n    Take care that the other list starts with an IN message, therefore, \n    if last message in self is also an IN kind, their text will be joined as in add().\n\n    Args:\n        t: A Thread or a list of messages. Otherwise a single message as in add().\n\n    Raises:\n        TypeError: If bad arg types provided.\n    \"\"\"\n    if isinstance(t, Thread):\n        for msg in t:\n            self.add(msg)\n        self.inst = self.join_text(self.inst, t.inst)\n\n    elif isinstance(t, list): # message list\n        for msg in t:\n            self.add(msg)\n\n    elif isinstance(t, str) or isinstance(t, dict) or isinstance(t, tuple): # single message\n        self.add(t)\n\n    else:\n        raise TypeError(\"Arg t must be: Thread --or-- list[messages] --or-- an str, tuple or dict single message.\")\n</code></pre>"},{"location":"api-reference/#sibila.Thread.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Load this Thread from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to load.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def load(self,\n         path: str):\n    \"\"\"Load this Thread from a JSON file.\n\n    Args:\n        path: Path of file to load.\n    \"\"\"\n\n    with open(path, 'r', encoding='utf-8') as f:\n        js = f.read()\n    state = json.loads(js)\n\n    self._msgs = state[\"_msgs\"]\n    self.inst = state[\"inst\"]\n    self.join_sep = state[\"join_sep\"]\n</code></pre>"},{"location":"api-reference/#sibila.Thread.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Serialize this Thread to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to save into.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def save(self,\n         path: str):\n    \"\"\"Serialize this Thread to JSON.\n\n    Args:\n        path: Path of file to save into.\n    \"\"\"\n\n    state = {\"_msgs\": self._msgs,\n             \"inst\": self.inst,\n             \"join_sep\": self.join_sep\n             }\n\n    json_str = json.dumps(state, indent=2, default=vars)\n\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(json_str)\n</code></pre>"},{"location":"api-reference/#sibila.Thread.msg_as_chatml","title":"msg_as_chatml","text":"<pre><code>msg_as_chatml(index)\n</code></pre> <p>Returns message in a ChatML dict.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the message to return.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A ChatML dict with \"role\" and \"content\" keys.</p> Source code in <code>sibila/thread.py</code> <pre><code>def msg_as_chatml(self,\n                  index: int) -&gt; dict:\n    \"\"\"Returns message in a ChatML dict.\n\n    Args:\n        index: Index of the message to return.\n\n    Returns:\n        A ChatML dict with \"role\" and \"content\" keys.\n    \"\"\"\n    kind = Thread._kind_from_pos(index)\n    role = MsgKind.chatml_role_from_kind(kind)\n    text = self._msgs[index] if index &gt;= 0 else self.inst\n    return {\"role\": role, \"content\": text}\n</code></pre>"},{"location":"api-reference/#sibila.Thread.as_chatml","title":"as_chatml","text":"<pre><code>as_chatml()\n</code></pre> <p>Returns Thread as a list of ChatML messages.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of ChatML dict elements with \"role\" and \"content\" keys.</p> Source code in <code>sibila/thread.py</code> <pre><code>def as_chatml(self) -&gt; list[dict]:\n    \"\"\"Returns Thread as a list of ChatML messages.\n\n    Returns:\n        A list of ChatML dict elements with \"role\" and \"content\" keys.\n    \"\"\"\n    msgs = []\n\n    for index,msg in enumerate(self._msgs):\n        if index == 0 and self.inst:\n            msgs.append(self.msg_as_chatml(-1))\n        msgs.append(self.msg_as_chatml(index))\n\n    return msgs\n</code></pre>"},{"location":"api-reference/#sibila.Thread.has_text_lower","title":"has_text_lower","text":"<pre><code>has_text_lower(text_lower)\n</code></pre> <p>Can the lowercase text be found in one of the messages?</p> <p>Parameters:</p> Name Type Description Default <code>text_lower</code> <code>str</code> <p>The lowercase text to search for in messages.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if text was found.</p> Source code in <code>sibila/thread.py</code> <pre><code>def has_text_lower(self,\n                   text_lower: str) -&gt; bool:\n    \"\"\"Can the lowercase text be found in one of the messages?\n\n    Args:\n        text_lower: The lowercase text to search for in messages.\n\n    Returns:\n        True if text was found.\n    \"\"\"\n    for msg in self._msgs:\n        if text_lower in msg.lower():\n            return True\n\n    return False        \n</code></pre>"},{"location":"api-reference/#sibila.Trim","title":"Trim","text":"<p>Flags for Thread trimming.</p>"},{"location":"api-reference/#sibila.Trim.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre> <p>No trimming.</p>"},{"location":"api-reference/#sibila.Trim.INST","title":"INST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INST = 1\n</code></pre> <p>Can remove INST message.</p>"},{"location":"api-reference/#sibila.Trim.IN","title":"IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN = 2\n</code></pre> <p>Can remove IN messages.</p>"},{"location":"api-reference/#sibila.Trim.OUT","title":"OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUT = 4\n</code></pre> <p>Can remove OUT messages.</p>"},{"location":"api-reference/#sibila.Trim.KEEP_FIRST_IN","title":"KEEP_FIRST_IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEEP_FIRST_IN = 1024\n</code></pre> <p>If trimming IN messages, never remove first one.</p>"},{"location":"api-reference/#sibila.Trim.KEEP_FIRST_OUT","title":"KEEP_FIRST_OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEEP_FIRST_OUT = 2048\n</code></pre> <p>If trimming OUT messages, never remove first one.</p>"},{"location":"api-reference/#sibila.Context","title":"Context","text":"<pre><code>Context(\n    t=None,\n    max_token_len=None,\n    pinned_inst_text=\"\",\n    join_sep=\"\\n\",\n)\n</code></pre> <p>A class based on Thread that manages total token length to be kept below a certain value. Also supports a persistent instructions text.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Thread, list, str, dict]]</code> <p>Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.</p> <code>None</code> <code>max_token_len</code> <code>Optional[int]</code> <p>Maximum token count to use when trimming. Defaults to None, which will use max model context length.</p> <code>None</code> <code>pinned_inst_text</code> <code>Optional[str]</code> <p>Pinned inst text which survives clear(). Defaults to \"\".</p> <code>''</code> <code>join_sep</code> <code>Optional[str]</code> <p>Separator used when message text needs to be joined. Defaults to \"\\n\".</p> <code>'\\n'</code> Source code in <code>sibila/context.py</code> <pre><code>def __init__(self,\n\n             t: Optional[Union[Thread,list,str,dict]] = None,\n\n             max_token_len: Optional[int] = None,\n\n             pinned_inst_text: Optional[str] = \"\",\n\n             join_sep: Optional[str] = \"\\n\"):\n    \"\"\"\n    Args:\n        t: Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.\n        max_token_len: Maximum token count to use when trimming. Defaults to None, which will use max model context length.\n        pinned_inst_text: Pinned inst text which survives clear(). Defaults to \"\".\n        join_sep: Separator used when message text needs to be joined. Defaults to \"\\\\n\".\n    \"\"\"\n\n    super().__init__(t,\n                     inst=pinned_inst_text,\n                     join_sep=join_sep)\n\n    self.max_token_len = max_token_len\n\n    self.pinned_inst_text = pinned_inst_text\n</code></pre>"},{"location":"api-reference/#sibila.Context.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Delete all messages but reset inst to a pinned text if any.</p> Source code in <code>sibila/context.py</code> <pre><code>def clear(self):\n    \"\"\"Delete all messages but reset inst to a pinned text if any.\"\"\"\n    super().clear()        \n    if self.pinned_inst_text is not None:\n        self.inst = self.pinned_inst_text\n</code></pre>"},{"location":"api-reference/#sibila.Context.trim","title":"trim","text":"<pre><code>trim(\n    trim_flags, model, *, max_token_len=None, genconf=None\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>trim_flags</code> <code>Trim</code> <p>description</p> required <code>model</code> <code>Model</code> <p>description</p> required <code>max_token_len</code> <code>Optional[int]</code> <p>description. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to trim anything.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if any trimmin occurred.</p> Source code in <code>sibila/context.py</code> <pre><code>def trim(self,\n         trim_flags: Trim,\n         model: Model,\n         *,\n         max_token_len: Optional[int] = None,\n         genconf: Optional[GenConf] = None\n         ) -&gt; bool:\n    \"\"\"_summary_\n\n    Args:\n        trim_flags: _description_\n        model: _description_\n        max_token_len: _description_. Defaults to None.\n        genconf: _description_. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to trim anything.\n\n    Returns:\n        True if any trimmin occurred.\n    \"\"\"\n\n\n\n\n\n    if genconf is None:\n        genconf = model.genconf            \n\n    if max_token_len is None:\n        max_token_len = self.max_token_len\n\n    if max_token_len is None:\n        max_token_len = model.ctx_len\n\n    # assert max_token_len &lt; model.ctx_len, f\"max_token_len ({max_token_len}) must be &lt; model's context size ({model.ctx_len}) - genconf.max_new_tokens\"\n\n\n    if trim_flags == Trim.NONE: # no trimming\n        return False\n\n    thread = self.clone()\n\n    any_trim = False\n\n    while True:\n\n        curr_len = model.token_len(thread, genconf)\n\n        if curr_len &lt;= max_token_len:\n            break\n\n        logger.debug(f\"len={curr_len} / max={max_token_len}\")\n\n        if self.inst and trim_flags &amp; Trim.INST:\n            self.inst = ''\n            any_trim = True\n            logger.debug(f\"Cutting INST {self.inst[:80]} (...)\")\n            continue\n\n        # cut first possible nessage, starting from oldest (first are older)\n        trimmed = False\n        in_index = out_index = 0\n\n        for index,m in enumerate(thread):\n            kind,text = m\n\n            if kind == MsgKind.IN:\n                if trim_flags &amp; Trim.IN:\n                    if not (trim_flags &amp; Trim.KEEP_FIRST_IN and in_index == 0):\n                        del thread[index]\n                        trimmed = True\n                        logger.debug(f\"Cutting IN {text[:80]} (...)\")\n                        break\n                in_index += 1\n\n            elif kind == MsgKind.OUT:\n                if trim_flags &amp; Trim.OUT:                        \n                    if not (trim_flags &amp; Trim.KEEP_FIRST_OUT and out_index == 0):\n                        del thread[index]\n                        trimmed = True\n                        logger.debug(f\"Cutting OUT {text[:80]} (...)\")\n                        break\n                out_index += 1\n\n        if not trimmed:\n            # all thread messages were cycled but not a single could be cut, so size remains the same\n            # arriving here we did all we could for trim_flags but could not remove any more\n            raise RuntimeError(\"Unable to trim anything out of thread\")\n        else:\n            any_trim = True\n\n    # while end\n\n\n    if any_trim:\n        self._msgs = thread._msgs\n\n    return any_trim\n</code></pre>"},{"location":"api-reference/#generation-configs","title":"Generation Configs","text":""},{"location":"api-reference/#sibila.GenConf","title":"GenConf  <code>dataclass</code>","text":"<p>Model generation configuration, used in Model.gen() and variants.</p>"},{"location":"api-reference/#sibila.GenConf.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens = 0\n</code></pre> <p>Max generated token length. 0 means all available up to output context size (which equals: model.ctx_len - in_prompt_len)</p>"},{"location":"api-reference/#sibila.GenConf.stop","title":"stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop = field(default_factory=list)\n</code></pre> <p>List of generation stop text sequences</p>"},{"location":"api-reference/#sibila.GenConf.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature = 0.0\n</code></pre> <p>Generation temperature. Use 0 to always pick the most probable output, without random sampling. Other positive values will produce random outputs.</p>"},{"location":"api-reference/#sibila.GenConf.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p = 0.9\n</code></pre> <p>Nucleus sampling top_p value. Only applies if temperature &gt; 0.</p>"},{"location":"api-reference/#sibila.GenConf.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format = 'text'\n</code></pre> <p>Output format: \"text\" or \"json\". For JSON output, text is validaded as in json.loads(). Thread msgs must explicitely request JSON output or a warning will be emited if string json not present (this is automatically done in Model.json() and related calls).</p>"},{"location":"api-reference/#sibila.GenConf.json_schema","title":"json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>json_schema = None\n</code></pre> <p>A JSON schema to validate the JSON output. Thread msgs must list the JSON schema and request its use; must also set the format to \"json\".</p>"},{"location":"api-reference/#sibila.GenConf.asdict","title":"asdict","text":"<pre><code>asdict()\n</code></pre> <p>Return GenConf as a dict.</p> Source code in <code>sibila/gen.py</code> <pre><code>def asdict(self):\n    \"\"\"Return GenConf as a dict.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api-reference/#sibila.GenConf.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a copy of this configuration.</p> Source code in <code>sibila/gen.py</code> <pre><code>def clone(self):\n    \"\"\"Return a copy of this configuration.\"\"\"\n    return copy(self)\n</code></pre>"},{"location":"api-reference/#sibila.GenConf.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Return a copy of the current GenConf updated with values in kwargs. Doesn't modify object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>update settings of the same names in the returned copy.</p> <code>{}</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key does not exist.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A copy of the current object with kwargs values updated. Doesn't modify object.</p> Source code in <code>sibila/gen.py</code> <pre><code>def __call__(self,\n             **kwargs: Any) -&gt; Self:\n    \"\"\"Return a copy of the current GenConf updated with values in kwargs. Doesn't modify object.\n\n    Args:\n        **kwargs: update settings of the same names in the returned copy.\n\n    Raises:\n        KeyError: If key does not exist.\n\n    Returns:\n        A copy of the current object with kwargs values updated. Doesn't modify object.\n    \"\"\"\n\n    ret = copy(self)\n\n    for k,v in kwargs.items():\n        if not hasattr(ret, k):\n            raise KeyError(f\"No such key '{k}'\")\n        setattr(ret, k,v)\n\n    return ret\n</code></pre>"},{"location":"api-reference/#sibila.GenConf.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>sibila/gen.py</code> <pre><code>def __str__(self) -&gt; str:\n    return pformat(self)\n</code></pre>"},{"location":"api-reference/#sibila.JSchemaConf","title":"JSchemaConf  <code>dataclass</code>","text":"<p>Configuration for JSON schema massaging and validation.</p>"},{"location":"api-reference/#sibila.JSchemaConf.resolve_refs","title":"resolve_refs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resolve_refs = True\n</code></pre> <p>Set for $ref references to be resolved and replaced with actual definition.</p>"},{"location":"api-reference/#sibila.JSchemaConf.collapse_single_combines","title":"collapse_single_combines  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>collapse_single_combines = True\n</code></pre> <p>Any single-valued \"oneOf\"/\"anyOf\" is replaced with the actual value.</p>"},{"location":"api-reference/#sibila.JSchemaConf.description_from_title","title":"description_from_title  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description_from_title = False\n</code></pre> <p>If a value doesn't have a description entry, make one from its title or name.</p>"},{"location":"api-reference/#sibila.JSchemaConf.force_all_required","title":"force_all_required  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_all_required = False\n</code></pre> <p>Force all entries in an object to be required (except removed defaults if remove_with_default=True).</p>"},{"location":"api-reference/#sibila.JSchemaConf.remove_with_default","title":"remove_with_default  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>remove_with_default = False\n</code></pre> <p>Delete any values that have a \"default\" annotation.</p>"},{"location":"api-reference/#sibila.JSchemaConf.default_to_last","title":"default_to_last  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_to_last = True\n</code></pre> <p>Move any default value entry into the last position of properties dict.</p>"},{"location":"api-reference/#sibila.JSchemaConf.additional_allowed_root_keys","title":"additional_allowed_root_keys  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_allowed_root_keys = field(default_factory=list)\n</code></pre> <p>By default only \"properties\", \"type\", \"required\", \"additionalProperties\", \"allOf\", \"anyOf\", \"oneOf\", \"not\" are allowed in root - add to this setting for aditional ones.</p>"},{"location":"api-reference/#sibila.JSchemaConf.pydantic_strict_validation","title":"pydantic_strict_validation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pydantic_strict_validation = None\n</code></pre> <p>Validate JSON values in a strict manner or not. None means validate individually per each value in the obj. (for example in pydantic with: Field(strict=True)).</p>"},{"location":"api-reference/#generation-results-and-errors","title":"Generation Results and Errors","text":""},{"location":"api-reference/#sibila.GenRes","title":"GenRes","text":"<p>Model generation result.</p>"},{"location":"api-reference/#sibila.GenRes.OK_STOP","title":"OK_STOP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OK_STOP = 1\n</code></pre> <p>Generation complete without errors.</p>"},{"location":"api-reference/#sibila.GenRes.OK_LENGTH","title":"OK_LENGTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OK_LENGTH = 0\n</code></pre> <p>Generation stopped due to reaching max_tokens.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_JSON","title":"ERROR_JSON  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON = -1\n</code></pre> <p>Invalid JSON: this is often due to the model returning OK_LENGTH (finished due to max_tokens reached), which cuts off the JSON text.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_JSON_SCHEMA_VAL","title":"ERROR_JSON_SCHEMA_VAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON_SCHEMA_VAL = -2\n</code></pre> <p>Failed JSON schema validation.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_JSON_SCHEMA_ERROR","title":"ERROR_JSON_SCHEMA_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON_SCHEMA_ERROR = -2\n</code></pre> <p>JSON schema itself is not valid.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_MODEL","title":"ERROR_MODEL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_MODEL = -3\n</code></pre> <p>Other model internal error.</p>"},{"location":"api-reference/#sibila.GenRes.from_finish_reason","title":"from_finish_reason  <code>staticmethod</code>","text":"<pre><code>from_finish_reason(finish)\n</code></pre> <p>Convert a ChatCompletion finish result into a GenRes.</p> <p>Parameters:</p> Name Type Description Default <code>finish</code> <code>str</code> <p>ChatCompletion finish result.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A GenRes result.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef from_finish_reason(finish: str) -&gt; Self:\n    \"\"\"Convert a ChatCompletion finish result into a GenRes.\n\n    Args:\n        finish: ChatCompletion finish result.\n\n    Returns:\n        A GenRes result.\n    \"\"\"\n    if finish == 'stop':\n        return GenRes.OK_STOP\n    elif finish == 'length':\n        return GenRes.OK_LENGTH\n    elif finish == '!json':\n        return GenRes.ERROR_JSON\n    elif finish == '!json_schema_val':\n        return GenRes.ERROR_JSON_SCHEMA_VAL\n    elif finish == '!json_schema_error':\n        return GenRes.ERROR_JSON_SCHEMA_ERROR\n    else:\n        return GenRes.ERROR_MODEL\n</code></pre>"},{"location":"api-reference/#sibila.GenRes.as_text","title":"as_text  <code>staticmethod</code>","text":"<pre><code>as_text(res)\n</code></pre> <p>Returns a friendlier description of the result.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Self</code> <p>Model output result.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown GenRes.</p> <p>Returns:</p> Type Description <code>str</code> <p>A friendlier description of the GenRes.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef as_text(res: Self) -&gt; str:\n    \"\"\"Returns a friendlier description of the result.\n\n    Args:\n        res: Model output result.\n\n    Raises:\n        ValueError: If unknown GenRes.\n\n    Returns:\n        A friendlier description of the GenRes.\n    \"\"\"\n\n    if res == GenRes.OK_STOP:\n        return \"Stop\"\n    elif res == GenRes.OK_LENGTH:\n        return \"Length (output cut)\"\n    elif res == GenRes.ERROR_JSON:\n        return \"JSON decoding error\"\n\n    elif res == GenRes.ERROR_JSON_SCHEMA_VAL:\n        return \"JSON SCHEMA validation error\"\n    elif res == GenRes.ERROR_JSON_SCHEMA_ERROR:\n        return \"Error in JSON SCHEMA\"\n\n    elif res == GenRes.ERROR_MODEL:\n        return \"Model internal error\"\n    else:\n        raise ValueError(\"Bad/unknow GenRes\")\n</code></pre>"},{"location":"api-reference/#sibila.GenError","title":"GenError","text":"<pre><code>GenError(out)\n</code></pre> <p>Model generation exception.</p> <p>An error has happened during model generation.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>GenOut</code> <p>Model output</p> required Source code in <code>sibila/gen.py</code> <pre><code>def __init__(self, \n             out: GenOut):\n    \"\"\"An error has happened during model generation.\n\n    Args:\n        out: Model output\n    \"\"\"\n\n    assert out.res != GenRes.OK_STOP, \"OK_STOP is not an error\"      \n\n    super().__init__()\n\n    self.res = out.res\n    self.text = out.text\n    self.dic = out.dic\n    self.obj = out.obj\n</code></pre>"},{"location":"api-reference/#sibila.GenError.raise_if_error","title":"raise_if_error  <code>staticmethod</code>","text":"<pre><code>raise_if_error(out, ok_length_is_error)\n</code></pre> <p>Raise an exception if the model returned an error</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>GenOut</code> <p>Model returned info.</p> required <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error?</p> required <p>Raises:</p> Type Description <code>GenError</code> <p>If an error was returned by model.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef raise_if_error(out: GenOut,\n                   ok_length_is_error: bool):\n    \"\"\"Raise an exception if the model returned an error\n\n    Args:\n        out: Model returned info.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error?\n\n    Raises:\n        GenError: If an error was returned by model.\n    \"\"\"\n\n    if out.res != GenRes.OK_STOP:\n        if out.res == GenRes.OK_LENGTH and not ok_length_is_error:\n            return # OK_LENGTH to not be considered an error\n\n        raise GenError(out)\n</code></pre>"},{"location":"api-reference/#sibila.GenOut","title":"GenOut  <code>dataclass</code>","text":"<p>Model output, returned by gen_(), json_() and other Model calls that don't raise exceptions.</p>"},{"location":"api-reference/#sibila.GenOut.res","title":"res  <code>instance-attribute</code>","text":"<pre><code>res\n</code></pre> <p>Result of model generation.</p>"},{"location":"api-reference/#sibila.GenOut.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text\n</code></pre> <p>Text generated by model.</p>"},{"location":"api-reference/#sibila.GenOut.dic","title":"dic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dic = None\n</code></pre> <p>Python dictionary output by the structured calls like json_() or dictype_().</p>"},{"location":"api-reference/#sibila.GenOut.obj","title":"obj  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>obj = None\n</code></pre> <p>Pydantic BaseModel object initialized from class definition in pydantic_().</p>"},{"location":"api-reference/#sibila.GenOut.asdict","title":"asdict","text":"<pre><code>asdict()\n</code></pre> <p>Return GenOut as a dict.</p> Source code in <code>sibila/gen.py</code> <pre><code>def asdict(self):\n    \"\"\"Return GenOut as a dict.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api-reference/#sibila.GenOut.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>sibila/gen.py</code> <pre><code>def __str__(self):\n    out = f\"Error={self.res.as_text(self.res)} text=\u2588{self.text}\u2588\"\n    if self.dic is not None:\n        out += f\" dic={self.dic}\"\n    if self.obj is not None:\n        out += f\" obj={self.obj}\"\n    return out\n</code></pre>"},{"location":"api-reference/#directories","title":"Directories","text":""},{"location":"api-reference/#sibila.ModelDir","title":"ModelDir","text":"<p>Model directory that unifies (and simplify) model access and configuration.</p> <p>Useful to create models from resource names like \"llamacpp:openchat\" or \"openai:gpt-4\".  This makes it simple to change a model, store model settings, to compare model outputs, etc.</p> <p>User can add new entries from script or with JSON filenames, via the add() call. New directory entries with the same name are merged into existing ones for each added config.</p> <p>Uses file base_modeldir.json in this script's directory for the initial defaults,  which the user can augment by calling add() with own config files or directly adding model config with add_model().</p> <p>These env variables are checked and used during initialization:</p> <pre><code>SIBILA_MODEL_DIR_CONF: path of a JSON configuration file to add().\n\nSIBILA_MODEL_SEARCH_PATH: ';'-delimited list of folders where to find models.\n</code></pre> <p>An example of a model directory JSON config file:</p> <pre><code>{\n    # \"llamacpp\" is a provider, you can then create models with names \n    # like \"provider:model_name\", for ex: \"llamacpp:openchat\"\n    \"llamacpp\": { \n\n        \"default\": {\n            # Place here default args for all llamacpp: models. \n            # Each entry below can then override as needed.\n        },\n\n        \"openchat\": { # this is model definition\n            \"name\": \"openchat-3.5-1210.Q4_K_M.gguf\",\n            \"format\": \"openchat\" # FormatDir's format used by this model\n                                 # (formats are chat templates)\n        },\n\n        \"phi2\": {\n            \"name\": \"phi-2.Q5_K_M.gguf\", # model filename\n            \"format\": \"phi2\"\n        },\n\n        \"oc\": \"openchat\" \n        # this is an alias: \"oc\" forwards to the \"openchat\" entry\n    },\n\n    # The \"openai\" provider. A model can be created with name: \"openai:gpt-4\"\n    \"openai\": { \n\n        \"default\": {}, # default settings for all openai models\n\n        \"gpt-3.5\": {\n            \"name\": \"gpt-3.5-turbo-1106\" # OpenAI's  model name\n        },\n\n        \"gpt-4\": {\n            \"name\": \"gpt-4-1106-preview\"\n        },\n    },\n\n    # Entry \"alias\" is not a provider but a way to have simpler alias names.\n    # For example you can use \"alias:develop\" or even simpler, just \"develop\".\n    \"alias\": { \n        \"develop\": \"llamacpp:openchat\",\n        \"production\": \"openai:gpt-3.5\"\n    }\n\n}\n</code></pre>"},{"location":"api-reference/#sibila.ModelDir.add","title":"add  <code>classmethod</code>","text":"<pre><code>add(conf_path=None, conf=None)\n</code></pre> <p>Add a JSON file or configuration dict to the model directory. When adding a JSON file, its folder is also added to the search path for model files. ~/ can be used in paths for current account's home directory.</p> <p>Parameters:</p> Name Type Description Default <code>conf_path</code> <code>Optional[str]</code> <p>Path to a JSON file with directory configuration. See class doc for format. Defaults to None.</p> <code>None</code> <code>conf</code> <code>Optional[dict]</code> <p>A dict with configuration as if loaded from JSON by json.loads(). Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>Only one of conf_path or conf can be given.</p> Source code in <code>sibila/modeldir.py</code> <pre><code>@classmethod\ndef add(cls,\n        conf_path: Optional[str] = None,\n        conf: Optional[dict] = None,\n        ):\n    \"\"\"Add a JSON file or configuration dict to the model directory.\n    When adding a JSON file, its folder is also added to the search path for model files.\n    ~/ can be used in paths for current account's home directory.\n\n    Args:\n        conf_path: Path to a JSON file with directory configuration. See class __doc__ for format. Defaults to None.\n        conf: A dict with configuration as if loaded from JSON by json.loads(). Defaults to None.\n\n    Raises:\n        TypeError: Only one of conf_path or conf can be given.\n    \"\"\"\n\n    if not ((conf_path is not None) ^ (conf is not None)):\n        raise TypeError(\"Only one of conf_path or conf can be given\")\n\n    cls.ensure()\n\n    # conf directory loading\n    if conf is not None:\n        cls.dir.update(conf)\n    else:\n        merge_dir_json(conf_path, cls.dir, cls.search_path)\n\n    cls._sanity_check()\n</code></pre>"},{"location":"api-reference/#sibila.ModelDir.add_model","title":"add_model  <code>classmethod</code>","text":"<pre><code>add_model(res_name, conf_or_link)\n</code></pre> <p>Add configuration or model alias at given res_name.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>A name in the form \"provider:model_name\", for example \"openai:gtp-4\".</p> required <code>conf_or_link</code> <code>Union[dict, str]</code> <p>A configuration dict or an alias name (to an existing model).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown provider.</p> Source code in <code>sibila/modeldir.py</code> <pre><code>@classmethod\ndef add_model(cls,\n              res_name: str,\n              conf_or_link: Union[dict,str]):\n\n    \"\"\"Add configuration or model alias at given res_name.\n\n    Args:\n        res_name: A name in the form \"provider:model_name\", for example \"openai:gtp-4\".\n        conf_or_link: A configuration dict or an alias name (to an existing model).\n\n    Raises:\n        ValueError: If unknown provider.\n    \"\"\"\n\n    cls.ensure()\n\n    provider,_ = provider_name_from_urn(res_name)\n    if provider not in cls.ALL_PROVIDER_NAMES:\n        raise ValueError(f\"Unknown provider '{provider}' in '{res_name}'\")\n\n    cls.dir[provider] = conf_or_link\n\n    cls._sanity_check()\n</code></pre>"},{"location":"api-reference/#sibila.ModelDir.add_search_path","title":"add_search_path  <code>classmethod</code>","text":"<pre><code>add_search_path(path)\n</code></pre> <p>Prepends new paths to model search path.</p> <p>During initialization env variable SIBILA_MODEL_SEARCH_PATH is searched for ';'-delimited paths. ~/ can be used in paths for current account's home directory.        </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, list[str]]</code> <p>A path or list of paths to add to model search path.</p> required Source code in <code>sibila/modeldir.py</code> <pre><code>@classmethod\ndef add_search_path(cls,\n                    path: Union[str,list[str]]):\n    \"\"\"Prepends new paths to model search path.\n\n    During initialization env variable SIBILA_MODEL_SEARCH_PATH is searched for ';'-delimited paths.\n    ~/ can be used in paths for current account's home directory.        \n\n    Args:\n        path: A path or list of paths to add to model search path.\n    \"\"\"\n\n    cls.ensure()\n\n    prepend_path(cls.search_path, path)\n</code></pre>"},{"location":"api-reference/#sibila.ModelDir.set_genconf","title":"set_genconf  <code>classmethod</code>","text":"<pre><code>set_genconf(genconf)\n</code></pre> <p>Set the GenConf to use as default for model creation.</p> <p>Parameters:</p> Name Type Description Default <code>genconf</code> <code>GenConf</code> <p>Model generation configuration.</p> required Source code in <code>sibila/modeldir.py</code> <pre><code>@classmethod\ndef set_genconf(cls,\n                genconf: GenConf):\n    \"\"\"Set the GenConf to use as default for model creation.\n\n    Args:\n        genconf: Model generation configuration.\n    \"\"\"\n    cls.genconf = genconf\n</code></pre>"},{"location":"api-reference/#sibila.ModelDir.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(res_name, genconf=None, ctx_len=None, **over_args)\n</code></pre> <p>Create a model after an entry in the model directory.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Optional model generation configuration. Overrides set_genconf() value and any directory defaults. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used. Overrides directory defaults. Defaults to None.</p> <code>None</code> <code>over_args</code> <code>Union[Any]</code> <p>Model-specific creation args, which will override default args set in model directory.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>the initialized model.</p> Source code in <code>sibila/modeldir.py</code> <pre><code>@classmethod\ndef create(cls,\n           res_name: str,\n\n           # common to all providers\n           genconf: Optional[GenConf] = None,\n           ctx_len: Optional[int] = None,\n\n           # model-specific overriding:\n           **over_args: Union[Any]) -&gt; Model:\n    \"\"\"Create a model after an entry in the model directory.\n\n    Args:\n        res_name: Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".\n        genconf: Optional model generation configuration. Overrides set_genconf() value and any directory defaults. Defaults to None.\n        ctx_len: Maximum context length to be used. Overrides directory defaults. Defaults to None.\n        over_args: Model-specific creation args, which will override default args set in model directory.\n\n    Returns:\n        Model: the initialized model.\n    \"\"\"\n\n    cls.ensure()        \n\n    # resolve \"alias:name\" res names, or \"name\": \"link_name\" links\n    provider,name = cls.resolve_urn(res_name)\n    # arriving here, prov as a non-link dict entry\n    logger.debug(f\"Resolved '{res_name}' to '{provider}','{name}'\")\n\n    prov = cls.dir[provider]\n\n    args = (prov.get(\"default\")).copy() or {}\n    prov_conf = cls.PROVIDER_CONF[provider]    \n\n    if name in prov:\n        model_args = prov[name]\n\n        # default(if any) &lt;- model_args &lt;- over_args\n        args = (prov.get(\"default\")).copy() or {}\n        args.update(model_args)        \n        args.update(over_args)\n\n    else:                \n        if \"name_passthrough\" in prov_conf[\"flags\"]:\n            model_args = {\n                \"name\": name                \n            }\n        else:\n            raise ValueError(f\"Model '{name}' not found in provider '{provider}'\")\n\n        args.update(model_args)\n        args.update(over_args)\n\n    # override genconf, ctx_len\n    if genconf is None:\n        genconf = cls.genconf\n    if genconf is not None:\n        args[\"genconf\"] = genconf\n\n    if ctx_len is not None:\n        args[\"ctx_len\"] = ctx_len\n\n    logger.debug(f\"Creating model '{provider}:{name}' with resolved args: {args}\")\n\n    if provider == \"llamacpp\":\n\n        # resolve filename -&gt; path\n        path = cls._locate_file(args[\"name\"])\n        if path is None:\n            raise FileNotFoundError(f\"File not found in '{res_name}' while looking for file '{args['name']}'. Make sure you initialized ModelDir with a path to this file's folder\")\n\n        logger.debug(f\"Resolved '{args['name']}' to '{path}'\")\n\n        del args[\"name\"]\n        args[\"path\"] = path\n\n        from .llamacpp import LlamaCppModel\n\n        model = LlamaCppModel(**args)\n\n\n    elif provider == \"openai\":\n\n        from .openai import OpenAIModel\n\n        model = OpenAIModel(**args)\n\n    \"\"\"            \n    elif provider == \"hf\":\n        from .hf import HFModel\n\n        model = HFModel(**args)\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"api-reference/#sibila.ModelDir.clear","title":"clear  <code>classmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear the model directory.</p> Source code in <code>sibila/modeldir.py</code> <pre><code>@classmethod\ndef clear(cls):\n    \"\"\"Clear the model directory.\"\"\"\n    cls.dir = None\n    cls.search_path = []\n</code></pre>"},{"location":"api-reference/#sibila.FormatDir","title":"FormatDir","text":"<p>A singleton to store chat templates for the fine-tuned models used in Sibila.</p> <p>Detects chat templates from model name/filename or uses from metadata if possible.</p> <p>This directory can be setup from a JSON file or by calling add().</p> <p>Any new directory entries with the same name replace previous ones on each new call.</p> <p>Initializes from file base_formatdir.json in this module's directory.</p> <p>This env variable is checked during initialization to load from a file:</p> <pre><code>SIBILA_FORMAT_CONF: path of a JSON configuration file to add().\n</code></pre> <p>An example of a format directory JSON config file:</p> <pre><code>{\n    \"chatml\": {\n        # template is a Jinja2 template for this model\n        \"template\": \"{% for message in messages %}...\"\n    },\n\n    \"openchat\": {\n        \"match\": \"openchat.3\", # a regexp to match model name or filename\n        \"template\": \"{{ bos_token }}...\"\n    },    \n\n    \"phi2\": {\n        \"match\": \"phi-2\",\n        \"template\": \"...\"\n    },\n\n    \"phi\": \"phi2\",\n    # this is an alias \"phi\" -&gt; \"phi2\"\n}\n</code></pre> <p>Jinja2 templates receive a standard ChatML messages list (created from a Thread) and must deal with the following:</p> <ul> <li> <p>In models that don't use a system message, template must take care of prepending it to first user message.</p> </li> <li> <p>The add_generation_prompt template variable is always set as True.</p> </li> </ul>"},{"location":"api-reference/#sibila.FormatDir.add","title":"add  <code>classmethod</code>","text":"<pre><code>add(conf_path=None, conf=None)\n</code></pre> <p>Add a JSON file or configuration dict to the format directory.</p> <p>Parameters:</p> Name Type Description Default <code>conf_path</code> <code>Optional[str]</code> <p>Path to a JSON file with dirctory configuration. See class doc for format. Defaults to None.</p> <code>None</code> <code>conf</code> <code>Optional[dict]</code> <p>A dict with configuration as if loaded from JSON by json.loads(). Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>Only one of conf_path or conf can be given.</p> Source code in <code>sibila/formatdir.py</code> <pre><code>@classmethod\ndef add(cls,\n        conf_path: Optional[str] = None,\n        conf: Optional[dict] = None,\n        ):\n    \"\"\"Add a JSON file or configuration dict to the format directory.\n\n    Args:\n        conf_path: Path to a JSON file with dirctory configuration. See class __doc__ for format. Defaults to None.\n        conf: A dict with configuration as if loaded from JSON by json.loads(). Defaults to None.\n\n    Raises:\n        TypeError: Only one of conf_path or conf can be given.\n    \"\"\"\n\n    def expand_path(path: str) -&gt; str:\n        if '~' in path:\n            path = os.path.expanduser(path)\n        return os.path.abspath(path)\n\n    if not ((conf_path is not None) ^ (conf is not None)):\n        raise TypeError(\"One of conf_path or conf must be given\")\n\n    cls.ensure()\n\n    # conf directory loading\n    if conf is not None:\n        cls.dir.update(conf)\n    else:\n        update_dir_json(cls.dir, conf_path)\n\n    cls._sanity_check()\n</code></pre>"},{"location":"api-reference/#sibila.FormatDir.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(name)\n</code></pre> <p>Get a format entry by name, following aliases if required.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format name.</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Format dict with chat template.</p> Source code in <code>sibila/formatdir.py</code> <pre><code>@classmethod\ndef get(cls,\n        name: str) -&gt; Union[dict,None]:\n    \"\"\"Get a format entry by name, following aliases if required.\n\n    Args:\n        name: Format name.\n\n    Returns:\n        Format dict with chat template.\n    \"\"\"\n\n    cls.ensure()\n\n    na = name.lower()\n    while na in cls.dir.keys():\n        val = cls.dir[na]\n        if isinstance(val, str): # str means link -&gt; follow it\n            na = val\n        else:\n            logger.debug(f\"FormatDir get('{name}'): found '{na}' entry\")\n            return cls._prepare_entry(na, val)\n\n    return None\n</code></pre>"},{"location":"api-reference/#sibila.FormatDir.search","title":"search  <code>classmethod</code>","text":"<pre><code>search(model_id)\n</code></pre> <p>Search for model name or filename in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name of filename of model.</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Format dict with chat template or None if none found.</p> Source code in <code>sibila/formatdir.py</code> <pre><code>@classmethod\ndef search(cls,\n           model_id: str) -&gt; Union[dict,None]:\n    \"\"\"Search for model name or filename in the registry.\n\n    Args:\n        model_id: Name of filename of model.\n\n    Returns:\n        Format dict with chat template or None if none found.\n    \"\"\"\n\n    # Todo: cache compiled re patterns in \"_re\" entries\n\n    cls.ensure()\n\n    for name,val in cls.dir.items():\n        if isinstance(val, str): # a link: ignore when searching\n            continue\n        if \"match\" not in val:\n            continue\n\n        patterns = val[\"match\"]\n        if isinstance(patterns, str):\n            patterns = [patterns]\n\n        for pat in patterns:\n            if re.search(pat, model_id, flags=re.IGNORECASE):\n                logger.debug(f\"FormatDir search('{model_id}'): found '{name}' entry\")\n                return cls._prepare_entry(name, val)\n\n    return None\n</code></pre>"},{"location":"api-reference/#sibila.FormatDir.info","title":"info  <code>classmethod</code>","text":"<pre><code>info()\n</code></pre> <p>Format directory listing.</p> Source code in <code>sibila/formatdir.py</code> <pre><code>@classmethod\ndef info(cls) -&gt; str:\n    \"\"\"Format directory listing.\"\"\"\n    out = \"\"\n    out += f\"Directory: {pformat(cls.dir)}\"\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.FormatDir.clear","title":"clear  <code>classmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear the model directory.</p> Source code in <code>sibila/formatdir.py</code> <pre><code>@classmethod\ndef clear(cls):\n    \"\"\"Clear the model directory.\"\"\"\n    cls.dir = None\n</code></pre>"},{"location":"api-reference/#multigen","title":"Multigen","text":""},{"location":"api-reference/#sibila.multigen","title":"multigen","text":"<p>Functions for comparing output across models.</p> <ul> <li>thread_multigen(), query_multigen() and multigen(): Compare outputs across models.</li> <li>cycle_gen_print(): For a list of models, sequentially grow a Thread with model responses to given IN messages.</li> </ul>"},{"location":"api-reference/#sibila.multigen.thread_multigen","title":"thread_multigen","text":"<pre><code>thread_multigen(\n    threads,\n    model_names,\n    text=None,\n    csv=None,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"obj\"],\n    thread_titles=None,\n)\n</code></pre> <p>Generate a single thread on a list of models, returning/saving results in text/CSV.</p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>list[Thread]</code> <p>List of threads to input into each model.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of ModelDir names.</p> required <code>text</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.</p> <code>None</code> <code>csv</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.</p> <code>None</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>Optional[list[str]]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"obj\"].</p> <code>['text', 'dic', 'obj']</code> <code>thread_titles</code> <code>list[str]</code> <p>A human-friendly title for each Thread. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def thread_multigen(threads: list[Thread],\n                    model_names: list[str],\n\n                    text: Union[str,list[str],None] = None,\n                    csv: Union[str,list[str],None] = None,\n\n                    gencall: Optional[Callable] = None,                   \n                    genconf: Optional[GenConf] = None,\n\n                    out_keys: Optional[list[str]] = [\"text\",\"dic\", \"obj\"],\n\n                    thread_titles: list[str] = None                   \n                    ) -&gt; list[list[str]]:\n    \"\"\"Generate a single thread on a list of models, returning/saving results in text/CSV.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        threads: List of threads to input into each model.\n        model_names: A list of ModelDir names.\n        text: An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.\n        csv: An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"obj\"].\n        thread_titles: A human-friendly title for each Thread. Defaults to None.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"\n\n    assert isinstance(model_names, list), \"model_names must be a list of strings\"\n\n    table = multigen(threads,\n                     model_names=model_names, \n                     gencall=gencall,\n                     genconf=genconf)\n\n    # table[threads,models]\n\n    if thread_titles is None:\n        thread_titles = [str(th) for th in threads]\n\n    def format(format_fn, cmds):\n        if cmds is None or not cmds:\n            return\n\n        f = StringIO(newline='')\n\n        format_fn(f,\n                  table, \n                  title_list=thread_titles,\n                  model_names=model_names,\n                  out_keys=out_keys)\n        fmtd = f.getvalue()\n\n        if not isinstance(cmds, list):\n            cmds = [cmds]\n        for c in cmds:\n            if c == 'print':\n                print(fmtd)\n            else: # path\n                with open(c, \"w\", encoding=\"utf-8\") as f:\n                    f.write(fmtd)\n\n    format(format_text, text)\n    format(format_csv, csv)\n\n    return table\n</code></pre>"},{"location":"api-reference/#sibila.multigen.query_multigen","title":"query_multigen","text":"<pre><code>query_multigen(\n    in_list,\n    inst_text,\n    model_names,\n    text=None,\n    csv=None,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"obj\"],\n    in_titles=None,\n)\n</code></pre> <p>Generate an INST+IN thread on a list of models, returning/saving results in text/CSV.</p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>in_list</code> <code>list[str]</code> <p>List of IN messages to initialize Threads.</p> required <code>inst_text</code> <code>str</code> <p>The common INST to use in all models.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of ModelDir names.</p> required <code>text</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.</p> <code>None</code> <code>csv</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.</p> <code>None</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>Optional[list[str]]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"obj\"].</p> <code>['text', 'dic', 'obj']</code> <code>in_titles</code> <code>list[str]</code> <p>A human-friendly title for each Thread. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def query_multigen(in_list: list[str],\n                   inst_text: str,                                                \n                   model_names: list[str],\n\n                   text: Union[str,list[str],None] = None, # \"print\", path\n                   csv: Union[str,list[str],None] = None, # \"print\", path\n\n                   gencall: Optional[Callable] = None,                   \n                   genconf: Optional[GenConf] = None,\n\n                   out_keys: Optional[list[str]] = [\"text\",\"dic\", \"obj\"],\n                   in_titles: list[str] = None\n                   ) -&gt; list[list[str]]:\n    \"\"\"Generate an INST+IN thread on a list of models, returning/saving results in text/CSV.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        in_list: List of IN messages to initialize Threads.\n        inst_text: The common INST to use in all models.\n        model_names: A list of ModelDir names.\n        text: An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.\n        csv: An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"obj\"].\n        in_titles: A human-friendly title for each Thread. Defaults to None.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"    \n\n    th_list = []\n    for in_text in in_list:\n        th = Thread.make_INST_IN(inst_text, in_text)\n        th_list.append(th)\n\n    if in_titles is None:\n        in_titles = in_list\n\n    out = thread_multigen(th_list,                     \n                          model_names=model_names, \n                          text=text,\n                          csv=csv,\n                          gencall=gencall,\n                          genconf=genconf,\n                          out_keys=out_keys,\n                          thread_titles=in_titles)\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.multigen.multigen","title":"multigen","text":"<pre><code>multigen(\n    threads,\n    *,\n    models=None,\n    model_names=None,\n    model_names_del_after=True,\n    gencall=None,\n    genconf=None\n)\n</code></pre> <p>Generate a list of Threads in multiple models, returning the GenOut for each [thread,model] combination.</p> Actual generation for each model is implemented by the gencall arg Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>list[Thread]</code> <p>List of threads to input into each model.</p> required <code>models</code> <code>Optional[list[Model]]</code> <p>A list of initialized models. Defaults to None.</p> <code>None</code> <code>model_names</code> <code>Optional[list[str]]</code> <p>--Or-- A list of ModelDir names. Defaults to None.</p> <code>None</code> <code>model_names_del_after</code> <code>Optional[bool]</code> <p>Delete model_names models after using them: important or an out-of-memory error will eventually happen. Defaults to True.</p> <code>True</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Only one of models or model_names can be given.</p> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def multigen(threads: list[Thread],\n             *,\n             models: Optional[list[Model]] = None, # existing models\n\n             model_names: Optional[list[str]] = None,\n             model_names_del_after: Optional[bool] = True,\n\n             gencall: Optional[Callable] = None,\n             genconf: Optional[GenConf] = None\n             ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate a list of Threads in multiple models, returning the GenOut for each [thread,model] combination.\n\n    Actual generation for each model is implemented by the gencall arg Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        threads: List of threads to input into each model.\n        models: A list of initialized models. Defaults to None.\n        model_names: --Or-- A list of ModelDir names. Defaults to None.\n        model_names_del_after: Delete model_names models after using them: important or an out-of-memory error will eventually happen. Defaults to True.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n\n    Raises:\n        ValueError: Only one of models or model_names can be given.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"\n\n    if not ((models is None) ^ ((model_names is None))):\n        raise ValueError(\"Only one of models or model_names can be given\")\n\n    if gencall is None:\n        gencall = _default_gencall_text\n\n    mod_count = len(models) if models is not None else len(model_names)\n\n    all_out = []\n\n    for i in range(mod_count):\n        if models is not None:\n            model = models[i]\n            logger.debug(f\"Model: {model.desc}\")\n        else:\n            name = model_names[i]\n            model = ModelDir.create(name)\n            logger.info(f\"Model: {name} -&gt; {model.desc}\")\n\n        mod_out = []\n        for th in threads:\n            out = gencall(model, th, genconf)\n\n            mod_out.append(out)\n\n        all_out.append(mod_out)\n\n        if model_names_del_after and models is None:\n            del model\n\n    # all_out is currently shaped (M,T) -&gt; transpose to (T,M), so that each row contains thread t for all models\n    tout = []\n    for t in range(len(threads)):\n        tmout = [] # thread t for all models\n        for m in range(mod_count):\n            tmout.append(all_out[m][t])\n\n        tout.append(tmout)\n\n    return tout\n</code></pre>"},{"location":"api-reference/#sibila.multigen.cycle_gen_print","title":"cycle_gen_print","text":"<pre><code>cycle_gen_print(\n    in_list,\n    inst_text,\n    model_names,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"obj\"],\n    json_kwargs={\n        \"indent\": 2,\n        \"sort_keys\": False,\n        \"ensure_ascii\": False,\n    },\n)\n</code></pre> <p>For a list of models, sequentially grow a Thread with model responses to given IN messages and print the results.</p> <p>Works by doing: <pre><code>1) Generate an INST+IN prompt for a list of models. (Same INST for all).\n2) Append the output of each model to its own Thread.\n3) Append the next IN prompt and generate again. Back to 2.\n</code></pre></p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>in_list</code> <code>list[str]</code> <p>List of IN messages to initialize Threads.</p> required <code>inst_text</code> <code>str</code> <p>The common INST to use in all models.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of ModelDir names.</p> required <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>Optional[list[str]]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"obj\"].</p> <code>['text', 'dic', 'obj']</code> <code>json_kwargs</code> <code>Optional[dict]</code> <p>JSON dumps() configuration. Defaults to {\"indent\": 2, \"sort_keys\": False, \"ensure_ascii\": False }.</p> <code>{'indent': 2, 'sort_keys': False, 'ensure_ascii': False}</code> Source code in <code>sibila/multigen.py</code> <pre><code>def cycle_gen_print(in_list: list[str],\n                    inst_text: str,                                                \n                    model_names: list[str],\n\n                    gencall: Optional[Callable] = None,                   \n                    genconf: Optional[GenConf] = None,\n\n                    out_keys: Optional[list[str]] = [\"text\",\"dic\", \"obj\"],\n\n                    json_kwargs: Optional[dict] = {\"indent\": 2,\n                                                   \"sort_keys\": False,\n                                                   \"ensure_ascii\": False\n                                                   }\n                    ):\n    \"\"\"For a list of models, sequentially grow a Thread with model responses to given IN messages and print the results.\n\n    Works by doing:\n    ```\n    1) Generate an INST+IN prompt for a list of models. (Same INST for all).\n    2) Append the output of each model to its own Thread.\n    3) Append the next IN prompt and generate again. Back to 2.\n    ```\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        in_list: List of IN messages to initialize Threads.\n        inst_text: The common INST to use in all models.\n        model_names: A list of ModelDir names.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"obj\"].\n        json_kwargs: JSON dumps() configuration. Defaults to {\"indent\": 2, \"sort_keys\": False, \"ensure_ascii\": False }.\n    \"\"\"\n\n    assert isinstance(model_names, list), \"model_names must be a list of strings\"\n\n    if gencall is None:\n        gencall = _default_gencall_text\n\n\n    n_model = len(model_names)\n    n_ins = len(in_list)\n\n    for m in range(n_model):\n\n        name = model_names[m]\n        model = ModelDir.create(name)\n\n        print('=' * 80)\n        print(f\"Model: {name} -&gt; {model.desc}\")\n\n        th = Thread(inst=inst_text)\n\n        for i in range(n_ins):\n            in_text = in_list[i]\n            print(f\"IN: {in_text}\")\n\n            th += (MsgKind.IN, in_text)\n\n            out = gencall(model, th, genconf)\n\n            out_dict = out.asdict()\n\n            print(\"OUT\")\n\n            for k in out_keys:\n\n                if k in out_dict and out_dict[k] is not None:\n\n                    if k != out_keys[0]: # not first\n                        print(\"-\" * 20)\n\n                    val = nice_print(k, out_dict[k], json_kwargs)\n                    print(val)\n\n            th += (MsgKind.OUT, out.text)\n\n        del model\n</code></pre>"},{"location":"api-reference/#tools","title":"Tools","text":""},{"location":"api-reference/#sibila.tools","title":"tools","text":"<p>Tools for model interaction, summarization, etc.</p> <ul> <li>interact(): Interact with model as in a chat, using input().</li> <li>loop(): Iteratively append inputs and generate model outputs.</li> <li>recursive_summarize(): Recursively summarize a (large) text or text file.</li> </ul>"},{"location":"api-reference/#sibila.tools.interact","title":"interact","text":"<pre><code>interact(\n    model,\n    *,\n    ctx=None,\n    inst_text=None,\n    trim_flags=TRIM_DEFAULT,\n    genconf=None\n)\n</code></pre> <p>Interact with model as in a chat, using input().</p> <p>Includes a list of commands: type !? to see help.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to use for generating.</p> required <code>ctx</code> <code>Optional[Context]</code> <p>Optional input Context. Defaults to None.</p> <code>None</code> <code>inst_text</code> <code>Optional[str]</code> <p>text for Thread instructions. Defaults to None.</p> <code>None</code> <code>trim_flags</code> <code>Optional[Trim]</code> <p>Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.</p> <code>TRIM_DEFAULT</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, defaults to model's.    </p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context after all the interactions.</p> Source code in <code>sibila/tools.py</code> <pre><code>def interact(model: Model,\n             *,\n             ctx: Optional[Context] = None,\n             inst_text: Optional[str] = None,\n             trim_flags: Optional[Trim] = TRIM_DEFAULT,\n\n             genconf: Optional[GenConf] = None,\n             ) -&gt; Context:\n    \"\"\"Interact with model as in a chat, using input().\n\n    Includes a list of commands: type !? to see help.\n\n    Args:\n        model: Model to use for generating.\n        ctx: Optional input Context. Defaults to None.\n        inst_text: text for Thread instructions. Defaults to None.\n        trim_flags: Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.\n        genconf: Model generation configuration. Defaults to None, defaults to model's.    \n\n    Returns:\n        Context after all the interactions.\n    \"\"\"\n\n    def callback(out: Union[GenOut,None], \n                 ctx: Context, \n                 model: Model,\n                 genconf: GenConf) -&gt; bool:\n\n        if out is not None:\n            if out.res != GenRes.OK_STOP:\n                print(f\"***Result={GenRes.as_text(out.res)}***\")\n\n            if out.text:\n                text = out.text\n            else:\n                text = \"***No text out***\"\n\n            ctx.add_OUT(text)\n            print(text)\n            print()\n\n\n        def print_thread_info():\n            if ctx.max_token_len is not None: # use from ctx\n                max_token_len = ctx.max_token_len\n            else: # assume max possible for model context and genconf\n                max_token_len = model.ctx_len - genconf.max_tokens\n\n            length = model.token_len(ctx, genconf)\n            print(f\"Thread token len={length}, max len before next gen={max_token_len}\")\n\n\n\n        # input loop ===============================================\n        MARKER: str = '\"\"\"'\n        multiline: str = \"\"\n\n        while True:\n\n            user = input('&gt;').strip()\n\n            if multiline:\n                if user.endswith(MARKER):\n                    user = multiline + \"\\n\" + user[:-3]\n                    multiline = \"\"\n                else:\n                    multiline += \"\\n\" + user\n                    continue\n\n            else:\n                if not user:\n                    return False # terminate loop\n\n                elif user.startswith(MARKER):\n                    multiline = user[3:]\n                    continue\n\n                elif user.endswith(\"\\\\\"):\n                    user = user[:-1]\n                    user = user.replace(\"\\\\n\", \"\\n\")\n                    ctx.add_IN(user)\n                    continue\n\n                elif user.startswith(\"!\"): # a command\n                    params = user[1:].split(\"=\")\n                    cmd = params[0]\n                    params = params[1:]\n\n                    if cmd == \"inst\":\n                        ctx.clear()\n                        if params:\n                            text = params[0].replace(\"\\\\n\", \"\\n\")\n                            ctx.inst = text\n\n                    elif cmd == \"add\" or cmd == \"a\":\n                        if params:\n                            try:\n                                path = params[0]\n                                ctx.addx(path=path)\n                                ct = ctx.last.text\n                                print(ct[:500])\n                            except FileNotFoundError:\n                                print(f\"Could not load '{path}'\")\n                        else:\n                            print(\"Path needed\")\n\n                    elif cmd == 'c':\n                        print_thread_info()\n                        print(ctx)\n\n                    elif cmd == 'cl':\n                        if not params:\n                            params.append(\"ctx.json\")\n                        try:\n                            ctx.load(params[0])\n                            print(f\"Loaded context from {params[0]}\")\n                        except FileNotFoundError:\n                            print(f\"Could not load '{params[0]}'\")\n\n                    elif cmd == 'cs':\n                        if not params:\n                            params.append(\"ctx.json\")\n                        ctx.save(params[0])\n                        print(f\"Saved context to {params[0]}\")\n\n                    elif cmd == 'tl':\n                        print_thread_info()\n\n                    elif cmd == 'i':\n                        print(f\"Model:\\n{model.info()}\")\n                        print(f\"GenConf:\\n{genconf}\\n\")\n\n                        print_thread_info()\n\n                    # elif cmd == 'p':\n                    #     print(model.text_from_turns(ctx.turns))\n\n                    # elif cmd == 'to':\n                    #     token_ids = model.tokens_from_turns(ctx.turns)\n                    #     print(f\"Prompt tokens={token_ids}\")\n\n\n                    else:\n                        print(f\"Unknown command '!{cmd}' - known commands:\\n\"\n                              \" !inst[=text] - clear messages and add inst (system) message\\n\"\n                              \" !add|!a=path - load file and add to last msg\\n\"\n                              \" !c - list context msgs\\n\"\n                              \" !cl=path - load context (default=ctx.json)\\n\"\n                              \" !cs=path - save context (default=ctx.json)\\n\"\n                              \" !tl - thread's token length\\n\"\n                              \" !i - model and genconf info\\n\"\n                              ' Delimit with \"\"\" for multiline begin/end or terminate line with \\\\ to continue into a new line\\n'\n                              \" Empty line + enter to quit\"\n                              )\n                        # \" !p - show formatted prompt (if model supports it)\\n\"\n                        # \" !to - prompt's tokens\\n\"\n\n                    print()\n                    continue\n\n            # we have a user prompt\n            user = user.replace(\"\\\\n\", \"\\n\")\n            break\n\n\n        ctx.add_IN(user)\n\n        return True # continue looping\n\n\n\n    # start prompt loop\n    ctx = loop(callback,\n               model,\n\n               ctx=ctx,\n               inst_text=inst_text,\n               in_text=None, # call callback for first prompt\n               trim_flags=trim_flags,\n               genconf=genconf)\n\n    return ctx\n</code></pre>"},{"location":"api-reference/#sibila.tools.loop","title":"loop","text":"<pre><code>loop(\n    callback,\n    model,\n    *,\n    inst_text=None,\n    in_text=None,\n    trim_flags=TRIM_DEFAULT,\n    ctx=None,\n    genconf=None\n)\n</code></pre> <p>Iteratively append inputs and generate model outputs.</p> <p>Callback should call ctx.add_OUT(), ctx.add_IN() and return a bool to continue looping or not.</p> <p>If last Thread msg is not MsgKind.IN, callback() will be called with out_text=None.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Union[GenOut, None], Context, Model, GenConf], bool]</code> <p>A function(out, ctx, model) that will be iteratively called with model's output.</p> required <code>model</code> <code>Model</code> <p>Model to use for generating.</p> required <code>inst_text</code> <code>Optional[str]</code> <p>text for Thread instructions. Defaults to None.</p> <code>None</code> <code>in_text</code> <code>Optional[str]</code> <p>Text for Thread's initial MsgKind.IN. Defaults to None.</p> <code>None</code> <code>trim_flags</code> <code>Optional[Trim]</code> <p>Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.</p> <code>TRIM_DEFAULT</code> <code>ctx</code> <code>Optional[Context]</code> <p>Optional input Context. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, defaults to model's.</p> <code>None</code> Source code in <code>sibila/tools.py</code> <pre><code>def loop(callback: Callable[[Union[GenOut,None], Context, Model, GenConf], bool],\n         model: Model,\n         *,\n         inst_text: Optional[str] = None,\n         in_text: Optional[str] = None,\n\n         trim_flags: Optional[Trim] = TRIM_DEFAULT,\n         ctx: Optional[Context] = None,\n\n         genconf: Optional[GenConf] = None,\n         ):\n    \"\"\"Iteratively append inputs and generate model outputs.\n\n    Callback should call ctx.add_OUT(), ctx.add_IN() and return a bool to continue looping or not.\n\n    If last Thread msg is not MsgKind.IN, callback() will be called with out_text=None.\n\n    Args:\n        callback: A function(out, ctx, model) that will be iteratively called with model's output.\n        model: Model to use for generating.\n        inst_text: text for Thread instructions. Defaults to None.\n        in_text: Text for Thread's initial MsgKind.IN. Defaults to None.\n        trim_flags: Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.\n        ctx: Optional input Context. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, defaults to model's.\n    \"\"\"\n\n    if ctx is None:\n        ctx = Context()\n    else:\n        ctx = ctx\n\n    if inst_text is not None:\n        ctx.inst = inst_text\n    if in_text is not None:\n        ctx.add_IN(in_text)\n\n    if genconf is None:\n        genconf = model.genconf\n\n    if ctx.max_token_len is not None: # use from ctx\n        max_token_len = ctx.max_token_len\n    else: # assume max possible for model context and genconf\n        max_token_len = model.ctx_len - genconf.max_tokens\n\n\n    while True:\n\n        if len(ctx) and ctx.last_kind == MsgKind.IN:\n            # last is an IN message: we can trim and generate\n\n            ctx.trim(trim_flags,\n                     model,\n                     max_token_len=max_token_len,\n                     genconf=genconf\n                     )\n\n            out = model.gen_(ctx, genconf)\n        else:\n            out = None # first call\n\n        res = callback(out, \n                       ctx=ctx, \n                       model=model,\n                       genconf=genconf)\n\n        if not res:\n            break\n\n\n    return ctx\n</code></pre>"},{"location":"api-reference/#sibila.tools.recursive_summarize","title":"recursive_summarize","text":"<pre><code>recursive_summarize(\n    model, text=None, path=None, overlap_size=20\n)\n</code></pre> <p>Recursively summarize a (large) text or text file.</p> <p>Works by: <pre><code>1) Breaking text into chunks that fit models context.\n2) Run model to summarize chunks.\n3) Join summries and jump to 1) - do this until text size no longer decreases.\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to use for summarizing.</p> required <code>text</code> <code>Optional[str]</code> <p>Initial text.</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>--Or-- A path to an UTF-8 text file.</p> <code>None</code> <code>overlap_size</code> <code>int</code> <p>Size in model tokens of the overlapping portions at begining and end of chunks.</p> <code>20</code> <p>Returns:</p> Type Description <code>str</code> <p>The summarized text.</p> Source code in <code>sibila/tools.py</code> <pre><code>def recursive_summarize(model: Model,\n                        text: Optional[str] = None,\n                        path: Optional[str] = None,\n                        overlap_size: int = 20) -&gt; str:\n    \"\"\"Recursively summarize a (large) text or text file.\n\n    Works by:\n    ```\n    1) Breaking text into chunks that fit models context.\n    2) Run model to summarize chunks.\n    3) Join summries and jump to 1) - do this until text size no longer decreases.\n    ``` \n\n    Args:\n        model: Model to use for summarizing.\n        text: Initial text.\n        path: --Or-- A path to an UTF-8 text file.\n        overlap_size: Size in model tokens of the overlapping portions at begining and end of chunks.\n\n    Returns:\n        The summarized text.\n    \"\"\"\n\n    if (text is not None) + (path is not None) != 1:\n        raise ValueError(\"Only one of text or path can be given\")\n\n    if path is not None:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n\n    inst_text = \"\"\"Your task is to do short summaries of text.\"\"\"\n    in_text = \"Summarize the following text:\\n\"\n    ctx = Context(pinned_inst_text=inst_text)\n\n    # split initial text\n    max_token_len = model.ctx_len - model.genconf.max_tokens - (model.tokenizer.token_len(inst_text + in_text) + 16) \n    logger.debug(f\"Max ctx token len {max_token_len}\")\n\n    token_len_fn = model.tokenizer.token_len_lambda\n    logger.debug(f\"Initial text token_len {token_len_fn(text)}\")\n\n    spl = RecursiveTextSplitter(max_token_len, overlap_size, len_fn=token_len_fn)\n\n    round = 0\n    while True: # summarization rounds\n        logger.debug(f\"Round {round} {'='*60}\")\n\n        in_list = spl.split(text=text)\n        in_len = sum([len(t) for t in in_list])\n\n        logger.debug(f\"Split in {len(in_list)} parts, total len {in_len} chars\")\n\n        out_list = []\n        for i,t in enumerate(in_list):\n\n            logger.debug(f\"{round}&gt;{i} {'='*30}\")\n\n            ctx.clear()\n            ctx.add_IN(in_text)\n            ctx.add_IN(t)\n\n            out = model.gen_(ctx)        \n            logger.debug(out)\n\n            out_list.append(out.text)\n\n        text = \"\\n\".join(out_list)\n\n        out_len = len(text) # sum([len(t) for t in out_list])\n        if out_len &gt;= in_len:\n            break\n        elif len(out_list) == 1:\n            break\n        else:\n            round += 1\n\n    return text\n</code></pre>"},{"location":"api-reference/#dictype","title":"Dictype","text":""},{"location":"api-reference/#sibila.dictype","title":"dictype","text":"<p>A simple way to define typed fields in a dict. It's a convention to quickly define the structure  and types of a python dict directly in code. Kind of a simpler and convenient Pydantic for dicts.</p> <p>A dictype is defined in a python dict, where each entry represents a typed data field with an optional description.</p> <p>Each field is specified as a dict of \"type\" and (optional) \"description\", suppose you want an age field:     \"age\": { \"type\": int, \"desc\": \"Age of the cat\"}</p> <p>Descriptions are optional, but important in helping the model understand what you want.</p> <p>For example: <pre><code>dictype = {\n    \"summary\": { \"type\": str, \"desc\": \"A general summary of stuff\" },\n    \"names\": { \"type\": [str], \"desc\": \"List of stuff names\" },\n    \"cost\": { \"type\": float, \"desc\": \"Cost of stuff\", \"optional\": True },\n    \"kind\": { \"type\": [\"Open\", \"Closed\"], \"desc\": \"The kind of stuff\" },\n\n    # For lists, desc can be split in list_description|item_description:\n    \"colors\": { \"type\": [str], \"desc\": \"Color list for stuff|Color names\" },\n\n    # Or desc can be a list, first item for the list, second for its items: \n    \"colors2\": { \"type\": [str], \"desc\": [\"Color list for stuff\", \"Color names\"] },\n}\n</code></pre></p>"},{"location":"api-reference/#sibila.dictype--allowed-types","title":"Allowed types","text":"<pre><code>Primitive types:\n    bool\n    int\n    float\n    str\n</code></pre> <pre><code>Enums: Represented by a list of values (only primitive types).\n    For example:\n\n    \"kind\": { \"type\": [\"Open\", \"Closed\"], \"desc\": \"The kind of stuff\" }\n</code></pre> <pre><code>Lists: A list of an allowed type.\n    For example to define an str list (note the [] around str):\n\n    \"names\": { \"type\": [str], \"desc\": \"List of stuff names\" }\n\n    If you need to pass a separate description of the list itself and of its elements, \n    you can either pass a list with two strings (list description first) or a string split by a '|' character:\n\n    \"names\": { \"type\": [str], \"desc\": [\"List of stuff names\", \"A name but only round stuff\"] }\n    --or--\n    \"names\": { \"type\": [str], \"desc\": \"List of stuff names|A name but only round stuff\" }\n</code></pre> <pre><code>Dicts: A field can also be a dict, which allows for composing hierarchies.\n    For example, person_type is first defined and then used in the members field:\n\n    person_type = {\n        \"name\": { \"type\": str, \"desc\": \"Name of person\" },\n        \"occupation\": { \"type\": str, \"desc\": \"Person's occupation\" }\n    }\n\n    team_type = {\n        \"chief\": {\"type\": person_type, \"desc\": \"Team's chief\"},\n        \"members\": {\"type\": [person_type], \"desc\": \"List of team members\"}\n    }\n</code></pre>"},{"location":"api-reference/#sibila.dictype--alternative-shorter-notation","title":"Alternative shorter notation","text":"<pre><code>Fields can also be specified as an ordered list with the entries:\n    type, desc, \"optional\"/True\n\n    Only type is necessary, \"optional\" or True in the third position means that the field is not required.\n\n    For example:\n\n    dictype = {\n        \"summary\": [str, \"A general summary of stuff\"],\n        \"names\": [ [str], \"List of stuff names\" ],\n        \"cost\": [ float, \"Cost of stuff\", \"optional\" ],\n        \"kind\": [ [\"Open\", \"Closed\"], \"The kind of stuff\" ],\n\n        # lists can have two descriptions, for list then for its items:\n        \"colors\": [ [str], [\"Color list for stuff\", \"Color name\"] ],\n        \"colors2\": [ [str], \"Color list for stuff|Color name\" ]\n    }\n</code></pre>"},{"location":"api-reference/#sibila.dictype.json_schema_from_dictype","title":"json_schema_from_dictype","text":"<pre><code>json_schema_from_dictype(\n    dictype, desc=None, desc_from_title=0\n)\n</code></pre> <p>Create a JSON schema representation of a dictype.</p> <p>Parameters:</p> Name Type Description Default <code>dictype</code> <code>dict</code> <p>The dictype.</p> required <code>desc</code> <code>Optional[str]</code> <p>Top-level description of the dictype. Defaults to None.</p> <code>None</code> <code>desc_from_title</code> <code>int</code> <p>Should title be used as description for fields that don't have one?. Values: 0=No, 1=Yes, 2=Yes + capitalize first letter and convert _ to space. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If dictype inconsistencies found.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with the created JSON schema.</p> Source code in <code>sibila/dictype.py</code> <pre><code>def json_schema_from_dictype(dictype: dict,\n                             desc: Optional[str] = None,\n                             desc_from_title: int = 0) -&gt; dict:\n    \"\"\"Create a JSON schema representation of a dictype.\n\n    Args:\n        dictype: The dictype.\n        desc: Top-level description of the dictype. Defaults to None.\n        desc_from_title: Should title be used as description for fields that don't have one?. Values: 0=No, 1=Yes, 2=Yes + capitalize first letter and convert _ to space. Defaults to 0.\n\n    Raises:\n        TypeError: If dictype inconsistencies found.\n\n    Returns:\n        A dict with the created JSON schema.\n    \"\"\"\n\n    out = {}\n\n    if desc is not None:\n        out[\"description\"] = desc\n\n    props = out[\"properties\"] = {} \n\n    required_names = []\n\n    for index, (name, field_) in enumerate(dictype.items()):\n\n        # extract field's keys\n        if isinstance(field_, dict):\n            if \"type\" not in field_:\n                raise TypeError(f\"All fields must include a type key at '{name}': {field_}\")\n            ftype = field_[\"type\"]\n\n            fdesc = field_.get(\"desc\") or field_.get(\"description\")\n            foptional = field_.get(\"optional\")\n\n        elif isinstance(field_, list):\n            if len(field_) &lt; 1:\n                raise TypeError(f\"Fields specified as lists must at least have a type entry at '{name}': {field_}\")\n            ftype = field_[0]\n\n            if len(field_) &gt;= 2:\n                fdesc = field_[1]\n            else:\n                fdesc = None\n\n            if len(field_) &gt;= 3:\n                foptional = field_[2]\n\n                if len(field_) &gt; 3:\n                    raise TypeError(f\"Fields specified as lists must have 3 entries at most (type, desc, optional) at '{name}': {field_}\")                 \n            else:\n                foptional = None\n\n        else:\n            raise TypeError(f\"Fields can only be specified as dicts or lists at '{name}': {field_}\")                             \n\n        # verify field values\n        if fdesc is not None:\n            # extract \"list desc\" | \"item desc\"\n            if isinstance(fdesc, str):\n                fdesc = fdesc.split('|')\n                if len(fdesc) == 1:\n                    fdesc.append(None)\n            elif isinstance(fdesc, list):\n                raise TypeError(f\"Field's desc can only be list or str (divide str with '|' char) at '{name}': {field_}\")\n\n            if len(fdesc) &gt; 2:\n                raise TypeError(f\"Fields's desc can only have up to 2 strings at '{name}': {field_}\")\n        else:\n            fdesc = [None,None]\n\n        if foptional is not None:\n            if isinstance(foptional, str):\n                foptional = foptional.lower()\n                foptional = (foptional == \"optional\") or (foptional == \"true\")\n            elif not isinstance(foptional, bool):\n                raise TypeError(f\"Field's optional can only be bool or str at '{name}': {field_}\")\n        else:\n            foptional = False\n\n\n        # now emit\n        if isinstance(ftype, list): # list=[type] or [enum1,enum2]\n\n            if len(ftype) == 1: # a list of type or of enum\n\n                if isinstance(ftype[0], list): # list of enum\n                    items = make_enum('', ftype[0], fdesc[1],\n                                      desc_from_title=desc_from_title)\n\n                else: # list of type\n                    items = make_prim(ftype[0], '', fdesc[1],\n                                      desc_from_title=desc_from_title)\n\n                props[name] = make_prop(name, \"array\", fdesc[0], items=items,\n                                        desc_from_title=desc_from_title)\n\n            else: # list of enum\n                if fdesc[1] is not None:\n                    raise TypeError(f\"Enums must have single description at '{name}': {field_}\")\n\n                props[name] = make_enum(name, ftype, fdesc[0],\n                                        desc_from_title=desc_from_title)\n\n        else: # a primitive type\n            if fdesc[1] is not None:\n                raise TypeError(f\"Primitive types must have single description at '{name}': {field_}\")\n\n            props[name] = make_prim(ftype, name, fdesc[0],\n                                    desc_from_title=desc_from_title)\n\n\n        if not foptional:\n            required_names.append(name)\n\n\n    if len(required_names):\n        out[\"required\"] = required_names\n\n    out[\"type\"] = \"object\"\n\n    return out\n</code></pre>"},{"location":"api-reference/#tokenizers","title":"Tokenizers","text":"<p>Tokenizers used in models.</p>"},{"location":"api-reference/#sibila.LlamaCppTokenizer","title":"LlamaCppTokenizer","text":"<pre><code>LlamaCppTokenizer(llama, reg_flags=None)\n</code></pre> <p>Tokenizer for llama.cpp loaded GGUF models.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def __init__(self, \n             llama: Llama, \n             reg_flags: Optional[str] = None):\n    self._llama = llama\n\n    self.vocab_size = self._llama.n_vocab()\n\n    self.bos_token_id = self._llama.token_bos()\n    self.bos_token = llama_token_get_text(self._llama.model, self.bos_token_id).decode(\"utf-8\")\n\n    self.eos_token_id = self._llama.token_eos()\n    self.eos_token = llama_token_get_text(self._llama.model, self.eos_token_id).decode(\"utf-8\")\n\n    self.pad_token_id = None\n    self.pad_token = None\n\n    self.unk_token_id = None # ? fill by taking a look at id 0?\n    self.unk_token = None\n\n    # workaround for https://github.com/ggerganov/llama.cpp/issues/4772\n    self._workaround1 = reg_flags is not None and \"llamacpp1\" in reg_flags\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppTokenizer.encode","title":"encode","text":"<pre><code>encode(text)\n</code></pre> <p>Encode text into model tokens. Inverse of Decode().</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of ints with the encoded tokens.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def encode(self, \n           text: str) -&gt; list[int]:\n    \"\"\"Encode text into model tokens. Inverse of Decode().\n\n    Args:\n        text: Text to be encoded.\n\n    Returns:\n        A list of ints with the encoded tokens.\n    \"\"\"\n\n    if self._workaround1:\n        # append a space after each bos and eos, so that llama's tokenizer matches HF\n        def space_post(text, s):\n            out = \"\"\n            while (index := text.find(s)) != -1:\n                after = index + len(s)\n                out += text[:after]\n                if text[after] != ' ':\n                    out += ' '\n                text = text[after:]\n\n            out += text\n            return out\n\n        text = space_post(text, self.bos_token)\n        text = space_post(text, self.eos_token)\n        # print(text)\n\n    # str -&gt; bytes\n    text = text.encode(\"utf-8\", errors=\"ignore\")\n\n    return self._llama.tokenize(text, add_bos=False, special=True)\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppTokenizer.decode","title":"decode","text":"<pre><code>decode(token_ids, skip_special=True)\n</code></pre> <p>Decode model tokens to text. Inverse of Encode().</p> <p>Using instead of llama-cpp-python's to fix error: remove first character after a bos only if it's a space.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>List of model tokens.</p> required <code>skip_special</code> <code>bool</code> <p>Don't decode special tokens like bos and eos. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded text.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def decode(self,\n           token_ids: list[int],\n           skip_special: bool = True) -&gt; str:\n    \"\"\"Decode model tokens to text. Inverse of Encode().\n\n    Using instead of llama-cpp-python's to fix error: remove first character after a bos only if it's a space.\n\n    Args:\n        token_ids: List of model tokens.\n        skip_special: Don't decode special tokens like bos and eos. Defaults to True.\n\n    Returns:\n        Decoded text.\n    \"\"\"\n\n    if not len(token_ids):\n        return \"\"\n\n    output = b\"\"\n    size = 32\n    buffer = (ctypes.c_char * size)()\n\n    if not skip_special:\n        special_toks = {self.bos_token_id: self.bos_token.encode(\"utf-8\"), \n                        self.eos_token_id: self.eos_token.encode(\"utf-8\")}\n\n        for token in token_ids:\n            if token == self.bos_token_id:\n                output += special_toks[token]\n            elif token == self.eos_token_id:\n                output += special_toks[token]\n            else:\n                n = llama_cpp.llama_token_to_piece(\n                    self._llama.model, llama_cpp.llama_token(token), buffer, size\n                )\n                output += bytes(buffer[:n])\n\n    else: # skip special\n        for token in token_ids:\n            if token != self.bos_token_id and token != self.eos_token_id:\n                n = llama_cpp.llama_token_to_piece(\n                    self._llama.model, llama_cpp.llama_token(token), buffer, size\n                )\n                output += bytes(buffer[:n])\n\n\n    # \"User code is responsible for removing the leading whitespace of the first non-BOS token when decoding multiple tokens.\"\n    if (# token_ids[0] != self.bos_token_id and # we also try cutting if first is bos to approximate HF tokenizer\n       len(output) and output[0] &lt;= 32 # 32 = ord(' ')\n       ):\n        output = output[1:]\n\n    return output.decode(\"utf-8\", errors=\"ignore\")\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppTokenizer.token_len","title":"token_len","text":"<pre><code>token_len(text)\n</code></pre> <p>Returns token length for given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be measured.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>Token length for given text.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self, \n              text: str) -&gt; list[int]:\n    \"\"\"Returns token length for given text.\n\n    Args:\n        text: Text to be measured.\n\n    Returns:\n        Token length for given text.\n    \"\"\"\n\n    tokens = self.encode(text)\n    return len(tokens)        \n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer","title":"OpenAITokenizer","text":"<pre><code>OpenAITokenizer(model)\n</code></pre> <p>Tokenizer for OpenAI models.</p> Source code in <code>sibila/openai.py</code> <pre><code>def __init__(self, \n             model: str\n             ):\n\n    if not has_tiktoken:\n        raise Exception(\"Please install tiktoken by running: pip install tiktoken\")\n\n    self._tok = tiktoken.encoding_for_model(model)\n\n    self.vocab_size = self._tok.n_vocab\n\n    self.bos_token_id = None\n    self.bos_token = None\n\n    self.eos_token_id = None\n    self.eos_token = None\n\n    self.pad_token_id = None\n    self.pad_token = None\n\n    self.unk_token_id = None\n    self.unk_token = None\n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer.encode","title":"encode","text":"<pre><code>encode(text)\n</code></pre> <p>Encode text into model tokens. Inverse of Decode().</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of ints with the encoded tokens.</p> Source code in <code>sibila/openai.py</code> <pre><code>def encode(self, \n           text: str) -&gt; list[int]:\n    \"\"\"Encode text into model tokens. Inverse of Decode().\n\n    Args:\n        text: Text to be encoded.\n\n    Returns:\n        A list of ints with the encoded tokens.\n    \"\"\"\n    return self._tok.encode(text)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer.decode","title":"decode","text":"<pre><code>decode(token_ids, skip_special=True)\n</code></pre> <p>Decode model tokens to text. Inverse of Encode().</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>List of model tokens.</p> required <code>skip_special</code> <code>bool</code> <p>Don't decode special tokens like bos and eos. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def decode(self, \n           token_ids: list[int],\n           skip_special: bool = True) -&gt; str:\n    \"\"\"Decode model tokens to text. Inverse of Encode().\n\n    Args:\n        token_ids: List of model tokens.\n        skip_special: Don't decode special tokens like bos and eos. Defaults to True.\n\n    Returns:\n        Decoded text.\n    \"\"\"\n    assert skip_special, \"OpenAITokenizer only supports skip_special=True\"\n\n    return self._tok.decode(token_ids)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer.token_len","title":"token_len","text":"<pre><code>token_len(text)\n</code></pre> <p>Returns token length for given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be measured.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>Token length for given text.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self, \n              text: str) -&gt; list[int]:\n    \"\"\"Returns token length for given text.\n\n    Args:\n        text: Text to be measured.\n\n    Returns:\n        Token length for given text.\n    \"\"\"\n\n    tokens = self.encode(text)\n    return len(tokens)        \n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Sibila requires Python 3.9+ and uses the llama-cpp-python package for local models and OpenAI's API to access remote models like GPT-4.</p> <p>You can run it in a plain CPU, CUDA GPU or other accelerator supported by llama.cpp.</p> <p>For accelerated inference with local models, to take advantage of CUDA, Metal, etc, make sure you install llamacpp-python with the right settings - see more info here.</p> <p>To use Sibila, download the repository and from the base directory (which has a setup.py script) do:</p> <pre><code>pip install -e .\n</code></pre> <p>You should now be able to use Sibila to get structured information from local or remote models.</p>"},{"location":"getting-started/#using-open-ai-models","title":"Using OPEN AI models","text":"<p>To use an OpenAI remote model, you'll need a paid OpenAI account and its API key. You can explicitly pass this key when creating an OpenAIModel object but this is not a good security practice. A better way is to define an environment variable which the OpenAI API will use when needed.</p> <p>In Linux/Mac you can define this key by running: <pre><code>export OPENAI_API_KEY=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre></p> <p>And in Windows command prompt:</p> <pre><code>setx OPENAI_API_KEY \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Having set this variable with your OpenAI API key, you can run an \"Hello Model\" example :</p> <pre><code>from sibila import OpenAIModel, GenConf\n\n# make sure you set the environment variable named OPENAI_API_KEY with your API key.\n# create an OpenAI model with generation temperature=1\nmodel = OpenAIModel(\"gpt-4\",\n                    genconf=GenConf(temperature=1))\n\n# the instruction or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(in_text)\n\n# query the model with instructions and \"in\" text\ntext = model.query_gen(inst_text, in_text)\nprint(text)\n</code></pre> <p>This will display </p>"},{"location":"getting-started/#using-local-models-in-llamacpp","title":"Using local models in llama.cpp","text":"<p>Sibila can use llama.cpp (via the llamacpp-python package) to load models from local GGUF format files. Since LLM model files are quite big, they are usually quantized so that each parameter occupies less than a byte. </p> <p>See Setup local models to learn how where to find these models and how to use them in Sibila, then return here to run the following script:</p> <pre><code>from sibila import LlamaCppModel, GenConf\n\n# model file from the models folder\nmodel_path = \"../../models/openchat-3.5-1210.Q4_K_M.gguf\"\n\n# create an OpenAI model with generation temperature=1\nmodel = LlamaCppModel(model_path,\n                      genconf=GenConf(temperature=1))\n\n# the instruction or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(in_text)\n\n# query the model with instruction and \"in\" text\ntext = model.query_gen(inst_text, in_text)\nprint(text)\n</code></pre> <p>The script is available here: hello-llammacpp.py</p>"},{"location":"getting-started/#arrr-answer","title":"Arrr-answer","text":"<p>After running the above and/or OpenAI's script you'll receive the model's answer to your \"Hello there?\" - in arrr-style:</p> <pre><code>Hello there?\nAhoy, me hearty! How be it goin'? Me name's Captain Chatbot, and I be here to assist thee with whatever ye need! So, what can me crew and I do fer yer today? Arrr!\n</code></pre> <p>Which means Sibila is working. Check the examples.</p>"},{"location":"setup-local-models/","title":"How to Setup Local Models","text":"<p>Most current 7B quantized models are pretty capable for common data extraction tasks. Below we'll see how to find and setup local models for use with Sibila. If you only plan to use OpenAI remote models, this is not for you.</p>"},{"location":"setup-local-models/#default-model-used-in-the-examples-openchat","title":"Default model used in the examples: OpenChat","text":"<p>By default, most of the examples included with Sibila use OpenChat, a quantized 7B parameters model, that you can download from:</p> <p>https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF/blob/main/openchat-3.5-1210.Q4_K_M.gguf</p> <p>In this page, click \"download\" and save it into the \"models\" folder inside the Sibila project. It's a 4.4Gb download and can take some time.</p> <p>Once the file \"openchat-3.5-1210.Q4_K_M.gguf\" is placed in the \"models\" folder, you should be able to run the examples with this local model.</p> <p>But you can also search for and use other local models - keep reading to learn more.</p>"},{"location":"setup-local-models/#choose-the-model-chat-or-instruct-types","title":"Choose the model: chat or instruct types","text":"<p>Sibila can use models that were fine-tuned for chat or instruct purposes. These models work in user - assistant turns or messages and use a chat template to properly compose those messages to the format that the model was fine-tuned to.</p> <p>For example, the Llama2 model was released in two editions: a simple Llama2 text completion model and a Llama2-instruct model that was fine tuned for user-assistant turns. For Sibila you should always select chat or instruct versions of a model.</p> <p>But which model to choose? You can look at model benchmark scores in popular listing sites:</p> <ul> <li>https://llm.extractum.io/list/</li> <li>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</li> <li>https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</li> </ul>"},{"location":"setup-local-models/#find-a-quantized-version-of-the-model","title":"Find a quantized version of the model","text":"<p>Since Large Language Models are quite big, they are usually quantized so that each parameter occupies a little more than 4 bits or half a byte. </p> <p>Without quantization, a 7 billion parameters model would require 14Gb of memory (each parameter taking 16 bits) to load and a bit more during inference.</p> <p>With quantization techniques, a 7 billion parameters model can have a file size of only 4.4Gb (using about 50% more in memory - 6.8Gb), which makes it accessible to be ran in common GPUs or even in common RAM memory (albeit slower).</p> <p>Quantized models are stored in a file format popularized by llama.cpp, the GGUF format (which means GPT-Generated Unified Format). We're using llama.cpp to run local models, so we'll be needing GGUF files.</p> <p>A good place to find quantized models is in HuggingFace's model hub, particularly in the well-know TheBloke's (Tom Jobbins) area:</p> <p>https://huggingface.co/TheBloke</p> <p>TheBloke is very prolific in producing quality quantized versions of models, usually shortly after they are released.</p> <p>A good model that we'll be using for the examples is the 4 bit quantization of the OpenChat-3.5 model, which itself is a fine-tuning of Mistral-7b:</p> <p>https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF</p>"},{"location":"setup-local-models/#download-it-into-models-folder","title":"Download it into models/ folder","text":"<p>From HuggingFace, you can download the GGUF file (in this and other quantized models by TheBloke) by scrolling down to the \"Provided files\" section and clicking one of the links. Usually the files ending in \"Q4_K_M\" are very reasonable 4-bit quantizations.</p> <p>In this case you'll download the file \"openchat-3.5-1210.Q4_K_M.gguf\" - save it into the \"models\" folder inside Sibila.</p>"},{"location":"setup-local-models/#find-the-chat-template","title":"Find the chat template","text":"<p>Because these models were fine-tuned for chat or instruct interaction, they use a chat template, which is a Jinja2 format template that converts thread messages into text in the format that the model was trained on. These chat templates are similar to the following one for the ChatML format:</p> <pre><code>{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n</code></pre> <p>When ran over a message list with system, user and model messages, the template produces text like the following: <pre><code>&lt;|im_start|&gt;system\nYou speak like a pirate.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello there?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nAhoy there matey! How can I assist ye today on this here ship o' mine?&lt;|im_end|&gt;\n</code></pre></p> <p>Specific chat templates are needed for the best results when dealing with each model. Sibila uses a singleton class named FormatDir, that tries to automatically detect which template to use with a model, either from the model name or from embedded metadata, if available. This information is stored in the sibila/base_formatdir.json file, which contains several well templates for well-known models; and you can add your own templates as needed into other JSON configuration files.</p> <p>So, how to find the chat template for a new model you intend to use? When downloading a model file, you should look for mentions of the used chat template in its information page and then check if it's already available in FormatDir's base_formatdir.json initialization file.</p> <p>What if the model uses a new chat template that's not yet supported in FormatDir? It's becoming common to include the template in the model's GGUF file metadata, so you should look for a file named \"tokenizer_config.json\" in the main model files. This file should include an entry named \"chat_template\" which is what we want. For example in OpenChat's tokenizer_config.json:</p> <p>https://huggingface.co/openchat/openchat-3.5-1210/blob/main/tokenizer_config.json</p> <p>You'll find this line with the template:</p> <pre><code>{\n    \"...\": \"...\",\n\n    \"chat_template\": \"{{ bos_token }}{% for message in messages %}...{% endif %}\",\n\n    \"...\": \"...\"\n}\n</code></pre> <p>(Don't be confused by the text \"GPT4 correct...\", it's just the text format the model was trained on, and it's not related with OpenAI's)</p> <p>With this text string, we could create an entry in FormatDir and all further models with this name will then use the template.</p>"},{"location":"setup-local-models/#use-the-model-directly","title":"Use the model directly","text":"<p>You can create the model by passing its filename to LlamaCppModel. Suppose we wanted to use the Nous Hermes 2 Solar 10, we would download the file and:</p> <pre><code>model = LlamaCppModel(\"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\")\n</code></pre> <p>If automatic detection doesn't work and you receive an error that the chat template format is unknown: if you know the proper format name (\"chatml\" in this case), you can pass it in the format parameter:</p> <pre><code>model = LlamaCppModel(\"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\",\n                      format=\"chatml\")\n</code></pre> <p>Or if you know the chat template definition, you can also pass it in the format argument:</p> <pre><code>chat_template = \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '&lt;|end_of_turn|&gt;'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\"\n\nmodel = LlamaCppModel(\"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\",\n                      format=chat_template)\n</code></pre> <p>But most of the time, Sibila should automatically detect the used format from the model's filename.</p>"},{"location":"setup-local-models/#use-the-model-with-modeldir","title":"Use the model with ModelDir","text":"<p>Instead of manually creating a LlamaCppModel object, it's a better idea for continued use to create a model entry in ModelDir.</p> <p>In the \"models\" folder you'll find the file \"modeldir.json\". The idea is that you can use this file to configure all files in its folder and the file can be added to ModelDir's configuration by including this line in your scripts:</p> <pre><code>ModelDir.add(\"../../models/modeldir.json\")\n</code></pre> <p>This will register all the defined entries in ModelDir. For the \"nous-hermes-2-solar\" model above that uses \"chatml\" we could add this line into \"modeldir.json\":</p> <pre><code>\"nous-hermes-solar\": {\n    \"name\": \"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\",\n    \"format\": \"chatml\"\n}\n</code></pre> <p>The \"name\" key specify the filename, \"format\" the FormatDir entry that it should use.</p> <p>And we can use then use the model by simply doing:</p> <pre><code>model = ModelDir.create(\"llamacpp:nous-hermes-solar\")\n</code></pre> <p>Note the \"provider:model_name\" format above, where llamacpp is the provider \"and nous-hermes-solar\" is the name we created above in ModelDir.</p> <p>To be more flexible, Sibila also allows you to use the model filename directly, without setting up an entry in ModelDir, like this:</p> <pre><code>model = ModelDir.create(\"llamacpp:nous-hermes-2-solar-10.7b.Q4_K_M.gguf\")\n</code></pre> <p>Note that after \"llamacpp:\", instead of the model name we're directly passing the filename. If you plan to use a model for a while, creating an entry in ModelDir is more flexible.</p>"},{"location":"setup-local-models/#out-of-memory-running-local-models","title":"Out of memory running local models","text":"<p>An important thing to know if you'll be using local models is about Out of memory errors.</p> <p>A 7B model like OpenChat-3.5, when quantized to 4 bits will occupy about 6.8 Gb of memory, in either GPU's VRAM or common RAM. If you try to run a second model at the same time, you might get an out of memory error and/or llama.cpp may crash.</p> <p>This is less of a problem when running scripts from the command line, but in environments like Jupyter where you can have multiple open notebooks, you may get python kernel errors like:</p> <pre><code>Kernel Restarting\nThe kernel for sibila/examples/name.ipynb appears to have died. It will restart automatically.\n</code></pre> <p>If you get an error like this in JupyterLab, open the Kernel menu and select \"Shut Down All Kernels...\". This will get rid of any out-of-memory stuck models.</p> <p>A good practice is to delete any model after you no longer need it or right before loading a new one. A simple \"del model\" works fine, or you can add these two lines before creating a model:</p> <pre><code>try: del model\nexcept: ...\n\nmodel = LlamaCppModel(...)\n</code></pre> <p>This way, any existing model in the current notebook is deleted before creating a new one.</p> <p>However this won't work in across multiple notebooks. In those cases, open JupyterLab's Kernel menu and select \"Shut Down All Kernels...\". This will get rid of any models currently in memory.</p>"},{"location":"tips/","title":"Tips and Tricks","text":"<p>Some general tips from experience with constrained model output with Sibila.</p>"},{"location":"tips/#split-entities-into-separate-classes","title":"Split entities into separate classes","text":"<p>Suppose you want to extract a list of person names from a group. You could use the following class:</p> <pre><code>class Group(BaseModel):\n    persons: List[str] = Field(description=\"List of persons attending\")\n\nout = model.query_pydantic(Group,\n                           inst_text,\n                           in_text)\n</code></pre> <p>But it tends to work better to separate the Person entity into its own class and leave the list in Group:</p> <pre><code>class Person(BaseModel):\n    name: str\n\nclass Group(BaseModel):\n    persons: List[Person]\n\nout = model.query_pydantic(Group,\n                           inst_text,\n                           in_text)\n</code></pre> <p>The same applies to dictype definitions.</p> <p>Adding descriptions always helps, specially in non-trivial extraction.</p>"}]}