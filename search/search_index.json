{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>Extract structured data from LLM models, using a common API to access remote models like GPT-4 or local models via llama.cpp.</p> <ul> <li>Query structured data into dataset or Pydantic BaseModel objects.</li> <li>Use the same API for local and remote models.</li> <li>Thread-based interaction with chat/instruct fine-tuned models.</li> <li>Compare output across local/remote models with included utilities, text or CSV output.</li> <li>Model directory: store configurations and quickly switch between models.</li> <li>Automatic chat templates: identifies and uses the right templates for each model.</li> </ul> <p>With Sibila you can extract structured data from a local quantized model like OpenChat-3.5 with 7B params:</p> <pre><code>from sibila import (LlamaCppModel, OpenAIModel)\nfrom pydantic import BaseModel, Field\n\nclass Info(BaseModel):\n    event_year: int\n    first_name: str\n    last_name: str\n    age_at_the_time: int\n    nationality: str\n\nopenchat = LlamaCppModel(\"openchat-3.5-1210.Q5_K_M.gguf\")\n\nopenchat.extract(Info,\n                 \"Who was the first man in the moon?\",\n                 inst=\"Just be helpful.\") # instructions, aka system message\n</code></pre> <p>Outputs an object of class Info, initialized with the model's output:</p> <pre><code>Info(event_year=1969,\n     first_name='Neil',\n     last_name='Armstrong',\n     age_at_the_time=38,\n     nationality='American')\n</code></pre> <p>With the same API you can also query OpenAI models:</p> <pre><code>gpt4 = OpenAIModel(\"gpt-4-0613\")\n\ngpt4.extract(Info,\n             \"Who was the first man in the moon?\",\n             inst=\"Just be helpful.\") # instructions, aka system message\n</code></pre> <p>And this creates an Info object initialized from model's response, as above.</p> <p>If Pydantic BaseModel objects are too much for your project, you can also use ligher Python dataclass.</p> <p>Sibila also includes model management and tools to compare output between models.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Installation, accessing OpenAI, getting local models - how to get started.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Reference for the Sibila API.</p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#models","title":"Models","text":""},{"location":"api-reference/#sibila.LlamaCppModel","title":"LlamaCppModel","text":"<pre><code>LlamaCppModel(\n    path,\n    format=None,\n    format_search_order=[\"name\", \"meta_template\"],\n    *,\n    genconf=None,\n    schemaconf=None,\n    tokenizer=None,\n    ctx_len=2048,\n    n_gpu_layers=-1,\n    main_gpu=0,\n    n_batch=512,\n    seed=4294967295,\n    verbose=False,\n    **llamacpp_kwargs\n)\n</code></pre> <p>Use local GGUF format models via llama.cpp engine.</p> <p>Supports grammar-constrained JSON output following a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path to the GGUF file.</p> required <code>format</code> <code>Optional[Union[str, dict]]</code> <p>Chat template format to use with model. Leave as None for auto-detection.</p> <code>None</code> <code>format_search_order</code> <code>list[str]</code> <p>Search order for auto-detecting format, \"name\" searches in the filename, \"meta_template\" looks in the model's metadata. Defaults to [\"name\",\"meta_template\"].</p> <code>['name', 'meta_template']</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Default generation configuration, which can be used in gen() and related. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>int</code> <p>Maximum context length to be used (shared for input and output). Defaults to 2048.</p> <code>2048</code> <code>n_gpu_layers</code> <code>int</code> <p>Number of model layers to run in a GPU. Defaults to -1 for all.</p> <code>-1</code> <code>main_gpu</code> <code>int</code> <p>Index of the GPU to use. Defaults to 0.</p> <code>0</code> <code>n_batch</code> <code>int</code> <p>Prompt processing batch size. Defaults to 512.</p> <code>512</code> <code>seed</code> <code>int</code> <p>Random number generation seed, for non zero temperature inference. Defaults to 4294967295.</p> <code>4294967295</code> <code>verbose</code> <code>bool</code> <p>Emit (very) verbose output. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If llama-cpp-python is not installed.</p> <code>ValueError</code> <p>If ctx_len is 0 or larger than the values supported by model.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def __init__(self,\n             path: str,\n\n             format: Optional[Union[str,dict]] = None,                 \n             format_search_order: list[str] = [\"name\",\"meta_template\"],\n\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             tokenizer: Optional[Tokenizer] = None,\n             ctx_len: int = 2048,\n\n             # important LlamaCpp-specific args\n             n_gpu_layers: int = -1,\n             main_gpu: int = 0,\n             n_batch: int = 512,\n             seed: int = 4294967295,\n             verbose: bool = False,\n\n             # other LlamaCpp-specific args\n             **llamacpp_kwargs\n             ):\n    \"\"\"\n    Args:\n        path: File path to the GGUF file.\n        format: Chat template format to use with model. Leave as None for auto-detection.\n        format_search_order: Search order for auto-detecting format, \"name\" searches in the filename, \"meta_template\" looks in the model's metadata. Defaults to [\"name\",\"meta_template\"].\n        genconf: Default generation configuration, which can be used in gen() and related. Defaults to None.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). Defaults to 2048.\n        n_gpu_layers: Number of model layers to run in a GPU. Defaults to -1 for all.\n        main_gpu: Index of the GPU to use. Defaults to 0.\n        n_batch: Prompt processing batch size. Defaults to 512.\n        seed: Random number generation seed, for non zero temperature inference. Defaults to 4294967295.\n        verbose: Emit (very) verbose output. Defaults to False.\n\n    Raises:\n        ImportError: If llama-cpp-python is not installed.\n        ValueError: If ctx_len is 0 or larger than the values supported by model.\n    \"\"\"\n\n    self._llama = None # type: ignore[assignment]\n    self.tokenizer = None # type: ignore[assignment]\n\n    if not has_llama_cpp:\n        raise ImportError(\"Please install llama-cpp-python by running: pip install llama-cpp-python\")\n\n    if ctx_len == 0:\n        raise ValueError(\"LlamaCppModel doesn't support ctx_len=0\")\n\n    super().__init__(True,\n                     genconf,\n                     schemaconf,\n                     tokenizer\n                     )\n\n    # update kwargs from important args\n    llamacpp_kwargs.update(n_ctx=ctx_len,\n                           n_batch=n_batch,\n                           n_gpu_layers=n_gpu_layers,\n                           main_gpu=main_gpu,\n                           seed=seed,\n                           verbose=verbose\n                           )\n\n    logger.debug(f\"Creating Llama with model_path='{path}', llamacpp_kwargs={llamacpp_kwargs}\")\n\n    with normalize_notebook_stdout_stderr(not verbose):\n        self._llama = Llama(model_path=path, **llamacpp_kwargs)\n\n    self._model_path = path\n\n    # correct super __init__ values\n    self._ctx_len = self._llama.n_ctx()\n\n    n_ctx_train = self._llama._model.n_ctx_train()        \n    if self.ctx_len &gt; n_ctx_train:\n        raise ValueError(f\"ctx_len ({self.ctx_len}) is greater than n_ctx_train ({n_ctx_train})\")\n\n\n    if self.tokenizer is None:\n        self.tokenizer = LlamaCppTokenizer(self._llama)\n\n    try:\n        self.init_format(format,\n                         format_search_order,\n                         {\"name\": os.path.basename(self._model_path),\n                          \"meta_template_name\": \"tokenizer.chat_template\"}\n                         )\n    except Exception as e:\n        del self.tokenizer\n        del self._llama\n        raise e\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Free type constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:\n\n    \"\"\"Free type constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.json","title":"json","text":"<pre><code>json(\n    json_schema,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,             \n         json_schema: Union[dict,str,None],\n\n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(json_schema,                            \n                        thread,\n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: E811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. \n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    thread = self._prepare_gen_in(thread, genconf)\n\n    prompt = self.text_from_thread(thread)\n\n    logger.debug(f\"Prompt: \u2588{prompt}\u2588\")\n\n    text,finish = self._gen_text(prompt, genconf)\n\n    out = self._prepare_gen_out(text, finish, genconf)\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    json_schema,\n    thread,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             json_schema: Union[dict,str,None],\n\n             thread: Thread,\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    if genconf.json_schema is not None and json_schema is not None:\n        logger.warn(\"Both arg json_schema and genconf.json_schema are set: using json_schema arg\")\n\n    if json_schema is not None:\n        if schemaconf is None:\n            schemaconf = self.schemaconf\n\n        logger.debug(\"JSON schema conf:\\n\" + pformat(schemaconf))\n\n        if massage_schema:\n            if not isinstance(json_schema, dict):\n                json_schema = json.loads(json_schema)\n\n            json_schema = json_schema_massage(json_schema, schemaconf) # type: ignore[arg-type]\n            logger.debug(\"Massaged JSON schema:\\n\" + pformat(json_schema))\n\n    out = self.gen(thread, \n                   genconf(format=\"json\", \n                           json_schema=json_schema))\n\n    return out        \n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    if is_dataclass(cls):\n        schema = build_dataclass_object_json_schema(cls)\n    else:\n        raise TypeError(\"Only dataclass allowed for argument cls\")\n\n    out = self.gen_json(schema,\n                        thread,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    if out.dic is not None:\n        try:\n            obj = create_final_instance(cls, \n                                        is_list=False,\n                                        val=out.dic,\n                                        schemaconf=schemaconf)\n            out.value = obj\n\n        except TypeError as e:\n            out.res = GenRes.ERROR_JSON_SCHEMA_VAL # error initializing object from JSON\n            out.text += f\"\\nJSON Schema error: {e}\"\n    else:\n        # out.res already holds the right error\n        ...\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    if is_subclass_of(cls, BaseModel):\n        schema = json_schema_from_pydantic(cls)\n    else:\n        raise TypeError(\"Only pydantic BaseModel allowed for argument cls\")\n\n    out = self.gen_json(schema,\n                        thread,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    if out.dic is not None:\n        try:\n            obj = pydantic_obj_from_json(cls, \n                                         out.dic,\n                                         schemaconf=schemaconf)\n            out.value = obj\n\n        except TypeError as e:\n            out.res = GenRes.ERROR_JSON_SCHEMA_VAL # error validating for object (by Pydantic), but JSON is valid for its schema\n            out.text += f\"\\nJSON Schema error: {e}\"\n    else:\n        # out.res already holds the right error\n        ...\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.token_len","title":"token_len","text":"<pre><code>token_len(thread, _=None)\n</code></pre> <p>Calculate token length for a Thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>For token length calculation.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens the thread will use.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self,\n              thread: Thread,\n              _: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Calculate token length for a Thread.\n\n    Args:\n        thread: For token length calculation.\n\n    Returns:\n        Number of tokens the thread will use.\n    \"\"\"\n\n    text = self.text_from_thread(thread)\n    return self.tokenizer.token_len(text)\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = None\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppModel.ctx_len","title":"ctx_len  <code>property</code>","text":"<pre><code>ctx_len\n</code></pre> <p>Maximum context length, shared for input + output. We assume a common in+out context where total token length must always be less than this number.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.desc","title":"desc  <code>property</code>","text":"<pre><code>desc\n</code></pre> <p>Model description.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.n_embd","title":"n_embd  <code>property</code>","text":"<pre><code>n_embd\n</code></pre> <p>Embedding size of model.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.n_params","title":"n_params  <code>property</code>","text":"<pre><code>n_params\n</code></pre> <p>Total number of model parameters.</p>"},{"location":"api-reference/#sibila.LlamaCppModel.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata()\n</code></pre> <p>Returns model metadata.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def get_metadata(self):\n    \"\"\"Returns model metadata.\"\"\"\n    out = {}\n    buf = bytes(16 * 1024)\n    lmodel = self._llama.model\n    count = llama_cpp.llama_model_meta_count(lmodel)\n    for i in range(count):\n        res = llama_cpp.llama_model_meta_key_by_index(lmodel, i, buf,len(buf))\n        if res &gt;= 0:\n            key = buf[:res].decode('utf-8')\n            res = llama_cpp.llama_model_meta_val_str_by_index(lmodel, i, buf,len(buf))\n            if res &gt;= 0:\n                value = buf[:res].decode('utf-8')\n                out[key] = value\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel","title":"OpenAIModel","text":"<pre><code>OpenAIModel(\n    name,\n    unknown_name_mask=0,\n    *,\n    genconf=None,\n    schemaconf=None,\n    tokenizer=None,\n    ctx_len=0,\n    api_key=None,\n    base_url=None,\n    openai_init_kwargs={}\n)\n</code></pre> <p>Access an OpenAI model.</p> <p>Supports constrained JSON output, via the OpenAI API tools mechanism. Ref: https://platform.openai.com/docs/api-reference/chat/create</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name.</p> required <code>unknown_name_mask</code> <code>int</code> <p>How to deal with unmatched names, a mask of:</p> <ul> <li>2: Raise NameError if exact name not found.</li> <li>1: Only allow versioned names - raise NameError if generic non-versioned model name used.</li> <li>0: (default) Accept any name, use first in list if necessary.</li> </ul> <code>0</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>int</code> <p>Maximum context length to be used (shared for input and output). Defaults to 0 which means model's maximum.</p> <code>0</code> <code>api_key</code> <code>Optional[str]</code> <p>OpenAI API key. Defaults to None, which will use env variable OPENAI_API_KEY.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base location for API access. Defaults to None, which will use env variable OPENAI_BASE_URL.</p> <code>None</code> <code>openai_init_kwargs</code> <code>dict</code> <p>Extra args for OpenAI.OpenAI() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If OpenAI API is not installed.</p> Source code in <code>sibila/openai.py</code> <pre><code>def __init__(self,\n             name: str,\n             unknown_name_mask: int = 0,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             tokenizer: Optional[Tokenizer] = None,\n             ctx_len: int = 0,\n\n             # most important OpenAI-specific args\n             api_key: Optional[str] = None,\n             base_url: Optional[str] = None,\n\n             # OpenAI-specific args\n             openai_init_kwargs: dict = {},\n             ):\n    \"\"\"\n    Args:\n        name: Model name.\n        unknown_name_mask: How to deal with unmatched names, a mask of:\n\n            - 2: Raise NameError if exact name not found.\n            - 1: Only allow versioned names - raise NameError if generic non-versioned model name used.\n            - 0: (default) Accept any name, use first in list if necessary.\n\n        genconf: Model generation configuration. Defaults to None.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). Defaults to 0 which means model's maximum.\n        api_key: OpenAI API key. Defaults to None, which will use env variable OPENAI_API_KEY.\n        base_url: Base location for API access. Defaults to None, which will use env variable OPENAI_BASE_URL.\n        openai_init_kwargs: Extra args for OpenAI.OpenAI() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If OpenAI API is not installed.\n    \"\"\"\n\n\n    if not has_openai:\n        raise ImportError(\"Please install openai by running: pip install openai\")\n\n    self._model_name, max_ctx_len, self._tokens_per_message, self._tokens_per_name = resolve_model(\n        name,\n        unknown_name_mask\n    )\n\n\n    super().__init__(False,\n                     genconf,\n                     schemaconf,\n                     tokenizer\n                     )\n\n    # only check for \"json\" text presence as json schema is requested with the tools facility.\n    self.json_format_instructors[\"json_schema\"] = self.json_format_instructors[\"json\"]\n\n    logger.debug(f\"Creating OpenAI with base_url={base_url}, openai_init_kwargs={openai_init_kwargs}\")\n\n    self._client = openai.OpenAI(api_key=api_key,\n                                 base_url=base_url,\n\n                                 **openai_init_kwargs\n                                 )\n\n\n    # correct super __init__ values\n    if self.tokenizer is None:\n        self.tokenizer = OpenAITokenizer(self._model_name)\n\n    if ctx_len == 0:\n        self._ctx_len = max_ctx_len\n    else:\n        self._ctx_len = ctx_len\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Free type constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:\n\n    \"\"\"Free type constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If method was not defined by a derived class.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        NotImplementedError: If method was not defined by a derived class.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    token_len = self.token_len(thread, genconf)\n    if genconf.max_tokens == 0:\n        genconf = genconf(max_tokens=self.ctx_len - token_len)\n\n    elif token_len + genconf.max_tokens &gt; self.ctx_len:\n        # this is not true for all models: 1106 models have 128k max input and 4k max output (in and out ctx are not shared)\n        # so we assume the smaller max ctx length for the model\n        logger.warn(f\"Token length + genconf.max_tokens ({token_len + genconf.max_tokens}) is greater than model's context window length ({self.ctx_len})\")\n\n\n    thread = self._prepare_gen_in(thread, genconf)\n\n    fn_name = \"json_out\"\n\n    json_kwargs: dict = {}\n    format = genconf.format\n    if format == \"json\":\n\n        if genconf.json_schema is None:\n            json_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        else:\n            # use json_schema in OpenAi's tool API\n            json_kwargs[\"tool_choice\"] = {\n                \"type\": \"function\",\n                \"function\": {\"name\": fn_name},\n            }\n\n            if isinstance(genconf.json_schema, str):\n                params = json.loads(genconf.json_schema)\n            else:\n                params = genconf.json_schema\n\n            json_kwargs[\"tools\"] = [\n                {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": fn_name,\n                        \"parameters\": params\n                    }\n                }\n            ]\n\n    logger.debug(f\"OpenAI json args: {json_kwargs}\")\n\n    msgs = thread.as_chatml()\n\n    # https://platform.openai.com/docs/api-reference/chat/create\n    response = self._client.chat.completions.create(model=self._model_name,\n                                                    messages=msgs, # type: ignore[arg-type]\n\n                                                    max_tokens=genconf.max_tokens,\n                                                    stop=genconf.stop,\n                                                    temperature=genconf.temperature,\n                                                    top_p=genconf.top_p,\n                                                    **json_kwargs,\n\n                                                    n=1\n                                                    )\n\n    logger.debug(f\"OpenAI response: {response}\")\n\n    choice = response.choices[0]\n    finish = choice.finish_reason\n    message = choice.message\n\n    if \"tool_choice\" in json_kwargs:\n\n        # json schema generation via the tools API:\n        if message.tool_calls is not None:\n            fn = message.tool_calls[0].function\n            if fn.name != fn_name:\n                logger.debug(f\"OpenAIModel: different returned JSON function name ({fn.name})\")\n\n            text = fn.arguments\n        else: # use content instead\n            text = message.content # type: ignore[assignment]\n\n    else:\n        # text or simple json format\n        text = message.content # type: ignore[assignment]\n\n    out = self._prepare_gen_out(text, finish, genconf)\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.json","title":"json","text":"<pre><code>json(\n    json_schema,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,             \n         json_schema: Union[dict,str,None],\n\n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(json_schema,                            \n                        thread,\n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: E811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    json_schema,\n    thread,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             json_schema: Union[dict,str,None],\n\n             thread: Thread,\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    if genconf is None:\n        genconf = self.genconf\n\n    if genconf.json_schema is not None and json_schema is not None:\n        logger.warn(\"Both arg json_schema and genconf.json_schema are set: using json_schema arg\")\n\n    if json_schema is not None:\n        if schemaconf is None:\n            schemaconf = self.schemaconf\n\n        logger.debug(\"JSON schema conf:\\n\" + pformat(schemaconf))\n\n        if massage_schema:\n            if not isinstance(json_schema, dict):\n                json_schema = json.loads(json_schema)\n\n            json_schema = json_schema_massage(json_schema, schemaconf) # type: ignore[arg-type]\n            logger.debug(\"Massaged JSON schema:\\n\" + pformat(json_schema))\n\n    out = self.gen(thread, \n                   genconf(format=\"json\", \n                           json_schema=json_schema))\n\n    return out        \n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    if is_dataclass(cls):\n        schema = build_dataclass_object_json_schema(cls)\n    else:\n        raise TypeError(\"Only dataclass allowed for argument cls\")\n\n    out = self.gen_json(schema,\n                        thread,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    if out.dic is not None:\n        try:\n            obj = create_final_instance(cls, \n                                        is_list=False,\n                                        val=out.dic,\n                                        schemaconf=schemaconf)\n            out.value = obj\n\n        except TypeError as e:\n            out.res = GenRes.ERROR_JSON_SCHEMA_VAL # error initializing object from JSON\n            out.text += f\"\\nJSON Schema error: {e}\"\n    else:\n        # out.res already holds the right error\n        ...\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    if is_subclass_of(cls, BaseModel):\n        schema = json_schema_from_pydantic(cls)\n    else:\n        raise TypeError(\"Only pydantic BaseModel allowed for argument cls\")\n\n    out = self.gen_json(schema,\n                        thread,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    if out.dic is not None:\n        try:\n            obj = pydantic_obj_from_json(cls, \n                                         out.dic,\n                                         schemaconf=schemaconf)\n            out.value = obj\n\n        except TypeError as e:\n            out.res = GenRes.ERROR_JSON_SCHEMA_VAL # error validating for object (by Pydantic), but JSON is valid for its schema\n            out.text += f\"\\nJSON Schema error: {e}\"\n    else:\n        # out.res already holds the right error\n        ...\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.token_len","title":"token_len","text":"<pre><code>token_len(thread, genconf=None)\n</code></pre> <p>Calculate the number of tokens used by a list of messages. If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.</p> <p>From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p> <p>More info on calculating function_call (and tools?) tokens:</p> <p>https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24</p> <p>https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens the thread will use.</p> Source code in <code>sibila/openai.py</code> <pre><code>def token_len(self,\n              thread: Thread,\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Calculate the number of tokens used by a list of messages.\n    If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.\n\n    From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    More info on calculating function_call (and tools?) tokens:\n\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24\n\n    https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5\n\n    Args:\n        thread: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens the thread will use.\n    \"\"\"\n\n    # name = self._model_name\n\n    num_tokens = 0\n    for index in range(-1, len(thread)): # -1 for system message\n        message = thread.msg_as_chatml(index)\n        num_tokens += self._tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(self.tokenizer.encode(value))\n            # if key == \"name\":\n            #     num_tokens += self._tokens_per_name\n\n    num_tokens += 3  # every reply is primed with &lt;|start|&gt;assistant&lt;|message|&gt;\n\n    if genconf is not None and genconf.json_schema is not None:\n        if isinstance(genconf.json_schema, str):\n            js_str = genconf.json_schema\n        else:\n            js_str = json.dumps(genconf.json_schema)\n        # this is an upper bound, as empirically tested with the api.\n        num_tokens += self.tokenizer.token_len(js_str)                \n\n    return num_tokens\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = OpenAITokenizer(_model_name)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAIModel.ctx_len","title":"ctx_len  <code>property</code>","text":"<pre><code>ctx_len\n</code></pre> <p>Maximum context length, shared for input + output. We assume a common in+out context where total token length must always be less than this number.</p>"},{"location":"api-reference/#sibila.OpenAIModel.desc","title":"desc  <code>property</code>","text":"<pre><code>desc\n</code></pre> <p>Model description.</p>"},{"location":"api-reference/#models-directory","title":"Models directory","text":""},{"location":"api-reference/#sibila.Models","title":"Models","text":"<p>Model and template format directory that unifies (and simplifies) model access and configuration.</p> This env variable is checked and used during initialization <p>SIBILA_MODELS: ';'-delimited list of folders where to find: models.json, formats.json and the model files.</p> <p>= Model Directory ================================</p> <p>Useful to create models from resource names like \"llamacpp:openchat\" or \"openai:gpt-4\".  This makes it simple to change a model, store model settings, to compare model outputs, etc.</p> <p>User can add new entries from script or with JSON filenames, via the add() call. New directory entries with the same name are merged into existing ones for each added config.</p> <p>Uses file base_models.json in this script's directory for the initial defaults,  which the user can augment by calling setup() with own config files or directly adding model config with add_model().</p> <p>An example of a model directory JSON config file:</p> <pre><code>{\n    # \"llamacpp\" is a provider, you can then create models with names \n    # like \"provider:model_name\", for ex: \"llamacpp:openchat\"\n    \"llamacpp\": { \n\n        \"default\": {\n            # Place here default args for all llamacpp: models. \n            # Each entry below can then override as needed.\n        },\n\n        \"openchat\": { # this is model definition\n            \"name\": \"openchat-3.5-1210.Q4_K_M.gguf\",\n            \"format\": \"openchat\" # chat template format used by this model\n        },\n\n        \"phi2\": {\n            \"name\": \"phi-2.Q5_K_M.gguf\", # model filename\n            \"format\": \"phi2\"\n        },\n\n        \"oc\": \"openchat\" \n        # this is an alias: \"oc\" forwards to the \"openchat\" entry\n    },\n\n    # The \"openai\" provider. A model can be created with name: \"openai:gpt-4\"\n    \"openai\": { \n\n        \"default\": {}, # default settings for all openai models\n\n        \"gpt-3.5\": {\n            \"name\": \"gpt-3.5-turbo-1106\" # OpenAI's  model name\n        },\n\n        \"gpt-4\": {\n            \"name\": \"gpt-4-1106-preview\"\n        },\n    },\n\n    # Entry \"alias\" is not a provider but a way to have simpler alias names.\n    # For example you can use \"alias:develop\" or even simpler, just \"develop\".\n    \"alias\": { \n        \"develop\": \"llamacpp:openchat\",\n        \"production\": \"openai:gpt-3.5\"\n    }\n\n}\n</code></pre> <p>= Format Directory ================================</p> <p>Detects chat templates from model name/filename or uses from metadata if possible.</p> <p>This directory can be setup from a JSON file or by calling add().</p> <p>Any new directory entries with the same name replace previous ones on each new call.</p> <p>Initializes from file base_formats.json in this module's directory.</p> <p>This env variable is checked during initialization to load from a file:</p> <pre><code>SIBILA_FORMAT_CONF: path of a JSON configuration file to add().\n</code></pre> <p>An example of a format directory JSON config file:</p> <pre><code>{\n    \"chatml\": {\n        # template is a Jinja2 template for this model\n        \"template\": \"{% for message in messages %}...\"\n    },\n\n    \"openchat\": {\n        \"match\": \"openchat.3\", # a regexp to match model name or filename\n        \"template\": \"{{ bos_token }}...\"\n    },    \n\n    \"phi2\": {\n        \"match\": \"phi-2\",\n        \"template\": \"...\"\n    },\n\n    \"phi\": \"phi2\",\n    # this is an alias \"phi\" -&gt; \"phi2\"\n}\n</code></pre> <p>Jinja2 templates receive a standard ChatML messages list (created from a Thread) and must deal with the following:</p> <ul> <li> <p>In models that don't use a system message, template must take care of prepending it to first user message.</p> </li> <li> <p>The add_generation_prompt template variable is always set as True.</p> </li> </ul>"},{"location":"api-reference/#sibila.Models.setup","title":"setup  <code>classmethod</code>","text":"<pre><code>setup(path=None, clear=False)\n</code></pre> <p>Initialize models and formats directory from given model files folder and/or contained configuration files. Path can start with \"~/\" current account's home directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Union[str, list[str]]]</code> <p>Path to a folder or to \"models.json\" or \"formats.json\" configuration files. Defaults to None which tries to initialize from defaults and env variable.</p> <code>None</code> <code>clear</code> <code>bool</code> <p>Set to clear existing directories before loading from path arg.</p> <code>False</code> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef setup(cls,\n          path: Optional[Union[str,list[str]]] = None,\n          clear: bool = False):\n    \"\"\"Initialize models and formats directory from given model files folder and/or contained configuration files.\n    Path can start with \"~/\" current account's home directory.\n\n    Args:\n        path: Path to a folder or to \"models.json\" or \"formats.json\" configuration files. Defaults to None which tries to initialize from defaults and env variable.\n        clear: Set to clear existing directories before loading from path arg.\n    \"\"\"\n\n    if clear:\n        cls.clear()\n\n    cls._ensure()\n\n    if path is not None:\n        if isinstance(path, str):\n            path_list = [path]\n        else:\n            path_list = path\n\n        cls._read_any(path_list)\n\n    cls._sanity_check_models()\n    cls._sanity_check_formats()\n</code></pre>"},{"location":"api-reference/#sibila.Models.clear","title":"clear  <code>classmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear directories. Member genconf is not cleared.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef clear(cls):\n    \"\"\"Clear directories. Member genconf is not cleared.\"\"\"\n    cls.model_dir = None\n    cls.search_path = []\n    cls.format_dir = None\n</code></pre>"},{"location":"api-reference/#sibila.Models.info","title":"info  <code>classmethod</code>","text":"<pre><code>info(verbose=False)\n</code></pre> <p>Return information about current setup.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If False, formats directory values are abbreviated. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Textual information about the current setup.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef info(cls,\n         verbose: bool = False) -&gt; str:\n    \"\"\"Return information about current setup.\n\n    Args:\n        verbose: If False, formats directory values are abbreviated. Defaults to False.\n\n    Returns:\n        Textual information about the current setup.\n    \"\"\"\n\n    cls._ensure()\n\n    out = \"\"\n\n    out += f\"Model search path: {cls.search_path}\\n\"\n    out += f\"Models directory:\\n{pformat(cls.model_dir, sort_dicts=False)}\\n\"\n    out += f\"Model Genconf:\\n{cls.genconf}\\n\"\n\n    if not verbose:\n        fordir = {}\n        for key in cls.format_dir:\n            fordir[key] = copy(cls.format_dir[key])\n            if isinstance(fordir[key], dict) and \"template\" in fordir[key]:\n                fordir[key][\"template\"] = fordir[key][\"template\"][:14] + \"...\"\n    else:\n        fordir = cls.format_dir\n\n    out += f\"Formats directory:\\n{pformat(fordir)}\"\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.Models.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(res_name, genconf=None, ctx_len=None, **over_args)\n</code></pre> <p>Create a model.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Optional model generation configuration. Used instead of set_genconf() value and any directory defaults. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used. Overrides directory defaults. Defaults to None.</p> <code>None</code> <code>over_args</code> <code>Union[Any]</code> <p>Model-specific creation args, which will override default args set in model directory.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>the initialized model.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef create(cls,\n           res_name: str,\n\n           # common to all providers\n           genconf: Optional[GenConf] = None,\n           ctx_len: Optional[int] = None,\n\n           # model-specific overriding:\n           **over_args: Union[Any]) -&gt; Model:\n    \"\"\"Create a model.\n\n    Args:\n        res_name: Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".\n        genconf: Optional model generation configuration. Used instead of set_genconf() value and any directory defaults. Defaults to None.\n        ctx_len: Maximum context length to be used. Overrides directory defaults. Defaults to None.\n        over_args: Model-specific creation args, which will override default args set in model directory.\n\n    Returns:\n        Model: the initialized model.\n    \"\"\"\n\n    cls._ensure()        \n\n    # resolve \"alias:name\" res names, or \"name\": \"link_name\" links\n    provider,name = cls.resolve_model_urn(res_name)\n    # arriving here, prov as a non-link dict entry\n    logger.debug(f\"Resolved model '{res_name}' to '{provider}','{name}'\")\n\n    prov = cls.model_dir[provider]\n\n    args = (prov.get(\"default\")).copy() or {}\n    prov_conf = cls.PROVIDER_CONF[provider]    \n\n    if name in prov:\n        model_args = prov[name]\n\n        # default(if any) &lt;- model_args &lt;- over_args\n        args = (prov.get(\"default\")).copy() or {}\n        args.update(model_args)        \n        args.update(over_args)\n\n    else:                \n        if \"name_passthrough\" in prov_conf[\"flags\"]:\n            model_args = {\n                \"name\": name                \n            }\n        else:\n            raise ValueError(f\"Model '{name}' not found in provider '{provider}'\")\n\n        args.update(model_args)\n        args.update(over_args)\n\n    # override genconf, ctx_len\n    if genconf is None:\n        genconf = cls.genconf\n    if genconf is not None:\n        args[\"genconf\"] = genconf\n\n    if ctx_len is not None:\n        args[\"ctx_len\"] = ctx_len\n\n    logger.debug(f\"Creating model '{provider}:{name}' with resolved args: {args}\")\n\n    model: Model\n    if provider == \"llamacpp\":\n\n        # resolve filename -&gt; path\n        path = cls._locate_file(args[\"name\"])\n        if path is None:\n            raise FileNotFoundError(f\"File not found in '{res_name}' while looking for file '{args['name']}'. Make sure you initialized Models with a path to this file's folder\")\n\n        logger.debug(f\"Resolved llamacpp model '{args['name']}' to '{path}'\")\n\n        del args[\"name\"]\n        args[\"path\"] = path\n\n        from .llamacpp import LlamaCppModel\n\n        model = LlamaCppModel(**args)\n\n\n    elif provider == \"openai\":\n\n        from .openai import OpenAIModel\n\n        model = OpenAIModel(**args)\n\n    \"\"\"\n    elif provider == \"hf\":\n        from .hf import HFModel\n\n        model = HFModel(**args)\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"api-reference/#sibila.Models.add_search_path","title":"add_search_path  <code>classmethod</code>","text":"<pre><code>add_search_path(path)\n</code></pre> <p>Prepends new paths to model files search path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, list[str]]</code> <p>A path or list of paths to add to model search path.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef add_search_path(cls,\n                    path: Union[str,list[str]]):\n    \"\"\"Prepends new paths to model files search path.\n\n    Args:\n        path: A path or list of paths to add to model search path.\n    \"\"\"\n\n    cls._ensure()\n\n    prepend_path(cls.search_path, path)\n\n    logger.debug(f\"Adding '{path}' to search_path\")\n</code></pre>"},{"location":"api-reference/#sibila.Models.set_genconf","title":"set_genconf  <code>classmethod</code>","text":"<pre><code>set_genconf(genconf)\n</code></pre> <p>Set the GenConf to use as default for model creation.</p> <p>Parameters:</p> Name Type Description Default <code>genconf</code> <code>GenConf</code> <p>Model generation configuration.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef set_genconf(cls,\n                genconf: GenConf):\n    \"\"\"Set the GenConf to use as default for model creation.\n\n    Args:\n        genconf: Model generation configuration.\n    \"\"\"\n    cls.genconf = genconf\n</code></pre>"},{"location":"api-reference/#sibila.Models.add_model","title":"add_model  <code>classmethod</code>","text":"<pre><code>add_model(res_name, conf_or_link)\n</code></pre> <p>Add model configuration or name alias for given res_name.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>A name in the form \"provider:model_name\", for example \"openai:gtp-4\".</p> required <code>conf_or_link</code> <code>Union[dict, str]</code> <p>A configuration dict or an alias name (to an existing model).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown provider.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef add_model(cls,\n              res_name: str,\n              conf_or_link: Union[dict,str]):\n\n    \"\"\"Add model configuration or name alias for given res_name.\n\n    Args:\n        res_name: A name in the form \"provider:model_name\", for example \"openai:gtp-4\".\n        conf_or_link: A configuration dict or an alias name (to an existing model).\n\n    Raises:\n        ValueError: If unknown provider.\n    \"\"\"\n\n    cls._ensure()\n\n    provider,_ = provider_name_from_urn(res_name)\n    if provider not in cls.ALL_PROVIDER_NAMES:\n        raise ValueError(f\"Unknown provider '{provider}' in '{res_name}'\")\n\n    cls.model_dir[provider] = conf_or_link\n\n    cls._sanity_check_models()\n</code></pre>"},{"location":"api-reference/#sibila.Models.get_format","title":"get_format  <code>classmethod</code>","text":"<pre><code>get_format(name)\n</code></pre> <p>Get a format entry by name, following aliases if required.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format name.</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Format dict with chat template.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef get_format(cls,\n               name: str) -&gt; Union[dict,None]:\n    \"\"\"Get a format entry by name, following aliases if required.\n\n    Args:\n        name: Format name.\n\n    Returns:\n        Format dict with chat template.\n    \"\"\"\n\n    cls._ensure()\n\n    na = name.lower()\n    while na in cls.format_dir.keys():\n        val = cls.format_dir[na]\n        if isinstance(val, str): # str means link -&gt; follow it\n            na = val\n        else:\n            logger.debug(f\"Format get('{name}'): found '{na}' entry\")\n            return cls._prepare_format_entry(na, val)\n\n    return None\n</code></pre>"},{"location":"api-reference/#sibila.Models.search_format","title":"search_format  <code>classmethod</code>","text":"<pre><code>search_format(model_id)\n</code></pre> <p>Search for model name or filename in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name of filename of model.</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Format dict with chat template or None if none found.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef search_format(cls,\n                  model_id: str) -&gt; Union[dict,None]:\n    \"\"\"Search for model name or filename in the registry.\n\n    Args:\n        model_id: Name of filename of model.\n\n    Returns:\n        Format dict with chat template or None if none found.\n    \"\"\"\n\n    # Todo: cache compiled re patterns in \"_re\" entries\n\n    cls._ensure()\n\n    for name,val in cls.format_dir.items():\n        if isinstance(val, str): # a link: ignore when searching\n            continue\n        if \"match\" not in val:\n            continue\n\n        patterns = val[\"match\"]\n        if isinstance(patterns, str):\n            patterns = [patterns]\n\n        for pat in patterns:\n            if re.search(pat, model_id, flags=re.IGNORECASE):\n                logger.debug(f\"Format search for '{model_id}' found '{name}' entry\")\n                return cls._prepare_format_entry(name, val)\n\n    return None\n</code></pre>"},{"location":"api-reference/#sibila.Models.is_format_supported","title":"is_format_supported  <code>classmethod</code>","text":"<pre><code>is_format_supported(model_id)\n</code></pre> <p>Checks if there's template support for a model with this name.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model filename or general name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if Models knows the format.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef is_format_supported(cls,\n                        model_id: str) -&gt; bool:\n    \"\"\"Checks if there's template support for a model with this name.\n\n    Args:\n        model_id: Model filename or general name.\n\n    Returns:\n        True if Models knows the format.\n    \"\"\"\n    return cls.search_format(model_id) is not None\n</code></pre>"},{"location":"api-reference/#sibila.Models.add_format","title":"add_format  <code>classmethod</code>","text":"<pre><code>add_format(conf)\n</code></pre> <p>Add a JSON file or configuration dict to the format directory.</p> <p>Parameters:</p> Name Type Description Default <code>conf</code> <code>dict</code> <p>A dict with configuration as if loaded from JSON by json.loads(). Defaults to None.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef add_format(cls,\n               conf: dict,\n               ):\n    \"\"\"Add a JSON file or configuration dict to the format directory.\n\n    Args:\n        conf: A dict with configuration as if loaded from JSON by json.loads(). Defaults to None.\n    \"\"\"\n\n    cls._ensure()\n\n    cls.format_dir.update(conf)\n\n    cls._sanity_check_formats()\n</code></pre>"},{"location":"api-reference/#generation-configs","title":"Generation Configs","text":""},{"location":"api-reference/#sibila.GenConf","title":"GenConf  <code>dataclass</code>","text":"<p>Model generation configuration, used in Model.gen() and variants.</p>"},{"location":"api-reference/#sibila.GenConf.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens = 0\n</code></pre> <p>Max generated token length. 0 means all available up to output context size (which equals: model.ctx_len - in_prompt_len)</p>"},{"location":"api-reference/#sibila.GenConf.stop","title":"stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop = field(default_factory=list)\n</code></pre> <p>List of generation stop text sequences</p>"},{"location":"api-reference/#sibila.GenConf.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature = 0.0\n</code></pre> <p>Generation temperature. Use 0 to always pick the most probable output, without random sampling. Larger positive values will produce more random outputs.</p>"},{"location":"api-reference/#sibila.GenConf.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p = 0.9\n</code></pre> <p>Nucleus sampling top_p value. Only applies if temperature &gt; 0.</p>"},{"location":"api-reference/#sibila.GenConf.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format = 'text'\n</code></pre> <p>Output format: \"text\" or \"json\". For JSON output, text is validated as in json.loads(). Thread msgs must explicitly request JSON output or a warning will be emitted if string json not present (this is automatically done in Model.json() and related calls).</p>"},{"location":"api-reference/#sibila.GenConf.json_schema","title":"json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>json_schema = None\n</code></pre> <p>A JSON schema to validate the JSON output. Thread msgs must list the JSON schema and request its use; must also set the format to \"json\".</p>"},{"location":"api-reference/#sibila.GenConf.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Return a copy of the current GenConf updated with values in kwargs. Doesn't modify object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>update settings of the same names in the returned copy.</p> <code>{}</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key does not exist.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A copy of the current object with kwargs values updated. Doesn't modify object.</p> Source code in <code>sibila/gen.py</code> <pre><code>def __call__(self,\n             **kwargs: Any) -&gt; Self:\n    \"\"\"Return a copy of the current GenConf updated with values in kwargs. Doesn't modify object.\n\n    Args:\n        **kwargs: update settings of the same names in the returned copy.\n\n    Raises:\n        KeyError: If key does not exist.\n\n    Returns:\n        A copy of the current object with kwargs values updated. Doesn't modify object.\n    \"\"\"\n\n    ret = copy(self)\n\n    for k,v in kwargs.items():\n        if not hasattr(ret, k):\n            raise KeyError(f\"No such key '{k}'\")\n        setattr(ret, k,v)\n\n    return ret\n</code></pre>"},{"location":"api-reference/#sibila.GenConf.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a copy of this configuration.</p> Source code in <code>sibila/gen.py</code> <pre><code>def clone(self):\n    \"\"\"Return a copy of this configuration.\"\"\"\n    return copy(self)\n</code></pre>"},{"location":"api-reference/#sibila.GenConf.asdict","title":"asdict","text":"<pre><code>asdict()\n</code></pre> <p>Return GenConf as a dict.</p> Source code in <code>sibila/gen.py</code> <pre><code>def asdict(self):\n    \"\"\"Return GenConf as a dict.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api-reference/#sibila.JSchemaConf","title":"JSchemaConf  <code>dataclass</code>","text":"<p>Configuration for JSON schema massaging and validation.</p>"},{"location":"api-reference/#sibila.JSchemaConf.resolve_refs","title":"resolve_refs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resolve_refs = True\n</code></pre> <p>Set for $ref references to be resolved and replaced with actual definition.</p>"},{"location":"api-reference/#sibila.JSchemaConf.collapse_single_combines","title":"collapse_single_combines  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>collapse_single_combines = True\n</code></pre> <p>Any single-valued \"oneOf\"/\"anyOf\" is replaced with the actual value.</p>"},{"location":"api-reference/#sibila.JSchemaConf.description_from_title","title":"description_from_title  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description_from_title = 0\n</code></pre> <p>If a value doesn't have a description entry, make one from its title or name.</p> <ul> <li>0: don't make description from name</li> <li>1: copy title or name to description</li> <li>2: 1: + capitalize first letter and convert _ to space: class_label -&gt; \"class label\".</li> </ul>"},{"location":"api-reference/#sibila.JSchemaConf.force_all_required","title":"force_all_required  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_all_required = False\n</code></pre> <p>Force all entries in an object to be required (except removed defaults if remove_with_default=True).</p>"},{"location":"api-reference/#sibila.JSchemaConf.remove_with_default","title":"remove_with_default  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>remove_with_default = False\n</code></pre> <p>Delete any values that have a \"default\" annotation.</p>"},{"location":"api-reference/#sibila.JSchemaConf.default_to_last","title":"default_to_last  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_to_last = True\n</code></pre> <p>Move any default value entry into the last position of properties dict.</p>"},{"location":"api-reference/#sibila.JSchemaConf.additional_allowed_root_keys","title":"additional_allowed_root_keys  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_allowed_root_keys = field(default_factory=list)\n</code></pre> <p>By default only the following properties are allowed in schema's root:   description, properties, type, required, additionalProperties, allOf, anyOf, oneOf, not Add to this list to allow additional root properties.</p>"},{"location":"api-reference/#sibila.JSchemaConf.pydantic_strict_validation","title":"pydantic_strict_validation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pydantic_strict_validation = None\n</code></pre> <p>Validate JSON values in a strict manner or not. None means validate individually per each value in the obj. (for example in pydantic with: Field(strict=True)).</p>"},{"location":"api-reference/#sibila.JSchemaConf.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a copy of this configuration.</p> Source code in <code>sibila/json_schema.py</code> <pre><code>def clone(self):\n    \"\"\"Return a copy of this configuration.\"\"\"\n    return copy(self)\n</code></pre>"},{"location":"api-reference/#generation-results-and-errors","title":"Generation Results and Errors","text":""},{"location":"api-reference/#sibila.GenRes","title":"GenRes","text":"<p>Model generation result.</p>"},{"location":"api-reference/#sibila.GenRes.OK_STOP","title":"OK_STOP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OK_STOP = 1\n</code></pre> <p>Generation complete without errors.</p>"},{"location":"api-reference/#sibila.GenRes.OK_LENGTH","title":"OK_LENGTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OK_LENGTH = 0\n</code></pre> <p>Generation stopped due to reaching max_tokens.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_JSON","title":"ERROR_JSON  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON = -1\n</code></pre> <p>Invalid JSON: this is often due to the model returning OK_LENGTH (finished due to max_tokens reached), which cuts off the JSON text.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_JSON_SCHEMA_VAL","title":"ERROR_JSON_SCHEMA_VAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON_SCHEMA_VAL = -2\n</code></pre> <p>Failed JSON schema validation.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_JSON_SCHEMA_ERROR","title":"ERROR_JSON_SCHEMA_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON_SCHEMA_ERROR = -2\n</code></pre> <p>JSON schema itself is not valid.</p>"},{"location":"api-reference/#sibila.GenRes.ERROR_MODEL","title":"ERROR_MODEL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_MODEL = -3\n</code></pre> <p>Other model internal error.</p>"},{"location":"api-reference/#sibila.GenRes.from_finish_reason","title":"from_finish_reason  <code>staticmethod</code>","text":"<pre><code>from_finish_reason(finish)\n</code></pre> <p>Convert a ChatCompletion finish result into a GenRes.</p> <p>Parameters:</p> Name Type Description Default <code>finish</code> <code>str</code> <p>ChatCompletion finish result.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A GenRes result.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef from_finish_reason(finish: str) -&gt; Any: # Any=GenRes\n    \"\"\"Convert a ChatCompletion finish result into a GenRes.\n\n    Args:\n        finish: ChatCompletion finish result.\n\n    Returns:\n        A GenRes result.\n    \"\"\"\n    if finish == 'stop':\n        return GenRes.OK_STOP\n    elif finish == 'length':\n        return GenRes.OK_LENGTH\n    elif finish == '!json':\n        return GenRes.ERROR_JSON\n    elif finish == '!json_schema_val':\n        return GenRes.ERROR_JSON_SCHEMA_VAL\n    elif finish == '!json_schema_error':\n        return GenRes.ERROR_JSON_SCHEMA_ERROR\n    else:\n        return GenRes.ERROR_MODEL\n</code></pre>"},{"location":"api-reference/#sibila.GenRes.as_text","title":"as_text  <code>staticmethod</code>","text":"<pre><code>as_text(res)\n</code></pre> <p>Returns a friendlier description of the result.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Any</code> <p>Model output result.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown GenRes.</p> <p>Returns:</p> Type Description <code>str</code> <p>A friendlier description of the GenRes.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef as_text(res: Any) -&gt; str: # Any=GenRes\n    \"\"\"Returns a friendlier description of the result.\n\n    Args:\n        res: Model output result.\n\n    Raises:\n        ValueError: If unknown GenRes.\n\n    Returns:\n        A friendlier description of the GenRes.\n    \"\"\"\n\n    if res == GenRes.OK_STOP:\n        return \"Stop\"\n    elif res == GenRes.OK_LENGTH:\n        return \"Length (output cut)\"\n    elif res == GenRes.ERROR_JSON:\n        return \"JSON decoding error\"\n\n    elif res == GenRes.ERROR_JSON_SCHEMA_VAL:\n        return \"JSON SCHEMA validation error\"\n    elif res == GenRes.ERROR_JSON_SCHEMA_ERROR:\n        return \"Error in JSON SCHEMA\"\n\n    elif res == GenRes.ERROR_MODEL:\n        return \"Model internal error\"\n    else:\n        raise ValueError(\"Bad/unknow GenRes\")\n</code></pre>"},{"location":"api-reference/#sibila.GenError","title":"GenError","text":"<pre><code>GenError(out)\n</code></pre> <p>Model generation exception, raised when the model was unable to return a response.</p> <p>An error has happened during model generation.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>GenOut</code> <p>Model output</p> required Source code in <code>sibila/gen.py</code> <pre><code>def __init__(self, \n             out: GenOut):\n    \"\"\"An error has happened during model generation.\n\n    Args:\n        out: Model output\n    \"\"\"\n\n    assert out.res != GenRes.OK_STOP, \"OK_STOP is not an error\"      \n\n    super().__init__()\n\n    self.res = out.res\n    self.text = out.text\n    self.dic = out.dic\n    self.value = out.value\n</code></pre>"},{"location":"api-reference/#sibila.GenError.raise_if_error","title":"raise_if_error  <code>staticmethod</code>","text":"<pre><code>raise_if_error(out, ok_length_is_error)\n</code></pre> <p>Raise an exception if the model returned an error</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>GenOut</code> <p>Model returned info.</p> required <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error?</p> required <p>Raises:</p> Type Description <code>GenError</code> <p>If an error was returned by model.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef raise_if_error(out: GenOut,\n                   ok_length_is_error: bool):\n    \"\"\"Raise an exception if the model returned an error\n\n    Args:\n        out: Model returned info.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error?\n\n    Raises:\n        GenError: If an error was returned by model.\n    \"\"\"\n\n    if out.res != GenRes.OK_STOP:\n        if out.res == GenRes.OK_LENGTH and not ok_length_is_error:\n            return # OK_LENGTH to not be considered an error\n\n        raise GenError(out)\n</code></pre>"},{"location":"api-reference/#sibila.GenOut","title":"GenOut  <code>dataclass</code>","text":"<p>Model output, returned by gen_extract(), gen_json() and other model calls that don't raise exceptions.</p>"},{"location":"api-reference/#sibila.GenOut.res","title":"res  <code>instance-attribute</code>","text":"<pre><code>res\n</code></pre> <p>Result of model generation.</p>"},{"location":"api-reference/#sibila.GenOut.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text\n</code></pre> <p>Text generated by model.</p>"},{"location":"api-reference/#sibila.GenOut.dic","title":"dic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dic = None\n</code></pre> <p>Python dictionary, output by the structured calls like gen_json().</p>"},{"location":"api-reference/#sibila.GenOut.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value = None\n</code></pre> <p>Initialized instance value, dataclass or Pydantic BaseModel object, as returned in calls like extract().</p>"},{"location":"api-reference/#sibila.GenOut.asdict","title":"asdict","text":"<pre><code>asdict()\n</code></pre> <p>Return GenOut as a dict.</p> Source code in <code>sibila/gen.py</code> <pre><code>def asdict(self):\n    \"\"\"Return GenOut as a dict.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api-reference/#sibila.GenOut.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>sibila/gen.py</code> <pre><code>def __str__(self):\n    out = f\"Error={self.res.as_text(self.res)} text=\u2588{self.text}\u2588\"\n    if self.dic is not None:\n        out += f\" dic={self.dic}\"\n    if self.value is not None:\n        out += f\" value={self.value}\"\n    return out\n</code></pre>"},{"location":"api-reference/#messages-threads-context","title":"Messages, Threads, Context","text":""},{"location":"api-reference/#sibila.MsgKind","title":"MsgKind","text":"<p>Enumeration for kinds of messages in a Thread.</p>"},{"location":"api-reference/#sibila.MsgKind.IN","title":"IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN = 0\n</code></pre> <p>Input message, from user.</p>"},{"location":"api-reference/#sibila.MsgKind.OUT","title":"OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUT = 1\n</code></pre> <p>Model output message.</p>"},{"location":"api-reference/#sibila.MsgKind.INST","title":"INST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INST = 2\n</code></pre> <p>Initial instructions for model.</p>"},{"location":"api-reference/#sibila.Thread","title":"Thread","text":"<pre><code>Thread(t=None, inst='', join_sep='\\n')\n</code></pre> <p>A sequence of messages alternating between IN (\"user\" role) and OUT (\"assistant\" role).</p> <p>Stores a special initial INST information (known as \"system\" role in ChatML) providing instructions to the model. Some models don't use system instructions - in those cases it's prepended to first IN message.</p> <p>Messages are kept in a strict IN,OUT,IN,OUT,... order. To enforce this, if two IN messages are added, the second just appends to the text of the first.</p> <p>Examples:</p> <p>Creation with message list</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")],\n...             inst=\"Be helpful.\")\n&gt;&gt;&gt; print(th)\ninst=\u2588Be helpful.\u2588, sep='\\n', len=2\n0: IN=\u2588Hello model!\u2588\n1: OUT=\u2588Hello there human!\u2588\n</code></pre> <p>Adding messages</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread(inst=\"Be helpful.\")\n&gt;&gt;&gt; th.add(MsgKind.IN, \"Can you teach me how to cook?\")\n&gt;&gt;&gt; th.add_IN(\"I mean really cook as a chef?\") # gets appended\n&gt;&gt;&gt; print(th)\ninst=\u2588Be helpful.\u2588, sep='\\n', len=1\n0: IN=\u2588Can you teach me how to cook?\\nI mean really cook as a chef?\u2588\n</code></pre> <p>Another way to add a message</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread(inst=\"Be informative.\")\n&gt;&gt;&gt; th.add_IN(\"Tell me about kangaroos, please?\")\n&gt;&gt;&gt; th += \"They are so impressive.\" # appends text to last message\n&gt;&gt;&gt; print(th)\ninst=\u2588Be informative.\u2588, sep='\\n', len=1\n0: IN=\u2588Tell me about kangaroos, please?\\nThey are so impressive.\u2588\n</code></pre> <p>As a ChatML message list</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")], \n...             inst=\"Be helpful.\")\n&gt;&gt;&gt; th.as_chatml()\n[{'role': 'system', 'content': 'Be helpful.'},\n {'role': 'user', 'content': 'Hello model!'},\n {'role': 'assistant', 'content': 'Hello there human!'}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Any, list, str, dict, tuple]]</code> <p>Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.</p> <code>None</code> <code>join_sep</code> <code>str</code> <p>Separator used when message text needs to be joined. Defaults to \"\\n\".</p> <code>'\\n'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>On invalid args passed.</p> Source code in <code>sibila/thread.py</code> <pre><code>def __init__(self,\n             t: Optional[Union[Any,list,str,dict,tuple]] = None, # Any=Thread\n             inst: str = '',\n             join_sep: str = \"\\n\"):\n    \"\"\"\n    Examples:\n        Creation with message list\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")],\n        ...             inst=\"Be helpful.\")\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be helpful.\u2588, sep='\\\\n', len=2\n        0: IN=\u2588Hello model!\u2588\n        1: OUT=\u2588Hello there human!\u2588\n\n        Adding messages\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread(inst=\"Be helpful.\")\n        &gt;&gt;&gt; th.add(MsgKind.IN, \"Can you teach me how to cook?\")\n        &gt;&gt;&gt; th.add_IN(\"I mean really cook as a chef?\") # gets appended\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be helpful.\u2588, sep='\\\\n', len=1\n        0: IN=\u2588Can you teach me how to cook?\\\\nI mean really cook as a chef?\u2588\n\n        Another way to add a message\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread(inst=\"Be informative.\")\n        &gt;&gt;&gt; th.add_IN(\"Tell me about kangaroos, please?\")\n        &gt;&gt;&gt; th += \"They are so impressive.\" # appends text to last message\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be informative.\u2588, sep='\\\\n', len=1\n        0: IN=\u2588Tell me about kangaroos, please?\\\\nThey are so impressive.\u2588\n\n        As a ChatML message list\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")], \n        ...             inst=\"Be helpful.\")\n        &gt;&gt;&gt; th.as_chatml()\n        [{'role': 'system', 'content': 'Be helpful.'},\n         {'role': 'user', 'content': 'Hello model!'},\n         {'role': 'assistant', 'content': 'Hello there human!'}]\n\n    Args:\n        t: Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.\n        join_sep: Separator used when message text needs to be joined. Defaults to \"\\\\n\".\n\n    Raises:\n        TypeError: On invalid args passed.\n    \"\"\"\n\n    if isinstance(t, Thread):\n        self._msgs = t._msgs.copy()\n        self.inst = t.inst\n        self.join_sep = t.join_sep\n    else:\n        self._msgs = []\n        self.inst = inst\n        self.join_sep = join_sep\n\n        if t is not None:\n            self.concat(t)\n</code></pre>"},{"location":"api-reference/#sibila.Thread.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Delete all messages and clear inst.</p> Source code in <code>sibila/thread.py</code> <pre><code>def clear(self):\n    \"\"\"Delete all messages and clear inst.\"\"\"\n    self.inst = \"\"\n    self._msgs = []\n</code></pre>"},{"location":"api-reference/#sibila.Thread.last_kind","title":"last_kind  <code>property</code>","text":"<pre><code>last_kind\n</code></pre> <p>Get kind of last message in thread .</p> <p>Returns:</p> Type Description <code>MsgKind</code> <p>Kind of last message or MsgKind.IN if empty.</p>"},{"location":"api-reference/#sibila.Thread.last_text","title":"last_text  <code>property</code>","text":"<pre><code>last_text\n</code></pre> <p>Get text of last message in thread .</p> <p>Returns:</p> Type Description <code>str</code> <p>Last message text.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If thread is empty.</p>"},{"location":"api-reference/#sibila.Thread.inst","title":"inst  <code>instance-attribute</code>","text":"<pre><code>inst\n</code></pre> <p>Text for system instructions, defaults to empty string</p>"},{"location":"api-reference/#sibila.Thread.add","title":"add","text":"<pre><code>add(t, text=None)\n</code></pre> <p>Add a message to Thread by parsing a mix of types.</p> <p>Accepts any of these argument combinations:</p> <ul> <li>t=MsgKind, text=str</li> <li>t=str, text=None -&gt; uses last thread message's MsgKind</li> <li>(MsgKind, text)</li> <li>{\"kind\": \"...\", text: \"...\"}</li> <li>{\"role\": \"...\", content: \"...\"} - ChatML format</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[str, tuple, dict, MsgKind]</code> <p>One of the accepted types listed above.</p> required <code>text</code> <code>Optional[str]</code> <p>Message text if first type is MsgKind. Defaults to None.</p> <code>None</code> Source code in <code>sibila/thread.py</code> <pre><code>def add(self, \n        t: Union[str,tuple,dict,MsgKind],\n        text: Optional[str] = None):\n    \"\"\"Add a message to Thread by parsing a mix of types.\n\n    Accepts any of these argument combinations:\n\n    - t=MsgKind, text=str\n    - t=str, text=None -&gt; uses last thread message's MsgKind\n    - (MsgKind, text)\n    - {\"kind\": \"...\", text: \"...\"}\n    - {\"role\": \"...\", content: \"...\"} - ChatML format\n\n    Args:\n        t: One of the accepted types listed above.\n        text: Message text if first type is MsgKind. Defaults to None.\n    \"\"\"\n\n    kind, text = self._parse_msg(t, text)\n\n    if kind == MsgKind.INST:\n        self.inst = self.join_text(self.inst, text)\n    else:\n        if kind == self.last_kind and len(self._msgs):\n            self._msgs[-1] = self.join_text(self._msgs[-1], text)\n        else:\n            self._msgs.append(text) # in new kind\n</code></pre>"},{"location":"api-reference/#sibila.Thread.addx","title":"addx","text":"<pre><code>addx(path=None, text=None, kind=None)\n</code></pre> <p>Add message with text from a supplied arg or loaded from a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>If given, text is loaded from an UTF-8 file in this path. Defaults to None.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>If given, text is added. Defaults to None.</p> <code>None</code> <code>kind</code> <code>Optional[MsgKind]</code> <p>MsgKind of message. If not given or the same as last thread message, it's appended to it. Defaults to None.</p> <code>None</code> Source code in <code>sibila/thread.py</code> <pre><code>def addx(self, \n         path: Optional[str] = None, \n         text: Optional[str] = None,\n         kind: Optional[MsgKind] = None):\n    \"\"\"Add message with text from a supplied arg or loaded from a path.\n\n    Args:\n        path: If given, text is loaded from an UTF-8 file in this path. Defaults to None.\n        text: If given, text is added. Defaults to None.\n        kind: MsgKind of message. If not given or the same as last thread message, it's appended to it. Defaults to None.\n    \"\"\"\n\n    assert (path is not None) ^ (text is not None), \"Only one of path or text\"\n\n    if path is not None:\n        with open(path, 'r', encoding=\"utf-8\") as f:\n            text = f.read()\n\n    if kind is None: # use last message role, so that it gets appended\n        kind = self.last_kind\n\n    self.add(kind, text)\n</code></pre>"},{"location":"api-reference/#sibila.Thread.get_text","title":"get_text","text":"<pre><code>get_text(index)\n</code></pre> <p>Return text for message at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Message index. Use -1 to get inst value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Message text at index.</p> Source code in <code>sibila/thread.py</code> <pre><code>def get_text(self,\n             index: int) -&gt; str:\n    \"\"\"Return text for message at index.\n\n    Args:\n        index: Message index. Use -1 to get inst value.\n\n    Returns:\n        Message text at index.\n    \"\"\"        \n    if index == -1:\n        return self.inst\n    else:\n        return self._msgs[index]\n</code></pre>"},{"location":"api-reference/#sibila.Thread.set_text","title":"set_text","text":"<pre><code>set_text(index, text)\n</code></pre> <p>Set text for message at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Message index. Use -1 to set inst value.</p> required <code>text</code> <code>str</code> <p>Text to replace in message at index.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def set_text(self,\n             index: int,\n             text: str):        \n    \"\"\"Set text for message at index.\n\n    Args:\n        index: Message index. Use -1 to set inst value.\n        text: Text to replace in message at index.\n    \"\"\"\n    if index == -1:\n        self.inst = text\n    else:\n        self._msgs[index] = text\n</code></pre>"},{"location":"api-reference/#sibila.Thread.concat","title":"concat","text":"<pre><code>concat(t)\n</code></pre> <p>Concatenate a Thread or list of messages to the current Thread.</p> <p>Take care that the other list starts with an IN message, therefore,  if last message in self is also an IN kind, their text will be joined as in add().</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Self, list, str, dict, tuple]]</code> <p>A Thread or a list of messages. Otherwise a single message as in add().</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If bad arg types provided.</p> Source code in <code>sibila/thread.py</code> <pre><code>def concat(self,\n           t: Optional[Union[Self,list,str,dict,tuple]]):\n    \"\"\"Concatenate a Thread or list of messages to the current Thread.\n\n    Take care that the other list starts with an IN message, therefore, \n    if last message in self is also an IN kind, their text will be joined as in add().\n\n    Args:\n        t: A Thread or a list of messages. Otherwise a single message as in add().\n\n    Raises:\n        TypeError: If bad arg types provided.\n    \"\"\"\n    if isinstance(t, Thread):\n        for msg in t:\n            self.add(msg)\n        self.inst = self.join_text(self.inst, t.inst)\n\n    elif isinstance(t, list): # message list\n        for msg in t:\n            self.add(msg)\n\n    elif isinstance(t, str) or isinstance(t, dict) or isinstance(t, tuple): # single message\n        self.add(t)\n\n    else:\n        raise TypeError(\"Arg t must be: Thread --or-- list[messages] --or-- an str, tuple or dict single message.\")\n</code></pre>"},{"location":"api-reference/#sibila.Thread.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Load this Thread from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to load.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def load(self,\n         path: str):\n    \"\"\"Load this Thread from a JSON file.\n\n    Args:\n        path: Path of file to load.\n    \"\"\"\n\n    with open(path, 'r', encoding='utf-8') as f:\n        js = f.read()\n    state = json.loads(js)\n\n    self._msgs = state[\"_msgs\"]\n    self.inst = state[\"inst\"]\n    self.join_sep = state[\"join_sep\"]\n</code></pre>"},{"location":"api-reference/#sibila.Thread.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Serialize this Thread to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to save into.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def save(self,\n         path: str):\n    \"\"\"Serialize this Thread to JSON.\n\n    Args:\n        path: Path of file to save into.\n    \"\"\"\n\n    state = {\"_msgs\": self._msgs,\n             \"inst\": self.inst,\n             \"join_sep\": self.join_sep\n             }\n\n    json_str = json.dumps(state, indent=2, default=vars)\n\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(json_str)\n</code></pre>"},{"location":"api-reference/#sibila.Thread.msg_as_chatml","title":"msg_as_chatml","text":"<pre><code>msg_as_chatml(index)\n</code></pre> <p>Returns message in a ChatML dict.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the message to return.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A ChatML dict with \"role\" and \"content\" keys.</p> Source code in <code>sibila/thread.py</code> <pre><code>def msg_as_chatml(self,\n                  index: int) -&gt; dict:\n    \"\"\"Returns message in a ChatML dict.\n\n    Args:\n        index: Index of the message to return.\n\n    Returns:\n        A ChatML dict with \"role\" and \"content\" keys.\n    \"\"\"\n    kind = Thread._kind_from_pos(index)\n    role = MsgKind.chatml_role_from_kind(kind)\n    text = self._msgs[index] if index &gt;= 0 else self.inst\n    return {\"role\": role, \"content\": text}\n</code></pre>"},{"location":"api-reference/#sibila.Thread.as_chatml","title":"as_chatml","text":"<pre><code>as_chatml()\n</code></pre> <p>Returns Thread as a list of ChatML messages.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of ChatML dict elements with \"role\" and \"content\" keys.</p> Source code in <code>sibila/thread.py</code> <pre><code>def as_chatml(self) -&gt; list[dict]:\n    \"\"\"Returns Thread as a list of ChatML messages.\n\n    Returns:\n        A list of ChatML dict elements with \"role\" and \"content\" keys.\n    \"\"\"\n    msgs = []\n\n    for index,msg in enumerate(self._msgs):\n        if index == 0 and self.inst:\n            msgs.append(self.msg_as_chatml(-1))\n        msgs.append(self.msg_as_chatml(index))\n\n    return msgs\n</code></pre>"},{"location":"api-reference/#sibila.Thread.has_text_lower","title":"has_text_lower","text":"<pre><code>has_text_lower(text_lower)\n</code></pre> <p>Can the lowercase text be found in one of the messages?</p> <p>Parameters:</p> Name Type Description Default <code>text_lower</code> <code>str</code> <p>The lowercase text to search for in messages.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if such text was found.</p> Source code in <code>sibila/thread.py</code> <pre><code>def has_text_lower(self,\n                   text_lower: str) -&gt; bool:\n    \"\"\"Can the lowercase text be found in one of the messages?\n\n    Args:\n        text_lower: The lowercase text to search for in messages.\n\n    Returns:\n        True if such text was found.\n    \"\"\"\n    for msg in self._msgs:\n        if text_lower in msg.lower():\n            return True\n\n    return False        \n</code></pre>"},{"location":"api-reference/#sibila.Trim","title":"Trim","text":"<p>Flags for Thread trimming.</p>"},{"location":"api-reference/#sibila.Trim.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre> <p>No trimming.</p>"},{"location":"api-reference/#sibila.Trim.INST","title":"INST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INST = 1\n</code></pre> <p>Can remove INST message.</p>"},{"location":"api-reference/#sibila.Trim.IN","title":"IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN = 2\n</code></pre> <p>Can remove IN messages.</p>"},{"location":"api-reference/#sibila.Trim.OUT","title":"OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUT = 4\n</code></pre> <p>Can remove OUT messages.</p>"},{"location":"api-reference/#sibila.Trim.KEEP_FIRST_IN","title":"KEEP_FIRST_IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEEP_FIRST_IN = 1024\n</code></pre> <p>If trimming IN messages, never remove first one.</p>"},{"location":"api-reference/#sibila.Trim.KEEP_FIRST_OUT","title":"KEEP_FIRST_OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEEP_FIRST_OUT = 2048\n</code></pre> <p>If trimming OUT messages, never remove first one.</p>"},{"location":"api-reference/#sibila.Context","title":"Context","text":"<pre><code>Context(\n    t=None,\n    max_token_len=None,\n    pinned_inst_text=\"\",\n    join_sep=\"\\n\",\n)\n</code></pre> <p>A class based on Thread that manages total token length, so that it's kept under a certain value. Also supports a persistent inst (instructions) text.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Thread, list, str, dict, tuple]]</code> <p>Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.</p> <code>None</code> <code>max_token_len</code> <code>Optional[int]</code> <p>Maximum token count to use when trimming. Defaults to None, which will use max model context length.</p> <code>None</code> <code>pinned_inst_text</code> <code>str</code> <p>Pinned inst text which survives clear(). Defaults to \"\".</p> <code>''</code> <code>join_sep</code> <code>str</code> <p>Separator used when message text needs to be joined. Defaults to \"\\n\".</p> <code>'\\n'</code> Source code in <code>sibila/context.py</code> <pre><code>def __init__(self,                 \n             t: Optional[Union[Thread,list,str,dict,tuple]] = None, \n             max_token_len: Optional[int] = None,        \n             pinned_inst_text: str = \"\",\n             join_sep: str = \"\\n\"):\n    \"\"\"\n    Args:\n        t: Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.\n        max_token_len: Maximum token count to use when trimming. Defaults to None, which will use max model context length.\n        pinned_inst_text: Pinned inst text which survives clear(). Defaults to \"\".\n        join_sep: Separator used when message text needs to be joined. Defaults to \"\\\\n\".\n    \"\"\"\n\n    super().__init__(t,\n                     inst=pinned_inst_text,\n                     join_sep=join_sep)\n\n    self.max_token_len = max_token_len\n\n    self.pinned_inst_text = pinned_inst_text\n</code></pre>"},{"location":"api-reference/#sibila.Context.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Delete all messages but reset inst to a pinned text if any.</p> Source code in <code>sibila/context.py</code> <pre><code>def clear(self):\n    \"\"\"Delete all messages but reset inst to a pinned text if any.\"\"\"\n    super().clear()        \n    if self.pinned_inst_text is not None:\n        self.inst = self.pinned_inst_text\n</code></pre>"},{"location":"api-reference/#sibila.Context.trim","title":"trim","text":"<pre><code>trim(trim_flags, model, *, max_token_len=None)\n</code></pre> <p>Trim context by selectively removing older messages until thread fits max_token_len.</p> <p>Parameters:</p> Name Type Description Default <code>trim_flags</code> <code>Trim</code> <p>Flags to guide selection of which messages to remove.</p> required <code>model</code> <code>Model</code> <p>Model that will process the thread.</p> required <code>max_token_len</code> <code>Optional[int]</code> <p>Cut messages until size is lower than this number. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to trim anything.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if any context trimming occurred.</p> Source code in <code>sibila/context.py</code> <pre><code>def trim(self,\n         trim_flags: Trim,\n         model: Model,\n         *,\n         max_token_len: Optional[int] = None,\n         ) -&gt; bool:\n    \"\"\"Trim context by selectively removing older messages until thread fits max_token_len.\n\n    Args:\n        trim_flags: Flags to guide selection of which messages to remove.\n        model: Model that will process the thread.\n        max_token_len: Cut messages until size is lower than this number. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to trim anything.\n\n    Returns:\n        True if any context trimming occurred.\n    \"\"\"\n\n    if max_token_len is None:\n        max_token_len = self.max_token_len\n\n    if max_token_len is None:\n        max_token_len = model.ctx_len\n\n    # if genconf is None:\n    #     genconf = model.genconf            \n    # assert max_token_len &lt; model.ctx_len, f\"max_token_len ({max_token_len}) must be &lt; model's context size ({model.ctx_len}) - genconf.max_new_tokens\"\n\n    if trim_flags == Trim.NONE: # no trimming\n        return False\n\n    thread = self.clone()\n\n    any_trim = False\n\n    while True:\n\n        curr_len = model.token_len(thread)\n\n        if curr_len &lt;= max_token_len:\n            break\n\n        logger.debug(f\"len={curr_len} / max={max_token_len}\")\n\n        if self.inst and trim_flags &amp; Trim.INST:\n            self.inst = ''\n            any_trim = True\n            logger.debug(f\"Cutting INST {self.inst[:80]} (...)\")\n            continue\n\n        # cut first possible message, starting from oldest first ones\n        trimmed = False\n        in_index = out_index = 0\n\n        for index,m in enumerate(thread):\n            kind,text = m\n\n            if kind == MsgKind.IN:\n                if trim_flags &amp; Trim.IN:\n                    if not (trim_flags &amp; Trim.KEEP_FIRST_IN and in_index == 0):\n                        del thread[index]\n                        trimmed = True\n                        logger.debug(f\"Cutting IN {text[:80]} (...)\")\n                        break\n                in_index += 1\n\n            elif kind == MsgKind.OUT:\n                if trim_flags &amp; Trim.OUT:                        \n                    if not (trim_flags &amp; Trim.KEEP_FIRST_OUT and out_index == 0):\n                        del thread[index]\n                        trimmed = True\n                        logger.debug(f\"Cutting OUT {text[:80]} (...)\")\n                        break\n                out_index += 1\n\n        if not trimmed:\n            # all thread messages were cycled but not a single could be cut, so size remains the same\n            # arriving here we did all we could for trim_flags but could not remove any more\n            raise RuntimeError(\"Unable to trim anything out of thread\")\n        else:\n            any_trim = True\n\n    # while end\n\n\n    if any_trim:\n        self._msgs = thread._msgs\n\n    return any_trim\n</code></pre>"},{"location":"api-reference/#tools","title":"Tools","text":""},{"location":"api-reference/#sibila.tools","title":"tools","text":"<p>Tools for model interaction, summarization, etc.</p> <ul> <li>interact(): Interact with model as in a chat, using input().</li> <li>loop(): Iteratively append inputs and generate model outputs.</li> <li>recursive_summarize(): Recursively summarize a (large) text or text file.</li> </ul>"},{"location":"api-reference/#sibila.tools.interact","title":"interact","text":"<pre><code>interact(\n    model,\n    *,\n    ctx=None,\n    inst_text=None,\n    trim_flags=TRIM_DEFAULT,\n    genconf=None\n)\n</code></pre> <p>Interact with model as in a chat, using input().</p> <p>Includes a list of commands: type !? to see help.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to use for generating.</p> required <code>ctx</code> <code>Optional[Context]</code> <p>Optional input Context. Defaults to None.</p> <code>None</code> <code>inst_text</code> <code>Optional[str]</code> <p>text for Thread instructions. Defaults to None.</p> <code>None</code> <code>trim_flags</code> <code>Trim</code> <p>Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.</p> <code>TRIM_DEFAULT</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, defaults to model's.    </p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context after all the interactions.</p> Source code in <code>sibila/tools.py</code> <pre><code>def interact(model: Model,\n             *,\n             ctx: Optional[Context] = None,\n             inst_text: Optional[str] = None,\n             trim_flags: Trim = TRIM_DEFAULT,\n\n             genconf: Optional[GenConf] = None,\n             ) -&gt; Context:\n    \"\"\"Interact with model as in a chat, using input().\n\n    Includes a list of commands: type !? to see help.\n\n    Args:\n        model: Model to use for generating.\n        ctx: Optional input Context. Defaults to None.\n        inst_text: text for Thread instructions. Defaults to None.\n        trim_flags: Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.\n        genconf: Model generation configuration. Defaults to None, defaults to model's.    \n\n    Returns:\n        Context after all the interactions.\n    \"\"\"\n\n    def callback(out: Union[GenOut,None], \n                 ctx: Context, \n                 model: Model,\n                 genconf: GenConf) -&gt; bool:\n\n        if out is not None:\n            if out.res != GenRes.OK_STOP:\n                print(f\"***Result={GenRes.as_text(out.res)}***\")\n\n            if out.text:\n                text = out.text\n            else:\n                text = \"***No text out***\"\n\n            ctx.add_OUT(text)\n            print(text)\n            print()\n\n\n        def print_thread_info():\n            if ctx.max_token_len is not None: # use from ctx\n                max_token_len = ctx.max_token_len\n            else: # assume max possible for model context and genconf\n                max_token_len = model.ctx_len - genconf.max_tokens\n\n            length = model.token_len(ctx, genconf)\n            print(f\"Thread token len={length}, max len before next gen={max_token_len}\")\n\n\n\n        # input loop ===============================================\n        MARKER: str = '\"\"\"'\n        multiline: str = \"\"\n\n        while True:\n\n            user = input('&gt;').strip()\n\n            if multiline:\n                if user.endswith(MARKER):\n                    user = multiline + \"\\n\" + user[:-3]\n                    multiline = \"\"\n                else:\n                    multiline += \"\\n\" + user\n                    continue\n\n            else:\n                if not user:\n                    return False # terminate loop\n\n                elif user.startswith(MARKER):\n                    multiline = user[3:]\n                    continue\n\n                elif user.endswith(\"\\\\\"):\n                    user = user[:-1]\n                    user = user.replace(\"\\\\n\", \"\\n\")\n                    ctx.add_IN(user)\n                    continue\n\n                elif user.startswith(\"!\"): # a command\n                    params = user[1:].split(\"=\")\n                    cmd = params[0]\n                    params = params[1:]\n\n                    if cmd == \"inst\":\n                        ctx.clear()\n                        if params:\n                            text = params[0].replace(\"\\\\n\", \"\\n\")\n                            ctx.inst = text\n\n                    elif cmd == \"add\" or cmd == \"a\":\n                        if params:\n                            try:\n                                path = params[0]\n                                ctx.addx(path=path)\n                                ct = ctx.last_text\n                                print(ct[:500])\n                            except FileNotFoundError:\n                                print(f\"Could not load '{path}'\")\n                        else:\n                            print(\"Path needed\")\n\n                    elif cmd == 'c':\n                        print_thread_info()\n                        print(ctx)\n\n                    elif cmd == 'cl':\n                        if not params:\n                            params.append(\"ctx.json\")\n                        try:\n                            ctx.load(params[0])\n                            print(f\"Loaded context from {params[0]}\")\n                        except FileNotFoundError:\n                            print(f\"Could not load '{params[0]}'\")\n\n                    elif cmd == 'cs':\n                        if not params:\n                            params.append(\"ctx.json\")\n                        ctx.save(params[0])\n                        print(f\"Saved context to {params[0]}\")\n\n                    elif cmd == 'tl':\n                        print_thread_info()\n\n                    elif cmd == 'i':\n                        print(f\"Model:\\n{model.info()}\")\n                        print(f\"GenConf:\\n{genconf}\\n\")\n\n                        print_thread_info()\n\n                    # elif cmd == 'p':\n                    #     print(model.text_from_turns(ctx.turns))\n\n                    # elif cmd == 'to':\n                    #     token_ids = model.tokens_from_turns(ctx.turns)\n                    #     print(f\"Prompt tokens={token_ids}\")\n\n\n                    else:\n                        print(f\"Unknown command '!{cmd}' - known commands:\\n\"\n                              \" !inst[=text] - clear messages and add inst (system) message\\n\"\n                              \" !add|!a=path - load file and add to last msg\\n\"\n                              \" !c - list context msgs\\n\"\n                              \" !cl=path - load context (default=ctx.json)\\n\"\n                              \" !cs=path - save context (default=ctx.json)\\n\"\n                              \" !tl - thread's token length\\n\"\n                              \" !i - model and genconf info\\n\"\n                              ' Delimit with \"\"\" for multiline begin/end or terminate line with \\\\ to continue into a new line\\n'\n                              \" Empty line + enter to quit\"\n                              )\n                        # \" !p - show formatted prompt (if model supports it)\\n\"\n                        # \" !to - prompt's tokens\\n\"\n\n                    print()\n                    continue\n\n            # we have a user prompt\n            user = user.replace(\"\\\\n\", \"\\n\")\n            break\n\n\n        ctx.add_IN(user)\n\n        return True # continue looping\n\n\n\n    # start prompt loop\n    ctx = loop(callback,\n               model,\n\n               ctx=ctx,\n               inst_text=inst_text,\n               in_text=None, # call callback for first prompt\n               trim_flags=trim_flags)\n\n    return ctx\n</code></pre>"},{"location":"api-reference/#sibila.tools.loop","title":"loop","text":"<pre><code>loop(\n    callback,\n    model,\n    *,\n    inst_text=None,\n    in_text=None,\n    trim_flags=TRIM_DEFAULT,\n    ctx=None,\n    genconf=None\n)\n</code></pre> <p>Iteratively append inputs and generate model outputs.</p> <p>Callback should call ctx.add_OUT(), ctx.add_IN() and return a bool to continue looping or not.</p> <p>If last Thread msg is not MsgKind.IN, callback() will be called with out_text=None.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Union[GenOut, None], Context, Model, GenConf], bool]</code> <p>A function(out, ctx, model) that will be iteratively called with model's output.</p> required <code>model</code> <code>Model</code> <p>Model to use for generating.</p> required <code>inst_text</code> <code>Optional[str]</code> <p>text for Thread instructions. Defaults to None.</p> <code>None</code> <code>in_text</code> <code>Optional[str]</code> <p>Text for Thread's initial MsgKind.IN. Defaults to None.</p> <code>None</code> <code>trim_flags</code> <code>Trim</code> <p>Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.</p> <code>TRIM_DEFAULT</code> <code>ctx</code> <code>Optional[Context]</code> <p>Optional input Context. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, defaults to model's.</p> <code>None</code> Source code in <code>sibila/tools.py</code> <pre><code>def loop(callback: Callable[[Union[GenOut,None], Context, Model, GenConf], bool],\n         model: Model,\n         *,\n         inst_text: Optional[str] = None,\n         in_text: Optional[str] = None,\n\n         trim_flags: Trim = TRIM_DEFAULT,\n         ctx: Optional[Context] = None,\n\n         genconf: Optional[GenConf] = None,\n         ) -&gt; Context:\n    \"\"\"Iteratively append inputs and generate model outputs.\n\n    Callback should call ctx.add_OUT(), ctx.add_IN() and return a bool to continue looping or not.\n\n    If last Thread msg is not MsgKind.IN, callback() will be called with out_text=None.\n\n    Args:\n        callback: A function(out, ctx, model) that will be iteratively called with model's output.\n        model: Model to use for generating.\n        inst_text: text for Thread instructions. Defaults to None.\n        in_text: Text for Thread's initial MsgKind.IN. Defaults to None.\n        trim_flags: Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.\n        ctx: Optional input Context. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, defaults to model's.\n    \"\"\"\n\n    if ctx is None:\n        ctx = Context()\n    else:\n        ctx = ctx\n\n    if inst_text is not None:\n        ctx.inst = inst_text\n    if in_text is not None:\n        ctx.add_IN(in_text)\n\n    if genconf is None:\n        genconf = model.genconf\n\n    if ctx.max_token_len is not None: # use from ctx\n        max_token_len = ctx.max_token_len\n    else: # assume max possible for model context and genconf\n        max_token_len = model.ctx_len - genconf.max_tokens\n\n\n    while True:\n\n        if len(ctx) and ctx.last_kind == MsgKind.IN:\n            # last is an IN message: we can trim and generate\n\n            ctx.trim(trim_flags,\n                     model,\n                     max_token_len=max_token_len\n                     )\n\n            out = model.gen(ctx, genconf)\n        else:\n            out = None # first call\n\n        res = callback(out, \n                       ctx, \n                       model,\n                       genconf)\n\n        if not res:\n            break\n\n\n    return ctx\n</code></pre>"},{"location":"api-reference/#sibila.tools.recursive_summarize","title":"recursive_summarize","text":"<pre><code>recursive_summarize(\n    model, text=None, path=None, overlap_size=20\n)\n</code></pre> <p>Recursively summarize a (large) text or text file.</p> <p>Works by:</p> <ol> <li>Breaking text into chunks that fit models context.</li> <li>Run model to summarize chunks.</li> <li>Join generated summaries and jump to 1. - do this until text size no longer decreases.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to use for summarizing.</p> required <code>text</code> <code>Optional[str]</code> <p>Initial text.</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>--Or-- A path to an UTF-8 text file.</p> <code>None</code> <code>overlap_size</code> <code>int</code> <p>Size in model tokens of the overlapping portions at beginning and end of chunks.</p> <code>20</code> <p>Returns:</p> Type Description <code>str</code> <p>The summarized text.</p> Source code in <code>sibila/tools.py</code> <pre><code>def recursive_summarize(model: Model,\n                        text: Optional[str] = None,\n                        path: Optional[str] = None,\n                        overlap_size: int = 20) -&gt; str:\n    \"\"\"Recursively summarize a (large) text or text file.\n\n    Works by:\n\n    1. Breaking text into chunks that fit models context.\n    2. Run model to summarize chunks.\n    3. Join generated summaries and jump to 1. - do this until text size no longer decreases.\n\n    Args:\n        model: Model to use for summarizing.\n        text: Initial text.\n        path: --Or-- A path to an UTF-8 text file.\n        overlap_size: Size in model tokens of the overlapping portions at beginning and end of chunks.\n\n    Returns:\n        The summarized text.\n    \"\"\"\n\n    if (text is not None) + (path is not None) != 1:\n        raise ValueError(\"Only one of text or path can be given\")\n\n    if path is not None:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n\n    inst_text = \"\"\"Your task is to do short summaries of text.\"\"\"\n    in_text = \"Summarize the following text:\\n\"\n    ctx = Context(pinned_inst_text=inst_text)\n\n    # split initial text\n    max_token_len = model.ctx_len - model.genconf.max_tokens - (model.tokenizer.token_len(inst_text + in_text) + 16) \n    logger.debug(f\"Max ctx token len {max_token_len}\")\n\n    token_len_fn = model.tokenizer.token_len_lambda\n    logger.debug(f\"Initial text token_len {token_len_fn(text)}\") # type: ignore[arg-type]\n\n    spl = RecursiveTextSplitter(max_token_len, overlap_size, len_fn=token_len_fn)\n\n    round = 0\n    while True: # summarization rounds\n        logger.debug(f\"Round {round} {'='*60}\")\n\n        in_list = spl.split(text=text)\n        in_len = sum([len(t) for t in in_list])\n\n        logger.debug(f\"Split in {len(in_list)} parts, total len {in_len} chars\")\n\n        out_list = []\n        for i,t in enumerate(in_list):\n\n            logger.debug(f\"{round}&gt;{i} {'='*30}\")\n\n            ctx.clear()\n            ctx.add_IN(in_text)\n            ctx.add_IN(t)\n\n            out = model.gen(ctx)        \n            logger.debug(out)\n\n            out_list.append(out.text)\n\n        text = \"\\n\".join(out_list)\n\n        out_len = len(text) # sum([len(t) for t in out_list])\n        if out_len &gt;= in_len:\n            break\n        elif len(out_list) == 1:\n            break\n        else:\n            round += 1\n\n    return text\n</code></pre>"},{"location":"api-reference/#multigen","title":"Multigen","text":""},{"location":"api-reference/#sibila.multigen","title":"multigen","text":"<p>Functions for comparing output across models.</p> <ul> <li>thread_multigen(), query_multigen() and multigen(): Compare outputs across models.</li> <li>cycle_gen_print(): For a list of models, sequentially grow a Thread with model responses to given IN messages.</li> </ul>"},{"location":"api-reference/#sibila.multigen.thread_multigen","title":"thread_multigen","text":"<pre><code>thread_multigen(\n    threads,\n    model_names,\n    text=None,\n    csv=None,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"value\"],\n    thread_titles=None,\n)\n</code></pre> <p>Generate a single thread on a list of models, returning/saving results in text/CSV.</p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>list[Thread]</code> <p>List of threads to input into each model.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of Models names.</p> required <code>text</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.</p> <code>None</code> <code>csv</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.</p> <code>None</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>list[str]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].</p> <code>['text', 'dic', 'value']</code> <code>thread_titles</code> <code>Optional[list[str]]</code> <p>A human-friendly title for each Thread. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def thread_multigen(threads: list[Thread],\n                    model_names: list[str],\n\n                    text: Union[str,list[str],None] = None,\n                    csv: Union[str,list[str],None] = None,\n\n                    gencall: Optional[Callable] = None,                   \n                    genconf: Optional[GenConf] = None,\n\n                    out_keys: list[str] = [\"text\",\"dic\", \"value\"],\n\n                    thread_titles: Optional[list[str]] = None                   \n                    ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate a single thread on a list of models, returning/saving results in text/CSV.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        threads: List of threads to input into each model.\n        model_names: A list of Models names.\n        text: An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.\n        csv: An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].\n        thread_titles: A human-friendly title for each Thread. Defaults to None.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"\n\n    assert isinstance(model_names, list), \"model_names must be a list of strings\"\n\n    table = multigen(threads,\n                     model_names=model_names, \n                     gencall=gencall,\n                     genconf=genconf)\n\n    # table[threads,models]\n\n    if thread_titles is None:\n        thread_titles = [str(th) for th in threads]\n\n    def format(format_fn, cmds):\n        if cmds is None or not cmds:\n            return\n\n        f = StringIO(newline='')\n\n        format_fn(f,\n                  table, \n                  title_list=thread_titles,\n                  model_names=model_names,\n                  out_keys=out_keys)\n        fmtd = f.getvalue()\n\n        if not isinstance(cmds, list):\n            cmds = [cmds]\n        for c in cmds:\n            if c == 'print':\n                print(fmtd)\n            else: # path\n                with open(c, \"w\", encoding=\"utf-8\") as f:\n                    f.write(fmtd)\n\n    format(format_text, text)\n    format(format_csv, csv)\n\n    return table\n</code></pre>"},{"location":"api-reference/#sibila.multigen.query_multigen","title":"query_multigen","text":"<pre><code>query_multigen(\n    in_list,\n    inst_text,\n    model_names,\n    text=None,\n    csv=None,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"value\"],\n    in_titles=None,\n)\n</code></pre> <p>Generate an INST+IN thread on a list of models, returning/saving results in text/CSV.</p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>in_list</code> <code>list[str]</code> <p>List of IN messages to initialize Threads.</p> required <code>inst_text</code> <code>str</code> <p>The common INST to use in all models.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of Models names.</p> required <code>text</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.</p> <code>None</code> <code>csv</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.</p> <code>None</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>list[str]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].</p> <code>['text', 'dic', 'value']</code> <code>in_titles</code> <code>Optional[list[str]]</code> <p>A human-friendly title for each Thread. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)).        </p> <code>list[list[GenOut]]</code> <p>For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def query_multigen(in_list: list[str],\n                   inst_text: str,                                                \n                   model_names: list[str],\n\n                   text: Union[str,list[str],None] = None, # \"print\", path\n                   csv: Union[str,list[str],None] = None, # \"print\", path\n\n                   gencall: Optional[Callable] = None,                   \n                   genconf: Optional[GenConf] = None,\n\n                   out_keys: list[str] = [\"text\",\"dic\", \"value\"],\n                   in_titles: Optional[list[str]] = None\n                   ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate an INST+IN thread on a list of models, returning/saving results in text/CSV.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        in_list: List of IN messages to initialize Threads.\n        inst_text: The common INST to use in all models.\n        model_names: A list of Models names.\n        text: An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.\n        csv: An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].\n        in_titles: A human-friendly title for each Thread. Defaults to None.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)).        \n        For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"    \n\n    th_list = []\n    for in_text in in_list:\n        th = Thread.make_INST_IN(inst_text, in_text)\n        th_list.append(th)\n\n    if in_titles is None:\n        in_titles = in_list\n\n    out = thread_multigen(th_list,                     \n                          model_names=model_names, \n                          text=text,\n                          csv=csv,\n                          gencall=gencall,\n                          genconf=genconf,\n                          out_keys=out_keys,\n                          thread_titles=in_titles)\n\n    return out\n</code></pre>"},{"location":"api-reference/#sibila.multigen.multigen","title":"multigen","text":"<pre><code>multigen(\n    threads,\n    *,\n    models=None,\n    model_names=None,\n    model_names_del_after=True,\n    gencall=None,\n    genconf=None\n)\n</code></pre> <p>Generate a list of Threads in multiple models, returning the GenOut for each [thread,model] combination.</p> Actual generation for each model is implemented by the gencall arg Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>list[Thread]</code> <p>List of threads to input into each model.</p> required <code>models</code> <code>Optional[list[Model]]</code> <p>A list of initialized models. Defaults to None.</p> <code>None</code> <code>model_names</code> <code>Optional[list[str]]</code> <p>--Or-- A list of Models names. Defaults to None.</p> <code>None</code> <code>model_names_del_after</code> <code>bool</code> <p>Delete model_names models after using them: important or an out-of-memory error will eventually happen. Defaults to True.</p> <code>True</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Only one of models or model_names can be given.</p> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def multigen(threads: list[Thread],\n             *,\n             models: Optional[list[Model]] = None, # existing models\n\n             model_names: Optional[list[str]] = None,\n             model_names_del_after: bool = True,\n\n             gencall: Optional[Callable] = None,\n             genconf: Optional[GenConf] = None\n             ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate a list of Threads in multiple models, returning the GenOut for each [thread,model] combination.\n\n    Actual generation for each model is implemented by the gencall arg Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        threads: List of threads to input into each model.\n        models: A list of initialized models. Defaults to None.\n        model_names: --Or-- A list of Models names. Defaults to None.\n        model_names_del_after: Delete model_names models after using them: important or an out-of-memory error will eventually happen. Defaults to True.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n\n    Raises:\n        ValueError: Only one of models or model_names can be given.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"\n\n    if not ((models is None) ^ ((model_names is None))):\n        raise ValueError(\"Only one of models or model_names can be given\")\n\n    if gencall is None:\n        gencall = _default_gencall_text\n\n    mod_count = len(models) if models is not None else len(model_names) # type: ignore[arg-type]\n\n    all_out = []\n\n    for i in range(mod_count):\n        if models is not None:\n            model = models[i]\n            logger.debug(f\"Model: {model.desc}\")\n        else:\n            name = model_names[i] # type: ignore[index]\n            model = Models.create(name)\n            logger.info(f\"Model: {name} -&gt; {model.desc}\")\n\n        mod_out = []\n        for th in threads:\n            out = gencall(model, th, genconf)\n\n            mod_out.append(out)\n\n        all_out.append(mod_out)\n\n        if model_names_del_after and models is None:\n            del model\n\n    # all_out is currently shaped (M,T) -&gt; transpose to (T,M), so that each row contains thread t for all models\n    tout = []\n    for t in range(len(threads)):\n        tmout = [] # thread t for all models\n        for m in range(mod_count):\n            tmout.append(all_out[m][t])\n\n        tout.append(tmout)\n\n    return tout\n</code></pre>"},{"location":"api-reference/#sibila.multigen.cycle_gen_print","title":"cycle_gen_print","text":"<pre><code>cycle_gen_print(\n    in_list,\n    inst_text,\n    model_names,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"value\"],\n    json_kwargs={\n        \"indent\": 2,\n        \"sort_keys\": False,\n        \"ensure_ascii\": False,\n    },\n)\n</code></pre> <p>For a list of models, sequentially grow a Thread with model responses to given IN messages and print the results.</p> <p>Works by doing:</p> <ol> <li>Generate an INST+IN prompt for a list of models. (Same INST for all).</li> <li>Append the output of each model to its own Thread.</li> <li>Append the next IN prompt and generate again. Back to 2.</li> </ol> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>in_list</code> <code>list[str]</code> <p>List of IN messages to initialize Threads.</p> required <code>inst_text</code> <code>str</code> <p>The common INST to use in all models.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of Models names.</p> required <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>list[str]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].</p> <code>['text', 'dic', 'value']</code> <code>json_kwargs</code> <code>dict</code> <p>JSON dumps() configuration. Defaults to {\"indent\": 2, \"sort_keys\": False, \"ensure_ascii\": False }.</p> <code>{'indent': 2, 'sort_keys': False, 'ensure_ascii': False}</code> Source code in <code>sibila/multigen.py</code> <pre><code>def cycle_gen_print(in_list: list[str],\n                    inst_text: str,                                                \n                    model_names: list[str],\n\n                    gencall: Optional[Callable] = None,                   \n                    genconf: Optional[GenConf] = None,\n\n                    out_keys: list[str] = [\"text\",\"dic\", \"value\"],\n\n                    json_kwargs: dict = {\"indent\": 2,\n                                         \"sort_keys\": False,\n                                         \"ensure_ascii\": False}\n                    ):\n    \"\"\"For a list of models, sequentially grow a Thread with model responses to given IN messages and print the results.\n\n    Works by doing:\n\n    1. Generate an INST+IN prompt for a list of models. (Same INST for all).\n    2. Append the output of each model to its own Thread.\n    3. Append the next IN prompt and generate again. Back to 2.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        in_list: List of IN messages to initialize Threads.\n        inst_text: The common INST to use in all models.\n        model_names: A list of Models names.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].\n        json_kwargs: JSON dumps() configuration. Defaults to {\"indent\": 2, \"sort_keys\": False, \"ensure_ascii\": False }.\n    \"\"\"\n\n    assert isinstance(model_names, list), \"model_names must be a list of strings\"\n\n    if gencall is None:\n        gencall = _default_gencall_text\n\n\n    n_model = len(model_names)\n    n_ins = len(in_list)\n\n    for m in range(n_model):\n\n        name = model_names[m]\n        model = Models.create(name)\n\n        print('=' * 80)\n        print(f\"Model: {name} -&gt; {model.desc}\")\n\n        th = Thread(inst=inst_text)\n\n        for i in range(n_ins):\n            in_text = in_list[i]\n            print(f\"IN: {in_text}\")\n\n            th += (MsgKind.IN, in_text)\n\n            out = gencall(model, th, genconf)\n\n            out_dict = out.asdict()\n\n            print(\"OUT\")\n\n            for k in out_keys:\n\n                if k in out_dict and out_dict[k] is not None:\n\n                    if k != out_keys[0]: # not first\n                        print(\"-\" * 20)\n\n                    val = nice_print(k, out_dict[k], json_kwargs)\n                    print(val)\n\n            th += (MsgKind.OUT, out.text)\n\n        del model\n</code></pre>"},{"location":"api-reference/#tokenizers","title":"Tokenizers","text":"<p>Tokenizers used in models.</p>"},{"location":"api-reference/#sibila.LlamaCppTokenizer","title":"LlamaCppTokenizer","text":"<pre><code>LlamaCppTokenizer(llama, reg_flags=None)\n</code></pre> <p>Tokenizer for llama.cpp loaded GGUF models.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def __init__(self, \n             llama: Llama, \n             reg_flags: Optional[str] = None):\n    self._llama = llama\n\n    self.vocab_size = self._llama.n_vocab()\n\n    self.bos_token_id = self._llama.token_bos()\n    self.bos_token = llama_token_get_text(self._llama.model, self.bos_token_id).decode(\"utf-8\")\n\n    self.eos_token_id = self._llama.token_eos()\n    self.eos_token = llama_token_get_text(self._llama.model, self.eos_token_id).decode(\"utf-8\")\n\n    self.pad_token_id = None\n    self.pad_token = None\n\n    self.unk_token_id = None # ? fill by taking a look at id 0?\n    self.unk_token = None\n\n    # workaround for https://github.com/ggerganov/llama.cpp/issues/4772\n    self._workaround1 = reg_flags is not None and \"llamacpp1\" in reg_flags\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppTokenizer.encode","title":"encode","text":"<pre><code>encode(text)\n</code></pre> <p>Encode text into model tokens. Inverse of Decode().</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of ints with the encoded tokens.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def encode(self, \n           text: str) -&gt; list[int]:\n    \"\"\"Encode text into model tokens. Inverse of Decode().\n\n    Args:\n        text: Text to be encoded.\n\n    Returns:\n        A list of ints with the encoded tokens.\n    \"\"\"\n\n    if self._workaround1:\n        # append a space after each bos and eos, so that llama's tokenizer matches HF\n        def space_post(text, s):\n            out = \"\"\n            while (index := text.find(s)) != -1:\n                after = index + len(s)\n                out += text[:after]\n                if text[after] != ' ':\n                    out += ' '\n                text = text[after:]\n\n            out += text\n            return out\n\n        text = space_post(text, self.bos_token)\n        text = space_post(text, self.eos_token)\n        # print(text)\n\n    # str -&gt; bytes\n    btext = text.encode(\"utf-8\", errors=\"ignore\")\n\n    return self._llama.tokenize(btext, add_bos=False, special=True)\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppTokenizer.decode","title":"decode","text":"<pre><code>decode(token_ids, skip_special=True)\n</code></pre> <p>Decode model tokens to text. Inverse of Encode().</p> <p>Using instead of llama-cpp-python's to fix error: remove first character after a bos only if it's a space.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>List of model tokens.</p> required <code>skip_special</code> <code>bool</code> <p>Don't decode special tokens like bos and eos. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded text.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def decode(self,\n           token_ids: list[int],\n           skip_special: bool = True) -&gt; str:\n    \"\"\"Decode model tokens to text. Inverse of Encode().\n\n    Using instead of llama-cpp-python's to fix error: remove first character after a bos only if it's a space.\n\n    Args:\n        token_ids: List of model tokens.\n        skip_special: Don't decode special tokens like bos and eos. Defaults to True.\n\n    Returns:\n        Decoded text.\n    \"\"\"\n\n    if not len(token_ids):\n        return \"\"\n\n    output = b\"\"\n    size = 32\n    buffer = (ctypes.c_char * size)()\n\n    if not skip_special:\n        special_toks = {self.bos_token_id: self.bos_token.encode(\"utf-8\"), # type: ignore[union-attr]\n                        self.eos_token_id: self.eos_token.encode(\"utf-8\")} # type: ignore[union-attr]\n\n        for token in token_ids:\n            if token == self.bos_token_id:\n                output += special_toks[token]\n            elif token == self.eos_token_id:\n                output += special_toks[token]\n            else:\n                n = llama_cpp.llama_token_to_piece(\n                    self._llama.model, llama_cpp.llama_token(token), buffer, size\n                )\n                output += bytes(buffer[:n]) # type: ignore[arg-type]\n\n    else: # skip special\n        for token in token_ids:\n            if token != self.bos_token_id and token != self.eos_token_id:\n                n = llama_cpp.llama_token_to_piece(\n                    self._llama.model, llama_cpp.llama_token(token), buffer, size\n                )\n                output += bytes(buffer[:n]) # type: ignore[arg-type]\n\n\n    # \"User code is responsible for removing the leading whitespace of the first non-BOS token when decoding multiple tokens.\"\n    if (# token_ids[0] != self.bos_token_id and # we also try cutting if first is bos to approximate HF tokenizer\n       len(output) and output[0] &lt;= 32 # 32 = ord(' ')\n       ):\n        output = output[1:]\n\n    return output.decode(\"utf-8\", errors=\"ignore\")\n</code></pre>"},{"location":"api-reference/#sibila.LlamaCppTokenizer.token_len","title":"token_len","text":"<pre><code>token_len(text)\n</code></pre> <p>Returns token length for given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be measured.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Token length for given text.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self, \n              text: str) -&gt; int:\n    \"\"\"Returns token length for given text.\n\n    Args:\n        text: Text to be measured.\n\n    Returns:\n        Token length for given text.\n    \"\"\"\n\n    tokens = self.encode(text)\n    return len(tokens)        \n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer","title":"OpenAITokenizer","text":"<pre><code>OpenAITokenizer(model)\n</code></pre> <p>Tokenizer for OpenAI models.</p> Source code in <code>sibila/openai.py</code> <pre><code>def __init__(self, \n             model: str\n             ):\n\n    if not has_tiktoken:\n        raise Exception(\"Please install tiktoken by running: pip install tiktoken\")\n\n    self._tok = tiktoken.encoding_for_model(model)\n\n    self.vocab_size = self._tok.n_vocab\n\n    self.bos_token_id = None\n    self.bos_token = None\n\n    self.eos_token_id = None\n    self.eos_token = None\n\n    self.pad_token_id = None\n    self.pad_token = None\n\n    self.unk_token_id = None\n    self.unk_token = None\n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer.encode","title":"encode","text":"<pre><code>encode(text)\n</code></pre> <p>Encode text into model tokens. Inverse of Decode().</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of ints with the encoded tokens.</p> Source code in <code>sibila/openai.py</code> <pre><code>def encode(self, \n           text: str) -&gt; list[int]:\n    \"\"\"Encode text into model tokens. Inverse of Decode().\n\n    Args:\n        text: Text to be encoded.\n\n    Returns:\n        A list of ints with the encoded tokens.\n    \"\"\"\n    return self._tok.encode(text)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer.decode","title":"decode","text":"<pre><code>decode(token_ids, skip_special=True)\n</code></pre> <p>Decode model tokens to text. Inverse of Encode().</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>List of model tokens.</p> required <code>skip_special</code> <code>bool</code> <p>Don't decode special tokens like bos and eos. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def decode(self, \n           token_ids: list[int],\n           skip_special: bool = True) -&gt; str:\n    \"\"\"Decode model tokens to text. Inverse of Encode().\n\n    Args:\n        token_ids: List of model tokens.\n        skip_special: Don't decode special tokens like bos and eos. Defaults to True.\n\n    Returns:\n        Decoded text.\n    \"\"\"\n    assert skip_special, \"OpenAITokenizer only supports skip_special=True\"\n\n    return self._tok.decode(token_ids)\n</code></pre>"},{"location":"api-reference/#sibila.OpenAITokenizer.token_len","title":"token_len","text":"<pre><code>token_len(text)\n</code></pre> <p>Returns token length for given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be measured.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Token length for given text.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self, \n              text: str) -&gt; int:\n    \"\"\"Returns token length for given text.\n\n    Args:\n        text: Text to be measured.\n\n    Returns:\n        Token length for given text.\n    \"\"\"\n\n    tokens = self.encode(text)\n    return len(tokens)        \n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Sibila requires Python 3.9+ and uses the llama-cpp-python package for local models and OpenAI's API to access remote models like GPT-4.</p> <p>You can run local models in a plain CPU, CUDA GPU or other accelerator supported by llama.cpp.</p> <p>For local hardware accelerated inference, to take advantage of CUDA, Metal, etc, make sure you install llamacpp-python with the right settings - see more info here.</p> <p>Install Sibila from PyPI by running:</p> <pre><code>pip install sibila\n</code></pre> <p>Alternatively you can install Sibila in edit mode by downloading the GitHub repository and running the following in the base folder of the repository:</p> <pre><code>pip install -e .\n</code></pre> <p>Either way you should now be able to use Sibila.</p>"},{"location":"getting-started/#using-open-ai-models","title":"Using OPEN AI models","text":"<p>To use an OpenAI remote model, you'll need a paid OpenAI account and its API key. You can explicitly pass this key when creating an OpenAIModel object but this is not a good security practice. A better way is to define an environment variable which the OpenAI API will use when needed.</p> <p>In Linux/Mac you can define this key by running: <pre><code>export OPENAI_API_KEY=\"...\"\n</code></pre></p> <p>And in Windows command prompt:</p> <pre><code>setx OPENAI_API_KEY \"...\"\n</code></pre> <p>Having set this variable with your OpenAI API key, you can run an \"Hello Model\" example :</p> <pre><code>from sibila import OpenAIModel, GenConf\n\n# model file from the models folder\nmodel_path = \"../../models/openchat-3.5-1210.Q4_K_M.gguf\"\n\n# make sure you set the environment variable named OPENAI_API_KEY with your API key.\n# create an OpenAI model with generation temperature=1\nmodel = OpenAIModel(\"gpt-4\",\n                    genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n                inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>This will generate a pirate response as seen below.</p>"},{"location":"getting-started/#using-local-models-in-llamacpp","title":"Using local models in llama.cpp","text":"<p>Sibila can use llama.cpp (via the llamacpp-python package) to load models from local GGUF format files. Since LLM model files are quite big, they are usually quantized so that each parameter occupies less than a byte. </p> <p>See Setup local models to learn how where to find these models and how to use them in Sibila, then return here to run the following script:</p> <pre><code>from sibila import LlamaCppModel, GenConf\n\n# model file from the models folder\nmodel_path = \"../../models/openchat-3.5-1210.Q4_K_M.gguf\"\n\n# create a LlamaCpp model\nmodel = LlamaCppModel(model_path,\n                        genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n                inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>The script is available here: hello_llamacpp.py</p>"},{"location":"getting-started/#arrr-answer","title":"Arrr-answer!","text":"<p>After running the above and/or OpenAI's script you'll receive the model's answer to your \"Hello there?\" - in arrr-style:</p> <pre><code>User: Hello there?\nModel: Ahoy, me hearty! How be it goin'? Me name's Captain Chatbot, and I be here to assist thee with whatever ye need! So, what can me crew and I do fer yer today? Arrr!\n</code></pre> <p>Which means Sibila is working. Check the examples.</p>"},{"location":"setup-local-models/","title":"How to Setup Local Models","text":"<p>Most current 7B quantized models are pretty capable for common data extraction tasks. Below we'll see how to find and setup local models for use with Sibila. If you only plan to use OpenAI remote models, this is not for you.</p>"},{"location":"setup-local-models/#default-model-used-in-the-examples-openchat","title":"Default model used in the examples: OpenChat","text":"<p>By default, most of the examples included with Sibila use OpenChat, a quantized 7B parameters model, that you can download from:</p> <p>https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF/blob/main/openchat-3.5-1210.Q4_K_M.gguf</p> <p>In this page, click \"download\" and save it into the \"models\" folder inside the Sibila project. It's a 4.4Gb download and can take some time.</p> <p>Once the file \"openchat-3.5-1210.Q4_K_M.gguf\" is placed in the \"models\" folder, you should be able to run the examples with this local model.</p> <p>But you can also search for and use other local models - keep reading to learn more.</p>"},{"location":"setup-local-models/#choose-the-model-chat-or-instruct-types","title":"Choose the model: chat or instruct types","text":"<p>Sibila can use models that were fine-tuned for chat or instruct purposes. These models work in user - assistant turns or messages and use a chat template to properly compose those messages to the format that the model was fine-tuned to.</p> <p>For example, the Llama2 model was released in two editions: a simple Llama2 text completion model and a Llama2-instruct model that was fine tuned for user-assistant turns. For Sibila you should always select chat or instruct versions of a model.</p> <p>But which model to choose? You can look at model benchmark scores in popular listing sites:</p> <ul> <li>https://llm.extractum.io/list/</li> <li>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</li> <li>https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</li> </ul>"},{"location":"setup-local-models/#find-a-quantized-version-of-the-model","title":"Find a quantized version of the model","text":"<p>Since Large Language Models are quite big, they are usually quantized so that each parameter occupies a little more than 4 bits or half a byte. </p> <p>Without quantization, a 7 billion parameters model would require 14Gb of memory (each parameter taking 16 bits) to load and a bit more during inference.</p> <p>With quantization techniques, a 7 billion parameters model can have a file size of only 4.4Gb (using about 50% more in memory - 6.8Gb), which makes it accessible to be ran in common GPUs or even in common RAM memory (albeit slower).</p> <p>Quantized models are stored in a file format popularized by llama.cpp, the GGUF format (which means GPT-Generated Unified Format). We're using llama.cpp to run local models, so we'll be needing GGUF files.</p> <p>A good place to find quantized models is in HuggingFace's model hub, particularly in the well-know TheBloke's (Tom Jobbins) area:</p> <p>https://huggingface.co/TheBloke</p> <p>TheBloke is very prolific in producing quality quantized versions of models, usually shortly after they are released.</p> <p>A good model that we'll be using for the examples is the 4 bit quantization of the OpenChat-3.5 model, which itself is a fine-tuning of Mistral-7b:</p> <p>https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF</p>"},{"location":"setup-local-models/#download-it-into-models-folder","title":"Download it into models/ folder","text":"<p>From HuggingFace, you can download the GGUF file (in this and other quantized models by TheBloke) by scrolling down to the \"Provided files\" section and clicking one of the links. Usually the files ending in \"Q4_K_M\" are very reasonable 4-bit quantizations.</p> <p>In this case you'll download the file \"openchat-3.5-1210.Q4_K_M.gguf\" - save it into the \"models\" folder inside Sibila.</p>"},{"location":"setup-local-models/#find-the-chat-template","title":"Find the chat template","text":"<p>Because these models were fine-tuned for chat or instruct interaction, they use a chat template, which is a Jinja2 format template that converts thread messages into text in the format that the model was trained on. These chat templates are similar to the following one for the ChatML format:</p> <pre><code>{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n</code></pre> <p>When ran over a message list with system, user and model messages, the template produces text like the following: <pre><code>&lt;|im_start|&gt;system\nYou speak like a pirate.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello there?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nAhoy there matey! How can I assist ye today on this here ship o' mine?&lt;|im_end|&gt;\n</code></pre></p> <p>Specific chat templates are needed for the best results when dealing with each model. Sibila uses a singleton class named Models, that tries to automatically detect which template to use with a model, either from the model name or from embedded metadata, if available. This information is stored in JSON files named \"base_formats.json\". The base formats configuration file, which contains several well-known templates is available in \"sibila/base_formats.json\", and you can add your own templates as needed into other JSON configuration files.</p> <p>So, how to find the chat template for a new model you intend to use? Chances are, that it's already included in the GGUF model file and you don't need to do anything. If you receive an error when creating the local model, you should look for mentions of the used chat template in its information page and add it to a \"formats.json\" file in the models folder.</p> <p>If the chat template is not included in the GGUF file, you can look for a file named \"tokenizer_config.json\" in the main model files. This file should include an entry named \"chat_template\" which is what we want. For example in OpenChat's tokenizer_config.json:</p> <p>https://huggingface.co/openchat/openchat-3.5-1210/blob/main/tokenizer_config.json</p> <p>You'll find this line with the template:</p> <pre><code>{\n    \"...\": \"...\",\n\n    \"chat_template\": \"{{ bos_token }}{% for message in messages %}...{% endif %}\",\n\n    \"...\": \"...\"\n}\n</code></pre> <p>(Don't be confused by the text \"GPT4 correct...\", it's just the text format the model was trained on, and it's not related with OpenAI's)</p> <p>With this text string, we could create an entry in a \"formats.json\" file and all further models with this name will then use the template.</p>"},{"location":"setup-local-models/#use-the-model-directly","title":"Use the model directly","text":"<p>You can create the model by passing its filename to LlamaCppModel. Suppose we wanted to use the Nous Hermes 2 Solar 10, we would download the file and:</p> <pre><code>model = LlamaCppModel(\"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\")\n</code></pre> <p>If automatic detection doesn't work and you receive an error that the chat template format is unknown: if you know the proper format name (\"chatml\" in this case), you can pass it in the format parameter:</p> <pre><code>model = LlamaCppModel(\"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\",\n                      format=\"chatml\")\n</code></pre> <p>Or if you know the chat template definition, you can also pass it in the format argument:</p> <pre><code>chat_template = \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '&lt;|end_of_turn|&gt;'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\"\n\nmodel = LlamaCppModel(\"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\",\n                      format=chat_template)\n</code></pre> <p>But most of the time, Sibila should automatically detect the used format from the model's filename.</p>"},{"location":"setup-local-models/#use-the-model-with-the-models-class","title":"Use the model with the Models class","text":"<p>For continued use, it's a better idea for to create an entry for the model in the Models singleton, instead of manually creating a LlamaCppModel object that loads a model file - this allows future model changing to be much easier.</p> <p>Inside the \"models\" folder you'll find the file \"models.json\". The idea is that you can use this file to configure all files in its folder and the file can be added to Models' configuration by including this line in your scripts:</p> <pre><code>Models.setup(\"../../models\")\n</code></pre> <p>This will register all the defined entries in Models. For the \"nous-hermes-2-solar\" model above that uses \"chatml\" we could add this line to \"models.json\":</p> <pre><code>\"nous-hermes-solar\": {\n    \"name\": \"nous-hermes-2-solar-10.7b.Q4_K_M.gguf\",\n    \"format\": \"chatml\"\n}\n</code></pre> <p>The \"name\" key specifies the filename, \"format\" the formats entry that it should use.</p> <p>And we can use then use the model by simply doing:</p> <pre><code>model = Models.create(\"llamacpp:nous-hermes-solar\")\n</code></pre> <p>Note the \"provider:model_name\" format above, where llamacpp is the provider and \"nous-hermes-solar\" is the entry name we created above in Models.</p> <p>To be more flexible, Sibila also allows you to use the model filename directly, without setting up an entry in Models, like this:</p> <pre><code>model = Models.create(\"llamacpp:nous-hermes-2-solar-10.7b.Q4_K_M.gguf\")\n</code></pre> <p>Note that after \"llamacpp:\", instead of the model name we're directly passing the filename. If you plan to use a model for a while, creating an entry in Models is more flexible.</p>"},{"location":"setup-local-models/#out-of-memory-running-local-models","title":"Out of memory running local models","text":"<p>An important thing to know if you'll be using local models is about \"Out of memory\" errors.</p> <p>A 7B model like OpenChat-3.5, when quantized to 4 bits will occupy about 6.8 Gb of memory, in either GPU's VRAM or common RAM. If you try to run a second model at the same time, you might get an out of memory error and/or llama.cpp may crash.</p> <p>This is less of a problem when running scripts from the command line, but in environments like Jupyter where you can have multiple open notebooks, you may get python kernel errors like:</p> <pre><code>Kernel Restarting\nThe kernel for sibila/examples/name.ipynb appears to have died. It will restart automatically.\n</code></pre> <p>If you get an error like this in JupyterLab, open the Kernel menu and select \"Shut Down All Kernels...\". This will get rid of any out-of-memory stuck models.</p> <p>A good practice is to delete any model after you no longer need it or right before loading a new one. A simple \"del model\" works fine, or you can add these two lines before creating a model:</p> <pre><code>try: del model\nexcept: ...\n\nmodel = LlamaCppModel(...)\n</code></pre> <p>This way, any existing model in the current notebook is deleted before creating a new one.</p> <p>However this won't work in across multiple notebooks. In those cases, open JupyterLab's Kernel menu and select \"Shut Down All Kernels...\". This will get rid of any models currently in memory.</p>"},{"location":"tips/","title":"Tips and Tricks","text":"<p>Some general tips from experience with constrained model output with Sibila.</p>"},{"location":"tips/#split-entities-into-separate-classes","title":"Split entities into separate classes","text":"<p>Suppose you want to extract a list of person names from a group. You could use the following class:</p> <pre><code>class Group(BaseModel):\n    persons: list[str] = Field(description=\"List of persons\")\n    group_info: str\n\nout = model.extract(Group, in_text)\n</code></pre> <p>But it tends to work better to separate the Person entity into its own class and leave the list in Group:</p> <pre><code>class Person(BaseModel):\n    name: str\n\nclass Group(BaseModel):\n    persons: list[Person]\n    group_info: str\n\nout = model.extract(Group, in_text)\n</code></pre> <p>The same applies to the equivalent dataclass definitions.</p> <p>Adding descriptions seems to always help, specially for non-trivial extraction. Without descriptions, the model can only look into variable names for clues on what's wanted, so it's important to tell it what we want by adding field descriptions.</p>"}]}