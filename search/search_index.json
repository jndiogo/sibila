{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sibila","text":"<p>Extract structured data from remote or local LLM models. Predictable output is important for serious use of LLMs.</p> <ul> <li>Query structured data into Pydantic objects, dataclasses or simple types.</li> <li>Access remote models from OpenAI, Anthropic, Mistral AI and other providers.</li> <li>Use local models like Llama-3, Phi-3, OpenChat or any other GGUF file model.</li> <li>Besides structured extraction, Sibila is also a general purpose model access library, to generate plain text or free JSON results, with the same API for local and remote models.</li> <li>Model management: download models, manage configuration, quickly switch between models.</li> </ul> <p>No matter how well you craft a prompt begging a model for the output you need, it can always respond something else. Extracting structured data can be a big step into getting predictable behavior from your models.</p> <p>See What can you do with Sibila?</p> <p>To extract structured data from a local model:</p> <pre><code>from sibila import Models\nfrom pydantic import BaseModel\n\nclass Info(BaseModel):\n    event_year: int\n    first_name: str\n    last_name: str\n    age_at_the_time: int\n    nationality: str\n\nmodel = Models.create(\"llamacpp:openchat\")\n\nmodel.extract(Info, \"Who was the first man in the moon?\")\n</code></pre> <p>Returns an instance of class Info, created from the model's output:</p> <pre><code>Info(event_year=1969,\n     first_name='Neil',\n     last_name='Armstrong',\n     age_at_the_time=38,\n     nationality='American')\n</code></pre> <p>Or to use a remote model like OpenAI's GPT-4, we would simply replace the model's name:</p> <pre><code>model = Models.create(\"openai:gpt-4\")\n\nmodel.extract(Info, \"Who was the first man in the moon?\")\n</code></pre> <p>If Pydantic BaseModel objects are too much for your project, Sibila supports similar functionality with Python dataclass. Also includes asynchronous access to remote models.</p>"},{"location":"async/","title":"Asynchronous use","text":"<p>All the model calls like extract(), classify, json() etc, are also available in an asynchronous version of the same name but ending in _async, for example extract_async(). For example:</p> <p>Example</p> <pre><code>import asyncio\n\nfrom sibila import Models\n\nmodel = Models.create(\"openai:gpt-4\")\n\nasync def extract_names():    \n    return await model.extract_async(list[str],\n                                     \"Generate 20 English names with first name and surname\")\n\nasync def classify_spam():\n    return await model.classify_async([\"spam\", \"not spam\"],\n                                      \"I am a Nigerian prince and will make you very rich!\")\n\nasync def run_tasks():\n    tasks = [extract_names(), classify_spam()]\n    for task in asyncio.as_completed(tasks):\n            res = await task\n            print(\"Result:\", res)\n\nasyncio.run(run_tasks()) # or in Jupyter: await run_tasks()\n</code></pre> <p>Result</p> <pre><code>Result: spam\nResult: ['John Smith', 'Emily Johnson', 'Michael Brown', 'Jessica Williams', \n'David Jones', 'Sarah Davis', 'Daniel Miller', 'Laura Wilson', 'James Taylor', \n'Sophia Anderson', 'Christopher Thomas', 'Emma Thompson', 'Joseph White', \n'Olivia Lewis', 'Andrew Harris', 'Isabella Clark', 'Matthew Robinson', \n'Ava Hall', 'Ethan Allen', 'Mia Wright']\n</code></pre> <p>The first result, with only one of two tokens generated is quickly fetched from the model, while the 20 generated names take a while and arrive later. See the Async example to play with the above code.</p> <p>Asynchronous access has many advantages when parallel requests are needed, allowing responses to be handled as soon as they are ready, instead of sequentially sending and waiting for the model responses.</p>"},{"location":"async/#local-llamacpp-models","title":"Local llama.cpp models","text":"<p>Using LlamaCppModel objects to generate locally does not benefit from async functionality, because the local models must already be loaded in memory and can't benefit from asynchronous IO loading. When the async class methods are used with LlamaCppModel, inference will end up being made sequentially.</p>"},{"location":"first_run/","title":"First run","text":""},{"location":"first_run/#with-a-remote-model","title":"With a remote model","text":"<p>Sibila can use remote models from OpenAI, Anthropic, Mistral and other providers. For example, to use an OpenAI remote model, you'll need a paid OpenAI account and its API key. You can explicitly pass this key in your script but this is a poor security practice. </p> <p>A better way is to define an environment variable which the OpenAI API will use when needed:</p> Linux and MacWindows <pre><code>export OPENAI_API_KEY=\"...\"\n</code></pre> <pre><code>setx OPENAI_API_KEY \"...\"\n</code></pre> <p>Having set this variable with your OpenAI API key, you can run a \"Hello Model\" like this:</p> <p>Example</p> <pre><code>from sibila import OpenAIModel, GenConf\n\n# make sure you set the environment variable named OPENAI_API_KEY with your API key.\n# create an OpenAI model with generation temperature=1\nmodel = OpenAIModel(\"gpt-4\",\n                    genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n                inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>Result</p> <pre><code>User: Hello there?\nModel: Ahoy there, matey! What can this old sea dog do fer ye today?\n</code></pre> <p>You're all set if you only plan to use remote OpenAI models.</p>"},{"location":"first_run/#with-a-local-model","title":"With a local model","text":"<p>Local models run from files in GGUF format which are loaded run by the llama.cpp component.</p> <p>You'll need to download a GGUF model file: we suggest OpenChat 3.5 - an excellent 7B parameters quantized model that will run in less thant 7Gb of memory. </p> <p>To download the OpenChat model file, please see Download OpenChat model.</p> <p>After downloading the file, you can run this \"Hello Model\" script:</p> <p>Example</p> <pre><code>from sibila import LlamaCppModel, GenConf\n\n# model file from the models folder - change if different:\nmodel_path = \"../../models/openchat-3.5-1210.Q4_K_M.gguf\"\n\n# create a LlamaCpp model\nmodel = LlamaCppModel(model_path,\n                      genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n            inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>Result</p> <pre><code>User: Hello there?\nModel: Ahoy there matey! How can I assist ye today on this here ship o' mine?\nIs it be treasure you seek or maybe some tales from the sea?\nLet me know, and we'll set sail together!\n</code></pre> <p>If the above scripts output similar pirate talk, Sibila should be working fine.</p>"},{"location":"installing/","title":"Installing","text":""},{"location":"installing/#installation","title":"Installation","text":"<p>Sibila requires Python 3.9+ and uses the llama-cpp-python package for local models and OpenAI/Mistral/other libraries to access remote models.</p> <p>Install Sibila from PyPI by running:</p> <pre><code>pip install --upgrade sibila\n</code></pre> <p>If you only plan to use remote models (OpenAI), there's nothing else you need to do. See First Run to get it going.</p> Installation in edit mode <p>Alternatively you can install Sibila in edit mode by downloading the GitHub repository and running the following in the base folder of the repository:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"installing/#enabling-llamacpp-hardware-acceleration-for-local-models","title":"Enabling llama.cpp hardware acceleration for local models","text":"<p>Local models will run faster with hardware acceleration enabled. Sibila uses llama-cpp-python, a python wrapper for llama.cpp and it's a good idea to make sure it was installed with the best optimization your computer can offer. </p> <p>See the following sections: depending on which hardware you have, you can run the listed command which will reinstall llama-cpp-python with the selected optimization. If any error occurs you can always install the non-accelerated version, as listed at the end. </p>"},{"location":"installing/#for-cuda-nvidia-gpus","title":"For CUDA - NVIDIA GPUs","text":"<p>For CUDA acceleration in NVIDIA GPUs, you'll need to have the NVIDIA CUDA Toolkit installed. If looking for a specific CUDA version, see here.</p> LinuxWindows <p><pre><code>CMAKE_ARGS=\"-DLLAMA_CUDA=on\" \\\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre> The CUDA toolkit can also be installed from your Linux distro's package manager (e.g. apt install nvidia-cuda-toolkit).</p> <p><pre><code>$env:CMAKE_ARGS = \"-DLLAMA_CUDA=on\"\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre> Installing llama-cpp-python with NVIDIA GPU Acceleration on Windows: A Short Guide</p> <p>More info: Installing llama-cpp-python with GPU Support.</p>"},{"location":"installing/#for-metal-apple-silicon-macs","title":"For Metal - Apple silicon macs","text":"Mac <pre><code>CMAKE_ARGS=\"-DLLAMA_METAL=on\" \\\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre>"},{"location":"installing/#for-rocm-amd-gpus","title":"For ROCm AMD GPUS","text":"Linux and MacWindows <pre><code>CMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" \\\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre> <pre><code>$env:CMAKE_ARGS = \"-DLLAMA_HIPBLAS=on\"\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre>"},{"location":"installing/#for-vulkan-supporting-gpus","title":"For Vulkan supporting GPUs","text":"Linux and MacWindows <pre><code>CMAKE_ARGS=\"-DLLAMA_VULKAN=on\" \\\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre> <pre><code>$env:CMAKE_ARGS = \"-DLLAMA_VULKAN=on\"\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre>"},{"location":"installing/#cpu-acceleration-if-none-of-the-above","title":"CPU acceleration (if none of the above)","text":"Linux and MacWindows <pre><code>CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" \\\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre> <pre><code>$env:CMAKE_ARGS = \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre> <p>If you get an error running the above commands, please see llama-cpp-python's Installation configuration.</p>"},{"location":"installing/#non-accelerated","title":"Non-accelerated","text":"<p>In any case, you can always install llama-cpp-python without acceleration by running:</p> <pre><code>pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n</code></pre>"},{"location":"tips/","title":"Tips and Tricks","text":"<p>Some general tips from experience with constrained model output in Sibila.</p>"},{"location":"tips/#temperature","title":"Temperature","text":"<p>Sibila aims at exact results, so generation temperature defaults to 0. You should get the same results from the same model at all times.</p> <p>For \"creative\" outputs, you can set the temperature to a non-zero value. This is done in GenConf, which can be passed in many places, for example during actual generation/extraction:</p> <p>Example</p> <pre><code>from sibila import (Models, GenConf)\n\nModels.setup(\"../models\")\n\nmodel = Models.create(\"llamacpp:openchat\") # default GenConf could be passed here\n\nfor i in range(10):\n    print(model.extract(int,\n                \"Think of a random number between 1 and 100\",\n                genconf=GenConf(temperature=2.)))\n</code></pre> <p>Result</p> <pre><code>72\n78\n75\n68\n39\n47\n53\n82\n72\n63\n</code></pre>"},{"location":"tips/#deterministic-outputs","title":"Deterministic outputs","text":"<p>With temperature=0 and given a certain seed in GenConf, we should always get the same output for a fixed input prompt to a certain model. </p> <p>From what we've observed, in practice, when extracting structured data, you'll find variation inside free-form str fields, where the model is not being constrained. Other types like numbers will rarely see variating outputs.</p>"},{"location":"tips/#openai-models","title":"OpenAI models","text":"<p>In the OpenAI API link below, about \"Reproducible outputs\" you can read:</p> <pre><code>\"To receive (mostly) deterministic outputs across API calls, you can...\"\n\n\"There is a small chance that responses differ even when request parameters and system_fingerprint match, due to the inherent non-determinism of our models.\"\n</code></pre> <p>As far as logic goes, \"mostly deterministic\" and \"inherent non-determinism\" means not deterministic, so it seems you you can't have it in these models.</p> <p>https://platform.openai.com/docs/guides/text-generation/reproducible-outputs</p> <p>https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter</p>"},{"location":"tips/#local-llamacpp-models","title":"Local llama.cpp models","text":"<p>Some hardware accelerators like NVIDIA CUDA GPUS sacrifice determinism for better inference speed.</p> <p>You can find more information in these two links:</p> <p>https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility</p> <p>https://github.com/ggerganov/llama.cpp/issues/1340</p> <p>This happens inside CUDA hardware and is not related with the seed number you set in GenConf - it also happens if you always provide the same seed number. </p> <p>Interestingly, there is a pattern: in CUDA, if you set a fixed GenConf seed and generate multiple times after creating the model, the first output will be different and all the others will be equal. Sounds like some sort of warm-up, and can be accounted for by generating an initial dummy output (from the same inputs), after creating the model.</p> <p>We've never observed non-determinist outputs for llama.cpp fully running in the CPU, without hardware acceleration and this is probably true of other platforms. Given the same seed number and inputs you'll always get the same result when running in the CPU.</p> <p>It's something that should not have a great impact, but that's important to be aware of.</p>"},{"location":"tips/#split-entities-into-separate-classes","title":"Split entities into separate classes","text":"<p>Suppose you want to extract a list of person names from a group. You could use the following class:</p> <pre><code>class Group(BaseModel):\n    persons: list[str] = Field(description=\"List of persons\")\n    group_info: str\n\nout = model.extract(Group, in_text)\n</code></pre> <p>But it tends to work better to separate the Person entity into its own class and leave the list in Group:</p> <pre><code>class Person(BaseModel):\n    name: str\n\nclass Group(BaseModel):\n    persons: list[Person]\n    group_info: str\n\nout = model.extract(Group, in_text)\n</code></pre> <p>The same applies to the equivalent dataclass definitions.</p> <p>Adding descriptions seems to always help, specially for non-trivial extraction. Without descriptions, the model can only look into variable names for clues on what's wanted, so it's important to tell it what we want by adding field descriptions.</p>"},{"location":"tools/","title":"Tools","text":"<p>The tools module includes some utilities to simplify common tasks.</p>"},{"location":"tools/#interact","title":"Interact","text":"<p>The interact() function allows a back-and-forth chat session. The user enters messages in an input() prompt and can use some special \"!\" commands for more functionality. The model answers back after each user message.</p> <p>In a chat interaction, the model has to \"remember\" the previous messages exchanged. For this to work, a persistent context with the previous messages has to be provided to the model in each turn. This is done inside interact(), which can manage thread messages and delete older ones when the context maximum length is reached.</p> <p>Example</p> <pre><code>from sibila import Models, GenConf\nfrom sibila.tools import interact\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\nprint(\"Help available by typing '!'. Enter an empty line to quit.\")\n\ninteract(model,\n        inst_text=\"Be helpful.\", # model instructions text, also known as system message\n        genconf=GenConf(temperature=0.9))\n</code></pre> <p>Result</p> <pre><code>Help available by typing '!'. Enter an empty line to quit.\n\n&gt; Hello there!\n\nHello! How can I help you today?\n\n\n&gt; I'm doing an example of this LLM access package called Sibila - have you heard about it?\n\nYes, I am familiar with the AI language model named Sibila. It is a large language \nmodel developed by a Spanish company called Wall Street Horus. Sibila is designed to \nunderstand and generate text in multiple languages, particularly those spoken in \nSpain and Latin America. It is similar to other language models like OpenAI's GPT-3 \nand Google's BERT, but it has been specifically fine-tuned for the Spanish language.\nIf you have any questions about Sibila or need assistance related to it, feel free to ask!\n\n(NOTE: ignore the above model-generated paragraph, it's total hallucination)\n\n\n&gt; No way! I've just searched and there's no such company nor model. \nYou are hallucinating, Mr. Model!\n\nApologies for the confusion in my previous response. It appears that Sibila is not \nan existing AI language model or a known company. I may have mixed up information \nfrom different sources. If you have any other questions or need assistance with a \ndifferent topic, please feel free to ask!\n\n\n&gt; No problem. See, I'm using an function of package Sibila that allows for maintaining \na chat with you. It stores all user and model messages in a sibila.Context class.\n\nGot it! So, you are using a function from the Sibila package to have a conversation \nwith me, and it's storing our messages in a sibila.Context object. If you need any help \nor information during our chat, please don't hesitate to ask. I'm here to assist you!\n\n(Continues...)\n</code></pre> <p>Check the Interact example to see a chat created with this function.</p>"},{"location":"tools/#multigen","title":"Multigen","text":"<p>The multigen module includes functions that build a table of answers to a list of questions, as generated by multiple models. This can be very helpful to compare how two or more models react to the same input. </p> <p>The query_multigen() function generates a 2-D table of [ input , model ], where each row is the output from different models to the same question or input. Such table can be printed or saved as a CSV file.</p> <p>See the Compare example for a side-by-side comparison of a local and a remote model, answering to the same queries.</p>"},{"location":"what/","title":"What can you do with Sibila?","text":"<p>LLM models answer your questions in the best way their training allows, but they always answer back in plain text.</p> <p>With Sibila, you can extract structured data from LLM models. Not whatever the model chose to output (even if you asked it to answer in a certain format), but the exact fields and types that you need.</p> <p>This not only simplifies handling the model responses but can also open new possibilities: you can now deal with the model in a structured way.</p>"},{"location":"what/#extract-pydantic-dataclasses-or-simple-types","title":"Extract Pydantic, dataclasses or simple types","text":"<p>To specify the structured output that you want from the model, you can use Pydantic's BaseModel derived classes, or the lightweight Python dataclasses, if you don't need the whole Pydantic.</p> <p>With Sibila, you can also use simple data types like bool, int, str, enumerations or lists.  For example, need to classify something? </p> <p>Example</p> <pre><code>from sibila import Models\n\nmodel = Models.create(\"openai:gpt-4\")\n\nmodel.classify([\"good\", \"neutral\", \"bad\"], \n               \"Running with scissors\")\n</code></pre> <p>Result</p> <pre><code>'bad'\n</code></pre> <p>How does it work? Extraction to the given data types is guaranteed by automatic JSON Schema grammars in local models, or by the Tools functionality of OpenAI API (or the equivalent in other remote models).</p>"},{"location":"what/#from-your-models-or-remote-models","title":"From your models or remote models","text":"<p>Small downloadable 7B parameter models are getting better every month and they have reached a level where they are competent enough for most common data extraction or summarization tasks.</p> <p>With 8Gb or more of RAM or GPU memory, you can get good structured output from models like Llama-3, Phi-3, OpenChat or any other GGUF file.</p> <p>Or perhaps the task requires use of state of the art remote models from OpenAI, Anthropic, Mistral AI or other providers - this is also supported by Sibila. </p>"},{"location":"what/#same-api","title":"Same API","text":"<p>The same API is used for both remote and local models. This makes the switch to newer or alternative models much easier, and makes it simpler to evaluate model outputs.</p> <p>With Sibila you can choose the best model for each use, allowing you the freedom of choice.</p>"},{"location":"what/#and-with-model-management","title":"And with model management","text":"<p>Includes a Models factory that creates models from simple names instead of having to track model configurations, filenames or chat templates.</p> <pre><code>local_model = Models.create(\"llamacpp:openchat\")\n\nremote_model = Models.create(\"openai:gpt-4\")    \n</code></pre> <p>Chat templates are automatically used for local models from an included format registry.</p> <p>Sibila includes a CLI tool to download GGUF models from Hugging Face model hub, and to manage its Models factory.</p>"},{"location":"api-reference/changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog. The project adheres to Semantic Versioning.</p>"},{"location":"api-reference/changelog/#unreleased","title":"[Unreleased]","text":"<ul> <li>feat: Add seed setting to GenConf. Commented-out because of lack of support in OpenAI models and some llama.cpp hardware accelerations.</li> </ul>"},{"location":"api-reference/changelog/#042","title":"[0.4.2]","text":"<ul> <li>feat: Add Model.create() argument to retrieve the actual initialization params used to create the model.</li> <li>fix: Correct OpenAI's \"max_tokens_limit\" setting to 4096 in base_models.json, a more sensible default value for future models.</li> <li>fix: Update Model.version() formats to be simpler and simplify comparison between versions.</li> </ul>"},{"location":"api-reference/changelog/#041","title":"[0.4.1]","text":"<ul> <li>feat: Add Anthropic provider.</li> <li>feat: Add chat template formats for Llama3 and Phi-3 instruct models, StableLM-2, Command-R/Plus.</li> <li>feat: Add output_fn_name property to Model, for changing the output function name in models that use a Tools/Functions API.</li> <li>feat: Better JSON/Schema decoding errors.</li> <li>fix: Don't use a string representation of the dataclass when its doc string is unset, during JSON Schema creation, to keep equivalence with Pydantic-based generation.</li> <li>fix: Workaround for MistralModel, where the Mistral API misses api_key argument/env variable when run from pytest.</li> <li>fix: Consolidate all Model class info as methods to avoid property/method() calling confusion.</li> <li>docs: Update installation instructions and include info on new Anthropic provider.</li> <li>test: Better parametrized tests for remote and local models.</li> <li>test: Add tests for new provider.</li> </ul>"},{"location":"api-reference/changelog/#040","title":"[0.4.0]","text":"<ul> <li>feat: New providers: Mistral AI, Together.ai and Fireworks AI allowing access to all their chat-based models.</li> <li>feat: Model classes now support async calls with the '_async' prefix, for example extract_async(). This requires model API support: only remote models will benefit. Local models (via llama.cpp) can still be called with _async methods but do not have async IO that can run concurrently.</li> <li>feat: Add 'special' field to GenConf, allowing provider or model specific generation arguments.</li> <li>feat: All models now also accept model path/name starting with their provider names as in Models.create().</li> <li>feat: Change Model.json() to stop requiring a JSON Schema as first argument.</li> <li>fix: More robust JSON extraction for misbehaved remote models.</li> <li>fix: LlamaCppModel no longer outputting debug info when created in Jupyter notebook environment with verbose=False.</li> <li>fix: Default \"gpt-4\" model in 'sibila/res/base:models.json' now points to gpt-4-1106-preview, the first GPT-4 model that accepts json-object output.</li> <li>docs: Add API references for new classes and _async() methods.</li> <li>docs: Add new async example.</li> <li>test: Add new tests for new providers/model classes.</li> </ul>"},{"location":"api-reference/changelog/#036","title":"[0.3.6]","text":"<ul> <li>feat: Migrate hardcoded OpenAI model entries from OpenAIModel to 'res/base_models.json'.</li> <li>feat: OpenAI now accepts unknown models using defaults from 'openai:_default' key in  'res/base_models.json'.</li> <li>feat: Support OpenAI models with a limit on max_tokens output values, like \"gpt-4-turbo-preview\" (input ctx_len of 128k but only up to 4k output tokens).</li> <li>feat: Auto-discover maximum ctx_len in LlamaCppModel loaded files, when 0 is passed.</li> <li>feat: Add negative int factor mode to GenConf.max_tokens setting, allowing for a percentage of model's context length.</li> <li>fix: Add coherent error exceptions when loading local and remote models.</li> <li>fix: Correct interact() error when GenConf.max_tokens=0.</li> <li>fix: Correct several chat template formats.</li> <li>test: Add many new tests for gpt-3.5/4 and llama.cpp models.</li> <li>docs: Update tips section.</li> </ul>"},{"location":"api-reference/changelog/#035","title":"[0.3.5]","text":"<ul> <li>feat: Split Models factory config in two levels: base definitions in sibila/res and Models.setup() loaded definitions from user folders. These levels never mix, but a fusion of the two is used for models/formats resolution. Only in this manner can \"models\" folder definitions be kept clean.</li> <li>fix: Option sibila formats -u is removed as result of the two-level Models factory.</li> <li>fix: Correct delete of link entries in models.json and formats.json, which was resolving to targets (and deleting them).</li> <li>fix: Raise ValueError when trying to generate from an empty prompt in LLamaCppModel.</li> <li>fix: Update Models to check linked entries when deleting.</li> <li>fix: Update template format discovery to work in more edge cases.</li> <li>test: Add test cases for sibila CLI and LlamaCppModel.</li> </ul>"},{"location":"api-reference/changelog/#034","title":"[0.3.4]","text":"<ul> <li>feat: Improve template format discovery by looking in same folder for models/formats.json.</li> <li>fix: Update legacy importlib_resources reference.</li> <li>docs: Improve text.</li> </ul>"},{"location":"api-reference/changelog/#033","title":"[0.3.3]","text":"<ul> <li>fix: Move base_models.json and base_formats.json to sibila/res folder.</li> <li>fix: Add base_models.json and base_formats.json to project build.</li> <li>fix: Correct .gitignore skipping valid files.</li> <li>docs: Update installation help and mentions to base_models/formats.json.</li> </ul>"},{"location":"api-reference/changelog/#032","title":"[0.3.2]","text":"<ul> <li>feat: Added sibila CLI for models and formats management.</li> <li>feat: Added methods in Models class for CLI functionality.</li> <li>fix: Blacklisting character control set in JSON strings grammar.</li> <li>docs: Improved docs and added section about sibila CLI.</li> <li>docs: Added CLI example.</li> </ul>"},{"location":"api-reference/changelog/#031","title":"[0.3.1]","text":"<ul> <li>feat: Improved documentation.</li> <li>feat: Model.known_models() returns a list of fixed known models or None if unlimited.</li> <li>feat: LlamaCppModel now also looks for the chat template format in a 'formats.json' file in the same folder as the model file.</li> <li>feat: Added GenConf.from_dict() and renamed asdict() to as_dict().</li> <li>fix: Creating a model entry in \"models.json\" with a genconf key was not being passed on model creation.</li> </ul>"},{"location":"api-reference/changelog/#030","title":"[0.3.0]","text":"<ul> <li>feat: Added Models singleton class that centralizes ModelDir and FormatDir.</li> <li>feat: New extract() and classify() methods for type-independent extraction and classification.</li> <li>feat: Renamed confusing gen() and gen_() method names to simpler alternatives type() and gen_type().</li> <li>feat: Replaced dictype definitions with dataclasses, a better to extract dictionaries.</li> <li>feat: Added version() and provider_version() to Model and children classes.</li> <li>fix: Using 2 * \"\\n\" to separate message text from automatically added json_format_instructors (\"Output JSON\", etc.), to provide more meaningful separation.</li> <li>fix: Added requirement for package typing_extensions because of Self type and Python 3.9+ compatibility.</li> </ul>"},{"location":"api-reference/generation/","title":"Generation configs, results and errors","text":""},{"location":"api-reference/generation/#generation-configs","title":"Generation Configs","text":""},{"location":"api-reference/generation/#sibila.GenConf","title":"GenConf  <code>dataclass</code>","text":"<p>Model generation configuration, used in Model.gen() and variants.</p>"},{"location":"api-reference/generation/#sibila.GenConf.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens = 0\n</code></pre> <p>Maximum output token length. Special value of 0 means all available context length, special values between -1 and -100 mean a -percentage of ctx_len. For example -20 allows output up to 20% of ctx_len.</p>"},{"location":"api-reference/generation/#sibila.GenConf.stop","title":"stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop = field(default_factory=list)\n</code></pre> <p>List of generation stop text sequences</p>"},{"location":"api-reference/generation/#sibila.GenConf.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature = 0.0\n</code></pre> <p>Generation temperature. Use 0 to always pick the most probable output, without random sampling. Larger positive values will produce more random outputs.</p>"},{"location":"api-reference/generation/#sibila.GenConf.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p = 0.9\n</code></pre> <p>Nucleus sampling top_p value. Only applies if temperature &gt; 0.</p>"},{"location":"api-reference/generation/#sibila.GenConf.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format = 'text'\n</code></pre> <p>Output format: \"text\" or \"json\". For JSON output, text is validated as in json.loads(). Thread msgs must explicitly request JSON output or a warning will be emitted if string json not present (this is automatically done in Model.json() and related calls).</p>"},{"location":"api-reference/generation/#sibila.GenConf.json_schema","title":"json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>json_schema = None\n</code></pre> <p>A JSON schema to validate the JSON output. Thread msgs must list the JSON schema and request its use; must also set the format to \"json\".</p>"},{"location":"api-reference/generation/#sibila.GenConf.special","title":"special  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>special = None\n</code></pre> <p>Special model or provider-specific generation arguments. Args in the base dict are included unconditionally for a model, while args in sub-keys with the model's provider name are only used for models from  that provider, for example \"openai\": {...} values are only used in OpenAI models.</p>"},{"location":"api-reference/generation/#sibila.GenConf.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Return a copy of the current GenConf updated with values in kwargs. Doesn't modify object. Key 'special' is updated element-wise.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>update settings of the same names in the returned copy.</p> <code>{}</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key does not exist.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A copy of the current object with kwargs values updated. Doesn't modify object.</p> Source code in <code>sibila/gen.py</code> <pre><code>def __call__(self,\n             **kwargs: Any) -&gt; Self:\n    \"\"\"Return a copy of the current GenConf updated with values in kwargs. Doesn't modify object.\n    Key 'special' is updated element-wise.\n\n    Args:\n        **kwargs: update settings of the same names in the returned copy.\n\n    Raises:\n        KeyError: If key does not exist.\n\n    Returns:\n        A copy of the current object with kwargs values updated. Doesn't modify object.\n    \"\"\"\n\n    ret = deepcopy(self)\n\n    for k,v in kwargs.items():\n        if not hasattr(ret, k):\n            raise KeyError(f\"No such key '{k}'\")\n        if k == \"special\":\n            if ret.special is None:\n                ret.special = {}\n            if v is None:\n                v = {}\n            ret.special.update(v)\n            if not ret.special:\n                ret.special = None\n        else:\n            setattr(ret, k,v)\n\n    return ret\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenConf.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a deep copy of this configuration.</p> Source code in <code>sibila/gen.py</code> <pre><code>def clone(self) -&gt; Self:\n    \"\"\"Return a deep copy of this configuration.\"\"\"\n    return deepcopy(self)\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenConf.as_dict","title":"as_dict","text":"<pre><code>as_dict()\n</code></pre> <p>Return GenConf as a dict.</p> Source code in <code>sibila/gen.py</code> <pre><code>def as_dict(self) -&gt; dict:\n    \"\"\"Return GenConf as a dict.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenConf.from_dict","title":"from_dict  <code>staticmethod</code>","text":"<pre><code>from_dict(dic)\n</code></pre> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef from_dict(dic: dict) -&gt; Any: # Any = GenConf\n    return GenConf(**dic)\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenConf.resolve_max_tokens","title":"resolve_max_tokens","text":"<pre><code>resolve_max_tokens(ctx_len, max_tokens_limit=None)\n</code></pre> <p>Calculate actual max_tokens value for cases where it's zero or a percentage of model's ctx_len)</p> <p>Parameters:</p> Name Type Description Default <code>ctx_len</code> <code>int</code> <p>Model's context length.</p> required <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Optional model's limit for max_tokens. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>An actual model maximum number of output tokens.</p> Source code in <code>sibila/gen.py</code> <pre><code>def resolve_max_tokens(self,\n                       ctx_len: int,\n                       max_tokens_limit: Optional[int] = None) -&gt; int:\n    \"\"\"Calculate actual max_tokens value for cases where it's zero or a percentage of model's ctx_len)\n\n    Args:\n        ctx_len: Model's context length.\n        max_tokens_limit: Optional model's limit for max_tokens. Defaults to None.\n\n    Returns:\n        An actual model maximum number of output tokens.\n    \"\"\"\n\n    max_tokens = self.max_tokens\n    if max_tokens &lt;= 0:\n        if max_tokens == 0:\n            max_tokens = ctx_len\n        else:\n            max_tokens = min(-max_tokens, 100)\n            max_tokens = int(max_tokens / 100.0 * ctx_len)\n            max_tokens = max(1,max_tokens)\n\n    if max_tokens_limit is not None:\n        max_tokens = min(max_tokens, max_tokens_limit)\n\n    return max_tokens\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenConf.resolve_special","title":"resolve_special","text":"<pre><code>resolve_special(provider=None)\n</code></pre> <p>Compiles settings from the 'special' field, for model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>If set will include any 'special' settings specified for that provider, inside a key named after the provider. If not given, only base keys are added.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>description</p> Source code in <code>sibila/gen.py</code> <pre><code>def resolve_special(self, \n                    provider: Optional[str] = None) -&gt; dict:\n    \"\"\"Compiles settings from the 'special' field, for model and provider.\n\n    Args:\n        provider: If set will include any 'special' settings specified for that provider, inside a key named after the provider. If not given, only base keys are added.\n\n    Returns:\n        _description_\n    \"\"\"\n\n    if self.special is None:\n        return {}\n\n    from .models import Models\n\n    out = {}\n    for k,v in self.special.items():\n        if k == provider: # provider-specific\n            if not isinstance(v,dict):\n                raise ValueError(f\"Config 'special' for provider '{provider}' must be a dict.\")\n            out.update(v)\n        else: # common args\n            if isinstance(v,dict) and k in Models.ALL_PROVIDER_NAMES: # skip other provider entries\n                continue\n            out[k] = v\n    return out\n</code></pre>"},{"location":"api-reference/generation/#sibila.JSchemaConf","title":"JSchemaConf  <code>dataclass</code>","text":"<p>Configuration for JSON schema massaging and validation.</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.resolve_refs","title":"resolve_refs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resolve_refs = True\n</code></pre> <p>Set for $ref references to be resolved and replaced with actual definition.</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.collapse_single_combines","title":"collapse_single_combines  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>collapse_single_combines = True\n</code></pre> <p>Any single-valued \"oneOf\"/\"anyOf\" is replaced with the actual value.</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.description_from_title","title":"description_from_title  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description_from_title = 0\n</code></pre> <p>If a value doesn't have a description entry, make one from its title or name.</p> <ul> <li>0: don't make description from name</li> <li>1: copy title or name to description</li> <li>2: 1: + capitalize first letter and convert _ to space: class_label -&gt; \"class label\".</li> </ul>"},{"location":"api-reference/generation/#sibila.JSchemaConf.force_all_required","title":"force_all_required  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_all_required = False\n</code></pre> <p>Force all entries in an object to be required (except removed defaults if remove_with_default=True).</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.remove_with_default","title":"remove_with_default  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>remove_with_default = False\n</code></pre> <p>Delete any values that have a \"default\" annotation.</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.default_to_last","title":"default_to_last  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_to_last = True\n</code></pre> <p>Move any default value entry into the last position of properties dict.</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.additional_allowed_root_keys","title":"additional_allowed_root_keys  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_allowed_root_keys = field(default_factory=list)\n</code></pre> <p>By default only the following properties are allowed in schema's root:   description, properties, type, required, additionalProperties, allOf, anyOf, oneOf, not Add to this list to allow additional root properties.</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.pydantic_strict_validation","title":"pydantic_strict_validation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pydantic_strict_validation = None\n</code></pre> <p>Validate JSON values in a strict manner or not. None means validate individually per each value in the obj. (for example in pydantic with: Field(strict=True)).</p>"},{"location":"api-reference/generation/#sibila.JSchemaConf.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a copy of this configuration.</p> Source code in <code>sibila/json_schema.py</code> <pre><code>def clone(self):\n    \"\"\"Return a copy of this configuration.\"\"\"\n    return copy(self)\n</code></pre>"},{"location":"api-reference/generation/#results","title":"Results","text":""},{"location":"api-reference/generation/#sibila.GenRes","title":"GenRes","text":"<p>Model generation result.</p>"},{"location":"api-reference/generation/#sibila.GenRes.OK_STOP","title":"OK_STOP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OK_STOP = 1\n</code></pre> <p>Generation complete without errors.</p>"},{"location":"api-reference/generation/#sibila.GenRes.OK_LENGTH","title":"OK_LENGTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OK_LENGTH = 0\n</code></pre> <p>Generation stopped due to reaching max_tokens.</p>"},{"location":"api-reference/generation/#sibila.GenRes.ERROR_JSON","title":"ERROR_JSON  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON = -1\n</code></pre> <p>Invalid JSON: this is often due to the model returning OK_LENGTH (finished due to max_tokens reached), which cuts off the JSON text.</p>"},{"location":"api-reference/generation/#sibila.GenRes.ERROR_JSON_SCHEMA_VAL","title":"ERROR_JSON_SCHEMA_VAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON_SCHEMA_VAL = -2\n</code></pre> <p>Failed JSON schema validation.</p>"},{"location":"api-reference/generation/#sibila.GenRes.ERROR_JSON_SCHEMA_ERROR","title":"ERROR_JSON_SCHEMA_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_JSON_SCHEMA_ERROR = -2\n</code></pre> <p>JSON schema itself is not valid.</p>"},{"location":"api-reference/generation/#sibila.GenRes.ERROR_MODEL","title":"ERROR_MODEL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR_MODEL = -3\n</code></pre> <p>Other model internal error.</p>"},{"location":"api-reference/generation/#sibila.GenRes.from_finish_reason","title":"from_finish_reason  <code>staticmethod</code>","text":"<pre><code>from_finish_reason(finish)\n</code></pre> <p>Convert a ChatCompletion finish result into a GenRes.</p> <p>Parameters:</p> Name Type Description Default <code>finish</code> <code>str</code> <p>ChatCompletion finish result.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A GenRes result.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef from_finish_reason(finish: str) -&gt; Any: # Any=GenRes\n    \"\"\"Convert a ChatCompletion finish result into a GenRes.\n\n    Args:\n        finish: ChatCompletion finish result.\n\n    Returns:\n        A GenRes result.\n    \"\"\"\n    if finish == 'stop':\n        return GenRes.OK_STOP\n    elif finish == 'length':\n        return GenRes.OK_LENGTH\n    elif finish == '!json':\n        return GenRes.ERROR_JSON\n    elif finish == '!json_schema_val':\n        return GenRes.ERROR_JSON_SCHEMA_VAL\n    elif finish == '!json_schema_error':\n        return GenRes.ERROR_JSON_SCHEMA_ERROR\n    else:\n        return GenRes.ERROR_MODEL\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenRes.as_text","title":"as_text  <code>staticmethod</code>","text":"<pre><code>as_text(res)\n</code></pre> <p>Returns a friendlier description of the result.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Any</code> <p>Model output result.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown GenRes.</p> <p>Returns:</p> Type Description <code>str</code> <p>A friendlier description of the GenRes.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef as_text(res: Any) -&gt; str: # Any=GenRes\n    \"\"\"Returns a friendlier description of the result.\n\n    Args:\n        res: Model output result.\n\n    Raises:\n        ValueError: If unknown GenRes.\n\n    Returns:\n        A friendlier description of the GenRes.\n    \"\"\"\n\n    if res == GenRes.OK_STOP:\n        return \"Stop\"\n    elif res == GenRes.OK_LENGTH:\n        return \"Length (output cut)\"\n    elif res == GenRes.ERROR_JSON:\n        return \"JSON decoding error\"\n\n    elif res == GenRes.ERROR_JSON_SCHEMA_VAL:\n        return \"JSON SCHEMA validation error\"\n    elif res == GenRes.ERROR_JSON_SCHEMA_ERROR:\n        return \"Error in JSON SCHEMA\"\n\n    elif res == GenRes.ERROR_MODEL:\n        return \"Model internal error\"\n    else:\n        raise ValueError(\"Bad/unknow GenRes\")\n</code></pre>"},{"location":"api-reference/generation/#errors","title":"Errors","text":""},{"location":"api-reference/generation/#sibila.GenError","title":"GenError","text":"<pre><code>GenError(out)\n</code></pre> <p>Model generation exception, raised when the model was unable to return a response.</p> <p>An error has happened during model generation.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>GenOut</code> <p>Model output</p> required Source code in <code>sibila/gen.py</code> <pre><code>def __init__(self, \n             out: GenOut):\n    \"\"\"An error has happened during model generation.\n\n    Args:\n        out: Model output\n    \"\"\"\n\n    assert out.res != GenRes.OK_STOP, \"OK_STOP is not an error\"      \n\n    super().__init__()\n\n    self.res = out.res\n    self.text = out.text\n    self.dic = out.dic\n    self.value = out.value\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenError.raise_if_error","title":"raise_if_error  <code>staticmethod</code>","text":"<pre><code>raise_if_error(out, ok_length_is_error)\n</code></pre> <p>Raise an exception if the model returned an error</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>GenOut</code> <p>Model returned info.</p> required <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error?</p> required <p>Raises:</p> Type Description <code>GenError</code> <p>If an error was returned by model.</p> Source code in <code>sibila/gen.py</code> <pre><code>@staticmethod\ndef raise_if_error(out: GenOut,\n                   ok_length_is_error: bool):\n    \"\"\"Raise an exception if the model returned an error\n\n    Args:\n        out: Model returned info.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error?\n\n    Raises:\n        GenError: If an error was returned by model.\n    \"\"\"\n\n    if out.res != GenRes.OK_STOP:\n        if out.res == GenRes.OK_LENGTH and not ok_length_is_error:\n            return # OK_LENGTH to not be considered an error\n\n        raise GenError(out)\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenOut","title":"GenOut  <code>dataclass</code>","text":"<p>Model output, returned by gen_extract(), gen_json() and other model calls that don't raise exceptions.</p>"},{"location":"api-reference/generation/#sibila.GenOut.res","title":"res  <code>instance-attribute</code>","text":"<pre><code>res\n</code></pre> <p>Result of model generation.</p>"},{"location":"api-reference/generation/#sibila.GenOut.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text\n</code></pre> <p>Text generated by model.</p>"},{"location":"api-reference/generation/#sibila.GenOut.dic","title":"dic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dic = None\n</code></pre> <p>Python dictionary, output by the structured calls like gen_json().</p>"},{"location":"api-reference/generation/#sibila.GenOut.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value = None\n</code></pre> <p>Initialized instance value, dataclass or Pydantic BaseModel object, as returned in calls like extract().</p>"},{"location":"api-reference/generation/#sibila.GenOut.as_dict","title":"as_dict","text":"<pre><code>as_dict()\n</code></pre> <p>Return GenOut as a dict.</p> Source code in <code>sibila/gen.py</code> <pre><code>def as_dict(self):\n    \"\"\"Return GenOut as a dict.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api-reference/generation/#sibila.GenOut.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>sibila/gen.py</code> <pre><code>def __str__(self):\n    out = f\"Error={self.res.as_text(self.res)} text=\u2588{self.text}\u2588\"\n    if self.dic is not None:\n        out += f\" dic={self.dic}\"\n    if self.value is not None:\n        out += f\" value={self.value}\"\n    return out\n</code></pre>"},{"location":"api-reference/local_model/","title":"Local model classes","text":""},{"location":"api-reference/local_model/#sibila.LlamaCppModel","title":"LlamaCppModel","text":"<pre><code>LlamaCppModel(\n    path,\n    format=None,\n    format_search_order=[\n        \"name\",\n        \"meta_template\",\n        \"folder_json\",\n    ],\n    *,\n    genconf=None,\n    schemaconf=None,\n    ctx_len=None,\n    max_tokens_limit=None,\n    tokenizer=None,\n    n_gpu_layers=-1,\n    main_gpu=0,\n    n_batch=512,\n    seed=4294967295,\n    verbose=False,\n    **llamacpp_kwargs\n)\n</code></pre> <p>Use local GGUF format models via llama.cpp engine.</p> <p>Supports grammar-constrained JSON output following a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path to the GGUF file.</p> required <code>format</code> <code>Optional[str]</code> <p>Chat template format to use with model. Leave as None for auto-detection.</p> <code>None</code> <code>format_search_order</code> <code>list[str]</code> <p>Search order for auto-detecting format, \"name\" searches in the filename, \"meta_template\" looks in the model's metadata, \"folder_json\" looks for configs in file's folder. Defaults to [\"name\",\"meta_template\", \"folder_json\"].</p> <code>['name', 'meta_template', 'folder_json']</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Default generation configuration, which can be used in gen() and related. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used. Use 0 for maximum possible size, which may raise an out of memory error. None will use a default from the 'llamacpp' provider's '_default' entry at 'res/base_models.json'.</p> <code>None</code> <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Maximum output tokens limit. None for no limit.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>n_gpu_layers</code> <code>int</code> <p>Number of model layers to run in a GPU. Defaults to -1 for all.</p> <code>-1</code> <code>main_gpu</code> <code>int</code> <p>Index of the GPU to use. Defaults to 0.</p> <code>0</code> <code>n_batch</code> <code>int</code> <p>Prompt processing batch size. Defaults to 512.</p> <code>512</code> <code>seed</code> <code>int</code> <p>Random number generation seed, for non zero temperature inference. Defaults to 4294967295.</p> <code>4294967295</code> <code>verbose</code> <code>bool</code> <p>Emit (very) verbose llama.cpp output. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If llama-cpp-python is not installed.</p> <code>ValueError</code> <p>For arguments or settings problems.</p> <code>NameError</code> <p>If the model was not found or the file is corrupt.</p> <code>AttributeError</code> <p>If a suitable template format was not found.</p> <code>MemoryError</code> <p>If an out of memory situation arises.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def __init__(self,\n             path: str,\n\n             format: Optional[str] = None,                 \n             format_search_order: list[str] = [\"name\", \"meta_template\", \"folder_json\"],\n\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             ctx_len: Optional[int] = None,\n             max_tokens_limit: Optional[int] = None,\n             tokenizer: Optional[Tokenizer] = None,\n\n             # important LlamaCpp-specific args\n             n_gpu_layers: int = -1,\n             main_gpu: int = 0,\n             n_batch: int = 512,\n             seed: int = 4294967295,\n             verbose: bool = False,\n\n             # other LlamaCpp-specific args\n             **llamacpp_kwargs\n             ):\n    \"\"\"\n    Args:\n        path: File path to the GGUF file.\n        format: Chat template format to use with model. Leave as None for auto-detection.\n        format_search_order: Search order for auto-detecting format, \"name\" searches in the filename, \"meta_template\" looks in the model's metadata, \"folder_json\" looks for configs in file's folder. Defaults to [\"name\",\"meta_template\", \"folder_json\"].\n        genconf: Default generation configuration, which can be used in gen() and related. Defaults to None.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.\n        ctx_len: Maximum context length to be used. Use 0 for maximum possible size, which may raise an out of memory error. None will use a default from the 'llamacpp' provider's '_default' entry at 'res/base_models.json'.\n        max_tokens_limit: Maximum output tokens limit. None for no limit.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        n_gpu_layers: Number of model layers to run in a GPU. Defaults to -1 for all.\n        main_gpu: Index of the GPU to use. Defaults to 0.\n        n_batch: Prompt processing batch size. Defaults to 512.\n        seed: Random number generation seed, for non zero temperature inference. Defaults to 4294967295.\n        verbose: Emit (very) verbose llama.cpp output. Defaults to False.\n\n    Raises:\n        ImportError: If llama-cpp-python is not installed.\n        ValueError: For arguments or settings problems.\n        NameError: If the model was not found or the file is corrupt.\n        AttributeError: If a suitable template format was not found.\n        MemoryError: If an out of memory situation arises.\n    \"\"\"\n\n    self._llama = None # type: ignore[assignment]\n    self.tokenizer = None # type: ignore[assignment]\n    self._own_tokenizer = False\n\n    if not has_llama_cpp:\n        raise ImportError(\"Please install llama-cpp-python by running: pip install llama-cpp-python\")\n\n    # also accept \"provider:path\" for ease of use\n    provider_name = self.PROVIDER_NAME + \":\"\n    if path.startswith(provider_name):\n        path = path[len(provider_name):]\n\n    if not os.path.isfile(path):\n        raise NameError(f\"Model file not found at '{path}'\")\n\n\n    # find ctx_len from metadata --and-- check file format\n    max_ctx_len = 0\n    try:\n        md = load_gguf_metadata(path)\n        if md is not None:\n            for key in md:\n                if key.endswith('.context_length'):\n                    max_ctx_len = int(md[key])\n                    break\n    except Exception as e:\n        raise NameError(f\"Error loading file '{path}': {e}\")\n\n\n    if ctx_len is None: # find a default in Models _default dict\n        defaults = Models.resolve_provider_defaults(\"llamacpp\", [\"ctx_len\"], 2)\n        if defaults[\"ctx_len\"] is not None:\n            ctx_len = defaults[\"ctx_len\"]\n            logger.debug(f\"Defaulting ctx_len={ctx_len} from Models '_default' entry\")\n\n    if ctx_len == 0: # default to maximum ctx_len - this can be dangerous, as big ctx_len will probably out of memory\n        if max_ctx_len != 0:\n            ctx_len = max_ctx_len\n        else:\n            raise ValueError(\"Cannot find model's maximum ctx_len information. Please provide a non-zero ctx_len arg\")\n\n    if max_ctx_len != 0:\n        if ctx_len &gt; max_ctx_len: # type: ignore[operator]\n            raise ValueError(f\"Arg ctx_len ({ctx_len}) is greater than model's maximum ({max_ctx_len})\")\n\n\n    super().__init__(True,\n                     genconf,\n                     schemaconf,\n                     tokenizer\n                     )\n\n    # update kwargs from important args\n    llamacpp_kwargs.update(n_ctx=ctx_len,\n                           n_batch=n_batch,\n                           n_gpu_layers=n_gpu_layers,\n                           main_gpu=main_gpu,\n                           seed=seed,\n                           verbose=verbose\n                           )\n\n    logger.debug(f\"Creating inner Llama with path='{path}', llamacpp_kwargs={llamacpp_kwargs}\")\n\n\n    try:\n        with llamacpp_verbosity_manager(verbose):\n            self._llama = Llama(model_path=path, **llamacpp_kwargs)\n\n    except Exception as e:\n        raise MemoryError(f\"Could not load model file '{path}'. \"\n                          \"This is usually an out of memory situation but could also be due to a corrupt file. \"\n                          f\"Internal error: {e}\")\n\n\n    self._model_path = path\n\n\n    # correct super __init__ values\n    self.ctx_len = self._llama.n_ctx()\n\n    if max_tokens_limit is not None:\n        self.max_tokens_limit = max_tokens_limit\n\n    self.max_tokens_limit = min(self.max_tokens_limit, self.ctx_len)\n\n\n    if self.tokenizer is None:\n        self.tokenizer = LlamaCppTokenizer(self._llama)\n        self._own_tokenizer = True\n    else:\n        self._own_tokenizer = False\n\n    try:\n        self.init_format(format,\n                         format_search_order,\n                         {\"name\": os.path.basename(self._model_path),\n                          \"path\": self._model_path,\n                          \"meta_template_name\": \"tokenizer.chat_template\"}\n                         )\n    except Exception as e:\n        del self.tokenizer\n        del self._llama\n        raise AttributeError(str(e))\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If trying to generate from an empty prompt.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n\n    Raises:\n        ValueError: If trying to generate from an empty prompt.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. \n    \"\"\"\n\n    genconf2: GenConf\n    prompt, genconf2 = self._gen_pre(thread, genconf)\n\n    text,finish = self._gen_text(prompt, genconf2)\n\n    return self._prepare_gen_out(text, finish, genconf2)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.token_len","title":"token_len","text":"<pre><code>token_len(thread_or_text, _=None)\n</code></pre> <p>Calculate token length for a Thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>For token length calculation.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens used.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self,\n              thread_or_text: Union[Thread,str],\n              _: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Calculate token length for a Thread.\n\n    Args:\n        thread_or_text: For token length calculation.\n\n    Returns:\n        Number of tokens used.\n    \"\"\"\n\n    if self.tokenizer is None:\n        raise ValueError(\"A TextModel object requires a tokenizer\")\n\n    if isinstance(thread_or_text, Thread):\n        text = self.text_from_thread(thread_or_text)\n    else:\n        text = thread_or_text\n\n    return self.tokenizer.token_len(text)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = None\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = n_ctx()\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>If the model can only use a fixed set of models, return their names. Otherwise, return None.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>If the model provider requires an API key, pass it here or set it in the respective env variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models or None if unable to fetch it.</p> Source code in <code>sibila/model.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"If the model can only use a fixed set of models, return their names. Otherwise, return None.\n\n    Args:\n        api_key: If the model provider requires an API key, pass it here or set it in the respective env variable.\n\n    Returns:\n        Returns a list of known models or None if unable to fetch it.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.desc","title":"desc","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    return f\"{type(self).__name__}: {self._model_path} - '{self._llama._model.desc()}'\"\n</code></pre>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.n_embd","title":"n_embd  <code>property</code>","text":"<pre><code>n_embd\n</code></pre> <p>Embedding size of model.</p>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.n_params","title":"n_params  <code>property</code>","text":"<pre><code>n_params\n</code></pre> <p>Total number of model parameters.</p>"},{"location":"api-reference/local_model/#sibila.LlamaCppModel.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata()\n</code></pre> <p>Returns model metadata.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def get_metadata(self):\n    \"\"\"Returns model metadata.\"\"\"\n    out = {}\n    buf = bytes(16 * 1024)\n    lmodel = self._llama.model\n    count = llama_cpp.llama_model_meta_count(lmodel)\n    for i in range(count):\n        res = llama_cpp.llama_model_meta_key_by_index(lmodel, i, buf,len(buf))\n        if res &gt;= 0:\n            key = buf[:res].decode('utf-8')\n            res = llama_cpp.llama_model_meta_val_str_by_index(lmodel, i, buf,len(buf))\n            if res &gt;= 0:\n                value = buf[:res].decode('utf-8')\n                out[key] = value\n    return out\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model","title":"Model","text":"<pre><code>Model(is_local_model, genconf, schemaconf, tokenizer)\n</code></pre> <p>Model is an abstract base class for common LLM model functionality. Many of the useful methods like extract() or json() are implemented here.</p> <p>It should not be instantiated directly, instead LlamaCppModel, OpenAIModel, etc, all derive from this class.</p> <p>Initializer for base model type, shared by actual model classes like LlamaCpp, OpenAI, etc.</p> <p>Parameters:</p> Name Type Description Default <code>is_local_model</code> <code>bool</code> <p>Is the model running locally?</p> required <code>genconf</code> <code>Union[GenConf, None]</code> <p>Default generation configuration options, used if generation call doesn't supply one.</p> required <code>schemaconf</code> <code>Union[JSchemaConf, None]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one.</p> required <code>tokenizer</code> <code>Union[Tokenizer, None]</code> <p>Tokenizer used to encode text (even for message-based models).</p> required Source code in <code>sibila/model.py</code> <pre><code>def __init__(self,\n             is_local_model: bool,\n             genconf: Union[GenConf, None],\n             schemaconf: Union[JSchemaConf, None],\n             tokenizer: Union[Tokenizer, None]):\n    \"\"\"Initializer for base model type, shared by actual model classes like LlamaCpp, OpenAI, etc.\n\n    Args:\n        is_local_model: Is the model running locally?\n        genconf: Default generation configuration options, used if generation call doesn't supply one.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one.\n        tokenizer: Tokenizer used to encode text (even for message-based models).\n    \"\"\"\n\n    self.is_local_model = is_local_model\n\n    self.ctx_len = 0\n    self.max_tokens_limit = sys.maxsize\n    self.output_key_name = \"output\"\n    self.output_fn_name = \"json_out\"\n\n    self.tokenizer = tokenizer # type: ignore[assignment]\n\n    if genconf is None:\n        self.genconf = GenConf()\n    else:\n        self.genconf = genconf.clone()\n\n    if schemaconf is None:\n        self.schemaconf = JSchemaConf()\n    else:\n        self.schemaconf = schemaconf.clone()\n\n\n\n    # set either \"json\" or \"json_schema\" key values to None to skip.\n    self.json_format_instructors = {\n        \"json\": {\n            \"bypass_if\": [\"json\"], # bypass appending if all lowercase text values are present in thread\n            \"append_text\": \"Output JSON.\",\n            \"sep_count\": 2\n        },\n        \"json_schema\": {\n            \"bypass_if\": [\"json\", \"schema\"],\n            \"append_text\": \"Output JSON matching the following schema:\\n{{json_schema}}\",\n            \"sep_count\": 2\n        }\n    }\n\n    # text going to model: tight, without \\u00xx\n    self.json_in_dumps_kwargs = {\n        \"indent\": None,\n        \"ensure_ascii\": False\n    } \n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>NotImplementedError</code> <p>If method was not defined by a derived class.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen(self,\n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        NotImplementedError: If method was not defined by a derived class.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.token_len","title":"token_len  <code>abstractmethod</code>","text":"<pre><code>token_len(thread_or_text)\n</code></pre> <p>Calculate or estimate the token length for a Thread or plain text string. In some cases it's not possible to calculate the exact token count,  but at least a conservative estimate should be provided.</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>A Thread or plain text string For token length calculation.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens occupied.</p> Source code in <code>sibila/model.py</code> <pre><code>@abstractmethod\ndef token_len(self,\n              thread_or_text: Union[Thread,str]) -&gt; int:\n    \"\"\"Calculate or estimate the token length for a Thread or plain text string.\n    In some cases it's not possible to calculate the exact token count, \n    but at least a conservative estimate should be provided.\n\n    Args:\n        thread_or_text: A Thread or plain text string For token length calculation.\n\n    Returns:\n        Number of tokens occupied.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = tokenizer\n</code></pre> <p>Tokenizer used to encode text (even for message-based models). Some remote models don't have tokenizer and token length is estimated</p>"},{"location":"api-reference/local_model/#sibila.Model.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = 0\n</code></pre> <p>Maximum context token length, including input and model output. There can be a limit for output tokens in the max_tokens_limit.</p>"},{"location":"api-reference/local_model/#sibila.Model.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>If the model can only use a fixed set of models, return their names. Otherwise, return None.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>If the model provider requires an API key, pass it here or set it in the respective env variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models or None if unable to fetch it.</p> Source code in <code>sibila/model.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"If the model can only use a fixed set of models, return their names. Otherwise, return None.\n\n    Args:\n        api_key: If the model provider requires an API key, pass it here or set it in the respective env variable.\n\n    Returns:\n        Returns a list of known models or None if unable to fetch it.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api-reference/local_model/#sibila.Model.desc","title":"desc  <code>abstractmethod</code>","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/model.py</code> <pre><code>@abstractmethod\ndef desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/models/","title":"Models factory","text":""},{"location":"api-reference/models/#sibila.Models","title":"Models","text":"<p>Model and template format directory that unifies (and simplifies) model access and configuration.</p> This env variable is checked and used during initialization <p>SIBILA_MODELS: ';'-delimited list of folders where to find: models.json, formats.json and model files.</p> <p>= Models Directory =</p> <p>Useful to create models from resource names like \"llamacpp:openchat\" or \"openai:gpt-4\".  This makes it simple to change a model, store model settings, to compare model outputs, etc.</p> <p>User can add new entries from script or with JSON filenames, via the add() call. New directory entries with the same name are merged into existing ones for each added config.</p> <p>Uses file \"sibila/res/base_models.json\" for the initial defaults, which the user can augment  by calling setup() with own config files or directly adding model config with set_model().</p> <p>An example of a model directory JSON config file:</p> <pre><code>{\n    # \"llamacpp\" is a provider, you can then create models with names \n    # like \"provider:model_name\", for ex: \"llamacpp:openchat\"\n    \"llamacpp\": { \n\n        \"_default\": { # place here default args for all llamacpp: models.\n            \"genconf\": {\"temperature\": 0.0}\n            # each model entry below can then override as needed\n        },\n\n        \"openchat\": { # a model definition\n            \"name\": \"openchat-3.5-1210.Q4_K_M.gguf\",\n            \"format\": \"openchat\" # chat template format used by this model\n        },\n\n        \"phi2\": {\n            \"name\": \"phi-2.Q4_K_M.gguf\", # model filename\n            \"format\": \"phi2\",\n            \"genconf\": {\"temperature\": 2.0} # a hot-headed model\n        },\n\n        \"oc\": \"openchat\" \n        # this is a link: \"oc\" forwards to the \"openchat\" entry\n    },\n\n    # The \"openai\" provider. A model can be created with name: \"openai:gpt-4\"\n    \"openai\": { \n\n        \"_default\": {}, # default settings for all OpenAI models\n\n        \"gpt-3.5\": {\n            \"name\": \"gpt-3.5-turbo-1106\" # OpenAI's model name\n        },\n\n        \"gpt-4\": {\n            \"name\": \"gpt-4-1106-preview\"\n        },\n    },\n\n    # \"alias\" entry is not a provider but a way to have simpler alias names.\n    # For example you can use \"alias:develop\" or even simpler, just \"develop\" to create the model:\n    \"alias\": { \n        \"develop\": \"llamacpp:openchat\",\n        \"production\": \"openai:gpt-3.5\"\n    }\n}\n</code></pre> <p>Rules for entry inheritance/overriding</p> <p>Entries in the '_default' key of each provider will serve as defaults for models of that provider. Model entries in base_models_dir (automatically loaded from 'res/base_models.json') are overridden  by any entries of the same name loaded from a local 'models.json' file with Models.setup(). Here, overridden means local keys of the same name replace base keys (as a dict.update()). However '_default' entries only apply separately to either base_models_dir or 'local models.json',  as in a lexical scope.</p> <p>= Format Directory =</p> <p>Detects chat templates from model name/filename or uses from metadata if possible.</p> <p>This directory can be setup from a JSON file or by calling set_format().</p> <p>Any new entries with the same name replace previous ones on each new call.</p> <p>Initializes from file \"sibila/res/base_formats.json\".</p> <p>Example of a \"formats.json\" file:</p> <pre><code>{\n    \"chatml\": {\n        # template is a Jinja template for this model\n        \"template\": \"{% for message in messages %}...\"\n    },\n\n    \"openchat\": {\n        \"match\": \"openchat\", # a regexp to match model name or filename\n        \"template\": \"{{ bos_token }}...\"\n    },    \n\n    \"phi\": {\n        \"match\": \"phi\",\n        \"template\": \"...\"\n    },\n\n    \"phi2\": \"phi\",\n    # this is a link: \"phi2\" -&gt; \"phi\"\n}\n</code></pre> <p>Jinja2 templates receive a standard ChatML messages list (created from a Thread) and must deal with the following:</p> <ul> <li> <p>In models that don't use a system message, template must take care of prepending it to first user message.</p> </li> <li> <p>The add_generation_prompt template variable is always set as True.</p> </li> </ul>"},{"location":"api-reference/models/#sibila.Models.setup","title":"setup  <code>classmethod</code>","text":"<pre><code>setup(\n    path=None, clear=False, add_cwd=True, load_from_env=True\n)\n</code></pre> <p>Initialize models and formats directory from given model files folder and/or contained configuration files. Path can start with \"~/\" current account's home directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Union[str, list[str]]]</code> <p>Path to a folder or to \"models.json\" or \"formats.json\" configuration files. Defaults to None which tries to initialize from defaults and env variable.</p> <code>None</code> <code>clear</code> <code>bool</code> <p>Set to clear existing directories before loading from path arg.</p> <code>False</code> <code>add_cwd</code> <code>bool</code> <p>Add current working directory to search path.</p> <code>True</code> <code>load_from_env</code> <code>bool</code> <p>Load from SIBILA_MODELS env variable?</p> <code>True</code> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef setup(cls,\n          path: Optional[Union[str,list[str]]] = None,\n          clear: bool = False,\n          add_cwd: bool = True,\n          load_from_env: bool = True):\n    \"\"\"Initialize models and formats directory from given model files folder and/or contained configuration files.\n    Path can start with \"~/\" current account's home directory.\n\n    Args:\n        path: Path to a folder or to \"models.json\" or \"formats.json\" configuration files. Defaults to None which tries to initialize from defaults and env variable.\n        clear: Set to clear existing directories before loading from path arg.\n        add_cwd: Add current working directory to search path.\n        load_from_env: Load from SIBILA_MODELS env variable?\n    \"\"\"\n\n    if clear:\n        cls.clear()\n\n    cls._ensure(add_cwd, \n                load_from_env)\n\n    if path is not None:\n        if isinstance(path, str):\n            path_list = [path]\n        else:\n            path_list = path\n\n        cls._read_any(path_list)\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    res_name,\n    genconf=None,\n    ctx_len=None,\n    *,\n    resolved_create_args=None,\n    **over_args\n)\n</code></pre> <p>Create a model.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Optional model generation configuration. Overrides set_genconf() value and any directory defaults. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used. Overrides directory defaults. Defaults to None.</p> <code>None</code> <code>resolved_create_args</code> <code>Optional[dict]</code> <p>Pass an empty dict to be filled by this method with the resolved args used in model creation. Defaults to None.</p> <code>None</code> <code>over_args</code> <code>Union[Any]</code> <p>Model-specific creation args, which will override default args set in model directory.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>the initialized model.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef create(cls,\n           res_name: str,\n\n           # common to all providers\n           genconf: Optional[GenConf] = None,\n           ctx_len: Optional[int] = None,\n\n           *,\n           # debug/testing\n           resolved_create_args: Optional[dict] = None,\n\n           # model-specific overriding:\n           **over_args: Union[Any]) -&gt; Model:\n    \"\"\"Create a model.\n\n    Args:\n        res_name: Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".\n        genconf: Optional model generation configuration. Overrides set_genconf() value and any directory defaults. Defaults to None.\n        ctx_len: Maximum context length to be used. Overrides directory defaults. Defaults to None.\n        resolved_create_args: Pass an empty dict to be filled by this method with the resolved args used in model creation. Defaults to None.\n        over_args: Model-specific creation args, which will override default args set in model directory.\n\n    Returns:\n        Model: the initialized model.\n    \"\"\"\n\n    try:\n        provider, name, args = cls.resolve_model_entry(res_name, **over_args)\n    except ValueError as e:\n        raise NameError(str({e}))\n\n    # override genconf, ctx_len\n    if genconf is None:\n        genconf = cls.genconf\n\n    if genconf is not None:\n        args[\"genconf\"] = genconf\n\n    elif \"genconf\" in args and isinstance(args[\"genconf\"], dict):\n        # transform dict into a GenConf instance:\n        args[\"genconf\"] = GenConf.from_dict(args[\"genconf\"])\n\n    if ctx_len is not None:\n        args[\"ctx_len\"] = ctx_len\n\n    if resolved_create_args is not None:\n        resolved_create_args.update(args)\n\n\n    logger.debug(f\"Resolved '{res_name}' to '{provider}:{name}' with args: {args}\")\n\n\n    model: Model\n    if provider == \"anthropic\":\n\n        from .anthropic import AnthropicModel\n        model = AnthropicModel(**args)\n\n    elif provider == \"fireworks\":\n\n        from .schema_format_openai import FireworksModel\n        model = FireworksModel(**args)\n\n    elif provider == \"llamacpp\":\n\n        # resolve filename -&gt; path\n        path = cls._locate_file(args[\"name\"])\n        if path is None:\n            path = args[\"name\"] # LlamaCppModel should raise the exception, not us\n        else:\n            logger.debug(f\"Resolved llamacpp model '{args['name']}' to '{path}'\")\n\n        # rename \"name\" -&gt; \"path\" which LlamaCppModel is expecting\n        del args[\"name\"]\n        args[\"path\"] = path\n\n        from .llamacpp import LlamaCppModel\n\n        model = LlamaCppModel(**args)\n\n    elif provider == \"mistral\":\n\n        from .mistral import MistralModel\n        model = MistralModel(**args)\n\n    elif provider == \"openai\":\n\n        from .openai import OpenAIModel\n        model = OpenAIModel(**args)\n\n    elif provider == \"together\":\n\n        from .schema_format_openai import TogetherModel\n        model = TogetherModel(**args)\n\n    \"\"\"\n    elif provider == \"hf\":\n        from .hf import HFModel\n\n        model = HFModel(**args)\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.add_models_search_path","title":"add_models_search_path  <code>classmethod</code>","text":"<pre><code>add_models_search_path(path)\n</code></pre> <p>Prepends new paths to model files search path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, list[str]]</code> <p>A path or list of paths to add to model search path.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef add_models_search_path(cls,\n                           path: Union[str,list[str]]):\n    \"\"\"Prepends new paths to model files search path.\n\n    Args:\n        path: A path or list of paths to add to model search path.\n    \"\"\"\n\n    cls._ensure()\n\n    prepend_path(cls.models_search_path, path)\n\n    logger.debug(f\"Adding '{path}' to search_path\")\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.set_genconf","title":"set_genconf  <code>classmethod</code>","text":"<pre><code>set_genconf(genconf)\n</code></pre> <p>Set the GenConf to use as default for model creation.</p> <p>Parameters:</p> Name Type Description Default <code>genconf</code> <code>GenConf</code> <p>Model generation configuration.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef set_genconf(cls,\n                genconf: GenConf):\n    \"\"\"Set the GenConf to use as default for model creation.\n\n    Args:\n        genconf: Model generation configuration.\n    \"\"\"\n    cls.genconf = genconf\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.list_models","title":"list_models  <code>classmethod</code>","text":"<pre><code>list_models(\n    name_query, providers, include_base, resolved_values\n)\n</code></pre> <p>List format entries matching query.</p> <p>Parameters:</p> Name Type Description Default <code>name_query</code> <code>str</code> <p>Case-insensitive substring to match model names. Empty string for all.</p> required <code>providers</code> <code>list[str]</code> <p>Filter by these exact provider names. Empty list for all.</p> required <code>include_base</code> <code>bool</code> <p>Also list fused values from base_models_dir.</p> required <code>resolved_values</code> <code>bool</code> <p>Return resolved entries or raw ones.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict where keys are model res_names and values are respective entries.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef list_models(cls,\n                name_query: str,\n                providers: list[str],\n                include_base: bool,\n                resolved_values: bool) -&gt; dict:\n    \"\"\"List format entries matching query.\n\n    Args:\n        name_query: Case-insensitive substring to match model names. Empty string for all.\n        providers: Filter by these exact provider names. Empty list for all.\n        include_base: Also list fused values from base_models_dir.\n        resolved_values: Return resolved entries or raw ones.\n\n    Returns:\n        A dict where keys are model res_names and values are respective entries.\n    \"\"\"\n\n    cls._ensure()\n\n    models_dir = cls.fused_models_dir() if include_base else cls.models_dir\n\n    out = {}\n\n    name_query = name_query.lower()\n\n    for prov_name in models_dir:\n\n        if providers and prov_name not in providers:\n            continue\n\n        prov_dic = models_dir[prov_name]\n\n        for name in prov_dic:\n\n            if name == cls.DEFAULT_ENTRY_NAME:\n                continue\n\n            if name_query and name_query not in name.lower():\n                continue\n\n            entry_res_name = prov_name + \":\" + name\n\n            if resolved_values:\n                # okay to use get_model_entry() because it resolves to fused\n                res = cls.get_model_entry(entry_res_name) # type: ignore[assignment]\n                if res is None:\n                    continue\n                else:\n                    val = res[1]\n            else:\n                val = prov_dic[name]\n\n            out[entry_res_name] = val\n\n    return out\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.get_model_entry","title":"get_model_entry  <code>classmethod</code>","text":"<pre><code>get_model_entry(res_name)\n</code></pre> <p>Get a resolved model entry. Resolved means following any links.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".</p> required <p>Returns:</p> Type Description <code>Union[tuple[str, dict], None]</code> <p>Resolved entry (res_name,dict) or None if not found.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef get_model_entry(cls,\n                    res_name: str) -&gt; Union[tuple[str,dict],None]:\n    \"\"\"Get a resolved model entry. Resolved means following any links.\n\n    Args:\n        res_name: Resource name in the format: provider:model_name, for example \"llamacpp:openchat\".\n\n    Returns:\n        Resolved entry (res_name,dict) or None if not found.\n    \"\"\"\n\n    cls._ensure()        \n\n    models_dir = cls.fused_models_dir()\n\n    # resolve \"alias:name\" res names, or \"name\": \"link_name\" links\n    provider,name = resolve_model(models_dir, res_name, cls.ALL_PROVIDER_NAMES)\n    # arriving here, prov as a non-link dict entry\n    logger.debug(f\"Resolved model '{res_name}' to '{provider}','{name}'\")\n\n    prov = models_dir[provider]\n\n    if name in prov:\n        return provider + \":\" + name, prov[name]\n    else:\n        return None\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.has_model_entry","title":"has_model_entry  <code>classmethod</code>","text":"<pre><code>has_model_entry(res_name)\n</code></pre> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef has_model_entry(cls,\n                    res_name: str) -&gt; bool:\n    return cls.get_model_entry(res_name) is not None\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.set_model","title":"set_model  <code>classmethod</code>","text":"<pre><code>set_model(\n    res_name, model_name, format_name=None, genconf=None\n)\n</code></pre> <p>Add model configuration for given res_name.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>A name in the form \"provider:model_name\", for example \"openai:gtp-4\".</p> required <code>model_name</code> <code>str</code> <p>Model name or filename identifier.</p> required <code>format_name</code> <code>Optional[str]</code> <p>Format name used by model. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Base GenConf to use when creating model. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown provider.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef set_model(cls,\n              res_name: str,\n              model_name: str,\n              format_name: Optional[str] = None,\n              genconf: Optional[GenConf] = None):\n    \"\"\"Add model configuration for given res_name.\n\n    Args:\n        res_name: A name in the form \"provider:model_name\", for example \"openai:gtp-4\".\n        model_name: Model name or filename identifier.\n        format_name: Format name used by model. Defaults to None.\n        genconf: Base GenConf to use when creating model. Defaults to None.\n\n    Raises:\n        ValueError: If unknown provider.\n    \"\"\"\n\n    cls._ensure()\n\n    provider,name = provider_name_from_urn(res_name, False)\n    if provider not in cls.ALL_PROVIDER_NAMES:\n        raise ValueError(f\"Unknown provider '{provider}' in '{res_name}'\")\n\n    entry: dict = {\n        \"name\": model_name\n    }\n\n    if format_name:\n        if not cls.has_format_entry(format_name):\n            raise ValueError(f\"Could not find format '{format_name}'\")\n        entry[\"format\"] = format_name\n\n    if genconf:\n        entry[\"genconf\"] = genconf.as_dict()\n\n    cls.models_dir[provider][name] = entry\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.update_model","title":"update_model  <code>classmethod</code>","text":"<pre><code>update_model(\n    res_name,\n    model_name=None,\n    format_name=None,\n    genconf=None,\n)\n</code></pre> <p>update model fields</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>A name in the form \"provider:model_name\", for example \"openai:gtp-4\".</p> required <code>model_name</code> <code>Optional[str]</code> <p>Model name or filename identifier. Defaults to None.</p> <code>None</code> <code>format_name</code> <code>Optional[str]</code> <p>Format name used by model. Use \"\" to delete. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Union[GenConf, str, None]</code> <p>Base GenConf to use when creating model. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown provider.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef update_model(cls,\n                 res_name: str,\n                 model_name: Optional[str] = None,\n                 format_name: Optional[str] = None,\n                 genconf: Union[GenConf,str,None] = None):\n\n    \"\"\"update model fields\n\n    Args:\n        res_name: A name in the form \"provider:model_name\", for example \"openai:gtp-4\".\n        model_name: Model name or filename identifier. Defaults to None.\n        format_name: Format name used by model. Use \"\" to delete. Defaults to None.\n        genconf: Base GenConf to use when creating model. Defaults to None.\n\n    Raises:\n        ValueError: If unknown provider.\n    \"\"\"\n\n    cls._ensure()\n\n    provider,name = provider_name_from_urn(res_name, False)\n    if provider not in cls.ALL_PROVIDER_NAMES:\n        raise ValueError(f\"Unknown provider '{provider}' in '{res_name}'\")\n\n    entry = cls.models_dir[provider][name]\n\n    if model_name:\n        entry[\"name\"] = model_name\n\n    if format_name is not None:\n        if format_name != \"\":\n            if not cls.has_format_entry(format_name):\n                raise ValueError(f\"Could not find format '{format_name}'\")\n            entry[\"format\"] = format_name\n        else:\n            del entry[\"format\"]\n\n    if genconf is not None:\n        if genconf != \"\":\n            entry[\"genconf\"] = genconf\n        else:\n            del entry[\"genconf\"]\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.set_model_link","title":"set_model_link  <code>classmethod</code>","text":"<pre><code>set_model_link(res_name, link_name)\n</code></pre> <p>Create a model link into another model.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>A name in the form \"provider:model_name\", for example \"openai:gtp-4\".</p> required <code>link_name</code> <code>str</code> <p>Name of model this entry links to.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown provider.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef set_model_link(cls,\n                   res_name: str,\n                   link_name: str):\n    \"\"\"Create a model link into another model.\n\n    Args:\n        res_name: A name in the form \"provider:model_name\", for example \"openai:gtp-4\".\n        link_name: Name of model this entry links to.\n\n    Raises:\n        ValueError: If unknown provider.\n    \"\"\"\n\n    cls._ensure()\n\n    provider,name = provider_name_from_urn(res_name, True)\n    if provider not in cls.ALL_PROVIDER_NAMES:\n        raise ValueError(f\"Unknown provider '{provider}' in '{res_name}'\")\n\n    # first: ensure link_name is a res_name\n    if ':' not in link_name:\n        link_name = provider + \":\" + link_name\n\n    if not cls.has_model_entry(link_name):\n        raise ValueError(f\"Could not find linked model '{link_name}'\")\n\n    # second: check link name is without provider if same\n    link_split = link_name.split(\":\")\n    if len(link_split) == 2:\n        if link_split[0] == provider: # remove same \"provider:\"\n            link_name = link_split[1]\n\n    cls.models_dir[provider][name] = link_name\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.delete_model","title":"delete_model  <code>classmethod</code>","text":"<pre><code>delete_model(res_name)\n</code></pre> <p>Delete a model entry.</p> <p>Parameters:</p> Name Type Description Default <code>res_name</code> <code>str</code> <p>Model entry in the form \"provider:name\".</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef delete_model(cls,\n                 res_name: str):\n    \"\"\"Delete a model entry.\n\n    Args:\n        res_name: Model entry in the form \"provider:name\".\n    \"\"\"\n\n    cls._ensure()\n\n    provider, name = provider_name_from_urn(res_name,\n                                            allow_alias_provider=False)\n\n    if provider not in cls.ALL_PROVIDER_NAMES:\n        raise ValueError(f\"Unknown provider '{provider}', must be one of: {cls.ALL_PROVIDER_NAMES}\")\n\n    prov = cls.models_dir[provider]        \n    if name not in prov:\n        raise ValueError(f\"Model '{res_name}' not found\")\n\n    # verify if any entry links to name:\n    def check_link_to(link_to_name: str, \n                      provider: str) -&gt; Union[str, None]:\n\n        for name,entry in cls.models_dir[provider].items():\n            if isinstance(entry,str) and entry == link_to_name:\n                return name\n        return None\n\n    offender = check_link_to(name, provider)\n    if offender is not None:\n        raise ValueError(f\"Cannot delete '{res_name}', as entry '{provider}:{offender}' links to it\")\n\n    offender = check_link_to(name, \"alias\")\n    if offender is not None:\n        raise ValueError(f\"Cannot delete '{res_name}', as entry 'alias:{offender}' links to it\")\n\n    del prov[name]\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.save_models","title":"save_models  <code>classmethod</code>","text":"<pre><code>save_models(path=None, include_base=False)\n</code></pre> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef save_models(cls,\n                path: Optional[str] = None,\n                include_base: bool = False):\n\n    cls._ensure()\n\n    if path is None:\n        if len(cls.models_search_path) != 1:\n            raise ValueError(\"No path arg provided and multiple path in cls.search_path. Don't know where to save.\")\n\n        path = os.path.join(cls.models_search_path[0], \"models.json\")\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        models_dir = cls.fused_models_dir() if include_base else cls.models_dir\n\n        # clear providers with no models:\n        for provider in cls.ALL_PROVIDER_NAMES:\n            if provider in models_dir and not models_dir[provider]:\n                del models_dir[provider]\n\n        json.dump(models_dir, f, indent=4)\n\n    return path\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.list_formats","title":"list_formats  <code>classmethod</code>","text":"<pre><code>list_formats(name_query, include_base, resolved_values)\n</code></pre> <p>List format entries matching query.</p> <p>Parameters:</p> Name Type Description Default <code>name_query</code> <code>str</code> <p>Case-insensitive substring to match format names. Empty string for all.</p> required <code>include_base</code> <code>bool</code> <p>Also list base_formats_dir.</p> required <code>resolved_values</code> <code>bool</code> <p>Return resolved entries or raw ones.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict where keys are format names and values are respective entries.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef list_formats(cls,\n                 name_query: str,\n                 include_base: bool,\n                 resolved_values: bool) -&gt; dict:\n    \"\"\"List format entries matching query.\n\n    Args:\n        name_query: Case-insensitive substring to match format names. Empty string for all.\n        include_base: Also list base_formats_dir.\n        resolved_values: Return resolved entries or raw ones.\n\n    Returns:\n        A dict where keys are format names and values are respective entries.\n    \"\"\"\n\n    cls._ensure()\n\n    out = {}\n\n    name_query = name_query.lower()\n\n    formats_dir = cls.fused_formats_dir() if include_base else cls.formats_dir\n\n    for name in formats_dir.keys():\n\n        if name_query and name_query not in name.lower():\n            continue\n\n        val = formats_dir[name]\n\n        if resolved_values:\n            res = cls.get_format_entry(name)\n            if res is None:\n                continue\n            else:\n                val = res[1]\n\n        out[name] = val\n\n    return out\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.get_format_entry","title":"get_format_entry  <code>classmethod</code>","text":"<pre><code>get_format_entry(name)\n</code></pre> <p>Get a resolved format entry by name, following links if required.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format name.</p> required <p>Returns:</p> Type Description <code>Union[tuple[str, dict], None]</code> <p>Tuple of (resolved_name, format_entry).</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef get_format_entry(cls,\n                     name: str) -&gt; Union[tuple[str,dict],None]:\n    \"\"\"Get a resolved format entry by name, following links if required.\n\n    Args:\n        name: Format name.\n\n    Returns:\n        Tuple of (resolved_name, format_entry).\n    \"\"\"\n\n    cls._ensure()\n\n    return get_format_entry(cls.fused_formats_dir(), name)\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.has_format_entry","title":"has_format_entry  <code>classmethod</code>","text":"<pre><code>has_format_entry(name)\n</code></pre> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef has_format_entry(cls,\n                     name: str) -&gt; bool:\n    return cls.get_format_entry(name) is not None\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.get_format_template","title":"get_format_template  <code>classmethod</code>","text":"<pre><code>get_format_template(name)\n</code></pre> <p>Get a format template by name, following links if required.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format name.</p> required <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Resolved format template str.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef get_format_template(cls,\n                        name: str) -&gt; Union[str,None]:\n    \"\"\"Get a format template by name, following links if required.\n\n    Args:\n        name: Format name.\n\n    Returns:\n        Resolved format template str.\n    \"\"\"\n\n    res = cls.get_format_entry(name)\n    return None if res is None else res[1][\"template\"]\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.match_format_entry","title":"match_format_entry  <code>classmethod</code>","text":"<pre><code>match_format_entry(name)\n</code></pre> <p>Search the formats registry, based on model name or filename.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name or filename of model.</p> required <p>Returns:</p> Type Description <code>Union[tuple[str, dict], None]</code> <p>Tuple (name, format_entry) where name is a resolved name. Or None if none found.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef match_format_entry(cls,\n                       name: str) -&gt; Union[tuple[str,dict],None]:\n    \"\"\"Search the formats registry, based on model name or filename.\n\n    Args:\n        name: Name or filename of model.\n\n    Returns:\n        Tuple (name, format_entry) where name is a resolved name. Or None if none found.\n    \"\"\"\n\n    cls._ensure()\n\n    return search_format(cls.fused_formats_dir(), name)\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.match_format_template","title":"match_format_template  <code>classmethod</code>","text":"<pre><code>match_format_template(name)\n</code></pre> <p>Search the formats registry, based on model name or filename.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name or filename of model.</p> required <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Format template or None if none found.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef match_format_template(cls,\n                          name: str) -&gt; Union[str,None]:\n    \"\"\"Search the formats registry, based on model name or filename.\n\n    Args:\n        name: Name or filename of model.\n\n    Returns:\n        Format template or None if none found.\n    \"\"\"\n\n    res = cls.match_format_entry(name)\n\n    return None if res is None else res[1][\"template\"]\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.set_format","title":"set_format  <code>classmethod</code>","text":"<pre><code>set_format(name, template, match=None)\n</code></pre> <p>Add a format entry to the format directory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format entry name.</p> required <code>template</code> <code>str</code> <p>The Chat template format in Jinja2 format</p> required <code>match</code> <code>Optional[str]</code> <p>Regex that matches names/filenames that use this format. Default is None.</p> <code>None</code> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef set_format(cls,\n               name: str,\n               template: str,\n               match: Optional[str] = None):\n    \"\"\"Add a format entry to the format directory.\n\n    Args:\n        name: Format entry name.\n        template: The Chat template format in Jinja2 format\n        match: Regex that matches names/filenames that use this format. Default is None.\n    \"\"\"\n\n    cls._ensure()\n\n    if \"{{\" not in template: # a link_name for the template\n        if not cls.has_format_entry(template):\n            raise ValueError(f\"Could not find linked template entry '{template}'.\")\n\n    entry = {\n        \"template\": template\n    }\n    if match is not None:\n        entry[\"match\"] = match\n\n    cls.formats_dir[name] = entry        \n</code></pre>"},{"location":"api-reference/models/#sibila.Models.set_format_link","title":"set_format_link  <code>classmethod</code>","text":"<pre><code>set_format_link(name, link_name)\n</code></pre> <p>Add a format link entry to the format directory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format entry name.</p> required <code>link_name</code> <code>str</code> <p>Name of format that this entry links to.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef set_format_link(cls,\n                    name: str,\n                    link_name: str):\n    \"\"\"Add a format link entry to the format directory.\n\n    Args:\n        name: Format entry name.\n        link_name: Name of format that this entry links to.\n    \"\"\"\n\n    cls._ensure()\n\n    if not cls.has_format_entry(link_name):\n        raise ValueError(f\"Could not find linked entry '{link_name}'.\")\n\n    cls.formats_dir[name] = link_name\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.delete_format","title":"delete_format  <code>classmethod</code>","text":"<pre><code>delete_format(name)\n</code></pre> <p>Delete a format entry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Format entry name.</p> required Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef delete_format(cls,\n                  name: str):\n    \"\"\"Delete a format entry.\n\n    Args:\n        name: Format entry name.\n    \"\"\"\n\n    cls._ensure()\n\n    if name not in cls.formats_dir:\n        raise ValueError(f\"Format name '{name}' not found.\")\n\n    for check_name,entry in cls.formats_dir.items():\n        if isinstance(entry,str) and entry == name:\n            raise ValueError(f\"Cannot delete '{name}', as entry '{check_name}' links to it\")\n\n    del cls.formats_dir[name]\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.save_formats","title":"save_formats  <code>classmethod</code>","text":"<pre><code>save_formats(path=None, include_base=False)\n</code></pre> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef save_formats(cls,\n                 path: Optional[str] = None,\n                 include_base: bool = False):\n\n    cls._ensure()\n\n    if path is None:\n        if len(cls.models_search_path) != 1:\n            raise ValueError(\"No path arg provided and multiple path in cls.search_path. Don't know where to save.\")\n\n        path = os.path.join(cls.models_search_path[0], \"formats.json\")\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        formats_dir = cls.fused_formats_dir() if include_base else cls.formats_dir\n        json.dump(formats_dir, f, indent=4)\n\n    return path\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.info","title":"info  <code>classmethod</code>","text":"<pre><code>info(include_base=True, verbose=False)\n</code></pre> <p>Return information about current setup.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If False, formats directory values are abbreviated. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Textual information about the current setup.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef info(cls,\n         include_base: bool = True,\n         verbose: bool = False) -&gt; str:\n    \"\"\"Return information about current setup.\n\n    Args:\n        verbose: If False, formats directory values are abbreviated. Defaults to False.\n\n    Returns:\n        Textual information about the current setup.\n    \"\"\"\n\n    cls._ensure()\n\n    out = \"\"\n\n    out += f\"Models search path: {cls.models_search_path}\\n\"\n\n    models_dir = cls.fused_models_dir() if include_base else cls.models_dir\n    out += f\"Models directory:\\n{pformat(models_dir, sort_dicts=False)}\\n\"\n\n    out += f\"Model Genconf:\\n{cls.genconf}\\n\"\n\n    formats_dir = cls.fused_formats_dir() if include_base else cls.formats_dir\n\n    if not verbose:\n        fordir = {}\n        for key in formats_dir:\n            fordir[key] = deepcopy(formats_dir[key])\n            if isinstance(fordir[key], dict) and \"template\" in fordir[key]:\n                fordir[key][\"template\"] = fordir[key][\"template\"][:14] + \"...\"\n    else:\n        fordir = formats_dir\n\n    out += f\"Formats directory:\\n{pformat(fordir)}\"\n\n    return out\n</code></pre>"},{"location":"api-reference/models/#sibila.Models.clear","title":"clear  <code>classmethod</code>","text":"<pre><code>clear()\n</code></pre> <p>Clear directories. Members base_models_dir and base_formats_dir and genconf are not cleared.</p> Source code in <code>sibila/models.py</code> <pre><code>@classmethod\ndef clear(cls):\n    \"\"\"Clear directories. Members base_models_dir and base_formats_dir and genconf are not cleared.\"\"\"\n    cls.models_dir = None\n    cls.models_search_path = []\n    cls.formats_dir = None\n</code></pre>"},{"location":"api-reference/multigen/","title":"Multigen","text":""},{"location":"api-reference/multigen/#sibila.multigen","title":"multigen","text":"<p>Functions for comparing output across models.</p> <ul> <li>thread_multigen(), query_multigen() and multigen(): Compare outputs across models.</li> <li>cycle_gen_print(): For a list of models, sequentially grow a Thread with model responses to given IN messages.</li> </ul>"},{"location":"api-reference/multigen/#sibila.multigen.thread_multigen","title":"thread_multigen","text":"<pre><code>thread_multigen(\n    threads,\n    model_names,\n    text=None,\n    csv=None,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"value\"],\n    thread_titles=None,\n)\n</code></pre> <p>Generate a single thread on a list of models, returning/saving results in text/CSV.</p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>list[Thread]</code> <p>List of threads to input into each model.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of Models names.</p> required <code>text</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.</p> <code>None</code> <code>csv</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.</p> <code>None</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>list[str]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].</p> <code>['text', 'dic', 'value']</code> <code>thread_titles</code> <code>Optional[list[str]]</code> <p>A human-friendly title for each Thread. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def thread_multigen(threads: list[Thread],\n                    model_names: list[str],\n\n                    text: Union[str,list[str],None] = None,\n                    csv: Union[str,list[str],None] = None,\n\n                    gencall: Optional[Callable] = None,                   \n                    genconf: Optional[GenConf] = None,\n\n                    out_keys: list[str] = [\"text\",\"dic\", \"value\"],\n\n                    thread_titles: Optional[list[str]] = None                   \n                    ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate a single thread on a list of models, returning/saving results in text/CSV.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        threads: List of threads to input into each model.\n        model_names: A list of Models names.\n        text: An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.\n        csv: An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].\n        thread_titles: A human-friendly title for each Thread. Defaults to None.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"\n\n    assert isinstance(model_names, list), \"model_names must be a list of strings\"\n\n    table = multigen(threads,\n                     model_names=model_names, \n                     gencall=gencall,\n                     genconf=genconf)\n\n    # table[threads,models]\n\n    if thread_titles is None:\n        thread_titles = [str(th) for th in threads]\n\n    def format(format_fn, cmds):\n        if cmds is None or not cmds:\n            return\n\n        f = StringIO(newline='')\n\n        format_fn(f,\n                  table, \n                  title_list=thread_titles,\n                  model_names=model_names,\n                  out_keys=out_keys)\n        fmtd = f.getvalue()\n\n        if not isinstance(cmds, list):\n            cmds = [cmds]\n        for c in cmds:\n            if c == 'print':\n                print(fmtd)\n            else: # path\n                with open(c, \"w\", encoding=\"utf-8\") as f:\n                    f.write(fmtd)\n\n    format(format_text, text)\n    format(format_csv, csv)\n\n    return table\n</code></pre>"},{"location":"api-reference/multigen/#sibila.multigen.query_multigen","title":"query_multigen","text":"<pre><code>query_multigen(\n    in_list,\n    inst_text,\n    model_names,\n    text=None,\n    csv=None,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"value\"],\n    in_titles=None,\n)\n</code></pre> <p>Generate an INST+IN thread on a list of models, returning/saving results in text/CSV.</p> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>in_list</code> <code>list[str]</code> <p>List of IN messages to initialize Threads.</p> required <code>inst_text</code> <code>str</code> <p>The common INST to use in all models.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of Models names.</p> required <code>text</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.</p> <code>None</code> <code>csv</code> <code>Union[str, list[str], None]</code> <p>An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.</p> <code>None</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>list[str]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].</p> <code>['text', 'dic', 'value']</code> <code>in_titles</code> <code>Optional[list[str]]</code> <p>A human-friendly title for each Thread. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)).        </p> <code>list[list[GenOut]]</code> <p>For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def query_multigen(in_list: list[str],\n                   inst_text: str,                                                \n                   model_names: list[str],\n\n                   text: Union[str,list[str],None] = None, # \"print\", path\n                   csv: Union[str,list[str],None] = None, # \"print\", path\n\n                   gencall: Optional[Callable] = None,                   \n                   genconf: Optional[GenConf] = None,\n\n                   out_keys: list[str] = [\"text\",\"dic\", \"value\"],\n                   in_titles: Optional[list[str]] = None\n                   ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate an INST+IN thread on a list of models, returning/saving results in text/CSV.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        in_list: List of IN messages to initialize Threads.\n        inst_text: The common INST to use in all models.\n        model_names: A list of Models names.\n        text: An str list with \"print\"=print results, path=a path to output a text file with results. Defaults to None.\n        csv: An str list with \"print\"=print CSV results, path=a path to output a CSV file with results. Defaults to None.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].\n        in_titles: A human-friendly title for each Thread. Defaults to None.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)).        \n        For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"    \n\n    th_list = []\n    for in_text in in_list:\n        th = Thread.make_INST_IN(inst_text, in_text)\n        th_list.append(th)\n\n    if in_titles is None:\n        in_titles = in_list\n\n    out = thread_multigen(th_list,                     \n                          model_names=model_names, \n                          text=text,\n                          csv=csv,\n                          gencall=gencall,\n                          genconf=genconf,\n                          out_keys=out_keys,\n                          thread_titles=in_titles)\n\n    return out\n</code></pre>"},{"location":"api-reference/multigen/#sibila.multigen.multigen","title":"multigen","text":"<pre><code>multigen(\n    threads,\n    *,\n    models=None,\n    model_names=None,\n    model_names_del_after=True,\n    gencall=None,\n    genconf=None\n)\n</code></pre> <p>Generate a list of Threads in multiple models, returning the GenOut for each [thread,model] combination.</p> Actual generation for each model is implemented by the gencall arg Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>list[Thread]</code> <p>List of threads to input into each model.</p> required <code>models</code> <code>Optional[list[Model]]</code> <p>A list of initialized models. Defaults to None.</p> <code>None</code> <code>model_names</code> <code>Optional[list[str]]</code> <p>--Or-- A list of Models names. Defaults to None.</p> <code>None</code> <code>model_names_del_after</code> <code>bool</code> <p>Delete model_names models after using them: important or an out-of-memory error will eventually happen. Defaults to True.</p> <code>True</code> <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Only one of models or model_names can be given.</p> <p>Returns:</p> Type Description <code>list[list[GenOut]]</code> <p>A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...</p> Source code in <code>sibila/multigen.py</code> <pre><code>def multigen(threads: list[Thread],\n             *,\n             models: Optional[list[Model]] = None, # existing models\n\n             model_names: Optional[list[str]] = None,\n             model_names_del_after: bool = True,\n\n             gencall: Optional[Callable] = None,\n             genconf: Optional[GenConf] = None\n             ) -&gt; list[list[GenOut]]:\n    \"\"\"Generate a list of Threads in multiple models, returning the GenOut for each [thread,model] combination.\n\n    Actual generation for each model is implemented by the gencall arg Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        threads: List of threads to input into each model.\n        models: A list of initialized models. Defaults to None.\n        model_names: --Or-- A list of Models names. Defaults to None.\n        model_names_del_after: Delete model_names models after using them: important or an out-of-memory error will eventually happen. Defaults to True.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n\n    Raises:\n        ValueError: Only one of models or model_names can be given.\n\n    Returns:\n        A list of lists in the format [thread,model] of shape (len(threads), len(models)). For example: out[0] holds threads[0] results on all models, out[1]: threads[1] on all models, ...\n    \"\"\"\n\n    if not ((models is None) ^ ((model_names is None))):\n        raise ValueError(\"Only one of models or model_names can be given\")\n\n    if gencall is None:\n        gencall = _default_gencall_text\n\n    mod_count = len(models) if models is not None else len(model_names) # type: ignore[arg-type]\n\n    all_out = []\n\n    for i in range(mod_count):\n        if models is not None:\n            model = models[i]\n            logger.debug(f\"Model: {model.desc}\")\n        else:\n            name = model_names[i] # type: ignore[index]\n            model = Models.create(name)\n            logger.info(f\"Model: {name} -&gt; {model.desc}\")\n\n        mod_out = []\n        for th in threads:\n            out = gencall(model, th, genconf)\n\n            mod_out.append(out)\n\n        all_out.append(mod_out)\n\n        if model_names_del_after and models is None:\n            del model\n\n    # all_out is currently shaped (M,T) -&gt; transpose to (T,M), so that each row contains thread t for all models\n    tout = []\n    for t in range(len(threads)):\n        tmout = [] # thread t for all models\n        for m in range(mod_count):\n            tmout.append(all_out[m][t])\n\n        tout.append(tmout)\n\n    return tout\n</code></pre>"},{"location":"api-reference/multigen/#sibila.multigen.cycle_gen_print","title":"cycle_gen_print","text":"<pre><code>cycle_gen_print(\n    in_list,\n    inst_text,\n    model_names,\n    gencall=None,\n    genconf=None,\n    out_keys=[\"text\", \"dic\", \"value\"],\n    json_kwargs={\n        \"indent\": 2,\n        \"sort_keys\": False,\n        \"ensure_ascii\": False,\n    },\n)\n</code></pre> <p>For a list of models, sequentially grow a Thread with model responses to given IN messages and print the results.</p> <p>Works by doing:</p> <ol> <li>Generate an INST+IN prompt for a list of models. (Same INST for all).</li> <li>Append the output of each model to its own Thread.</li> <li>Append the next IN prompt and generate again. Back to 2.</li> </ol> Actual generation for each model is implemented by an optional Callable with this signature <p>def gencall(model: Model,             thread: Thread,             genconf: GenConf) -&gt; GenOut</p> <p>Parameters:</p> Name Type Description Default <code>in_list</code> <code>list[str]</code> <p>List of IN messages to initialize Threads.</p> required <code>inst_text</code> <code>str</code> <p>The common INST to use in all models.</p> required <code>model_names</code> <code>list[str]</code> <p>A list of Models names.</p> required <code>gencall</code> <code>Optional[Callable]</code> <p>Callable function that does the actual generation. Defaults to None, which will use a text generation default function.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration to use in models. Defaults to None, meaning default values.</p> <code>None</code> <code>out_keys</code> <code>list[str]</code> <p>A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].</p> <code>['text', 'dic', 'value']</code> <code>json_kwargs</code> <code>dict</code> <p>JSON dumps() configuration. Defaults to {\"indent\": 2, \"sort_keys\": False, \"ensure_ascii\": False }.</p> <code>{'indent': 2, 'sort_keys': False, 'ensure_ascii': False}</code> Source code in <code>sibila/multigen.py</code> <pre><code>def cycle_gen_print(in_list: list[str],\n                    inst_text: str,                                                \n                    model_names: list[str],\n\n                    gencall: Optional[Callable] = None,                   \n                    genconf: Optional[GenConf] = None,\n\n                    out_keys: list[str] = [\"text\",\"dic\", \"value\"],\n\n                    json_kwargs: dict = {\"indent\": 2,\n                                         \"sort_keys\": False,\n                                         \"ensure_ascii\": False}\n                    ):\n    \"\"\"For a list of models, sequentially grow a Thread with model responses to given IN messages and print the results.\n\n    Works by doing:\n\n    1. Generate an INST+IN prompt for a list of models. (Same INST for all).\n    2. Append the output of each model to its own Thread.\n    3. Append the next IN prompt and generate again. Back to 2.\n\n    Actual generation for each model is implemented by an optional Callable with this signature:\n        def gencall(model: Model,\n                    thread: Thread,\n                    genconf: GenConf) -&gt; GenOut\n\n    Args:\n        in_list: List of IN messages to initialize Threads.\n        inst_text: The common INST to use in all models.\n        model_names: A list of Models names.\n        gencall: Callable function that does the actual generation. Defaults to None, which will use a text generation default function.\n        genconf: Model generation configuration to use in models. Defaults to None, meaning default values.\n        out_keys: A list with GenOut members to output. Defaults to [\"text\",\"dic\", \"value\"].\n        json_kwargs: JSON dumps() configuration. Defaults to {\"indent\": 2, \"sort_keys\": False, \"ensure_ascii\": False }.\n    \"\"\"\n\n    assert isinstance(model_names, list), \"model_names must be a list of strings\"\n\n    if gencall is None:\n        gencall = _default_gencall_text\n\n\n    n_model = len(model_names)\n    n_ins = len(in_list)\n\n    for m in range(n_model):\n\n        name = model_names[m]\n        model = Models.create(name)\n\n        print('=' * 80)\n        print(f\"Model: {name} -&gt; {model.desc}\")\n\n        th = Thread(inst=inst_text)\n\n        for i in range(n_ins):\n            in_text = in_list[i]\n            print(f\"IN: {in_text}\")\n\n            th += (MsgKind.IN, in_text)\n\n            out = gencall(model, th, genconf)\n\n            out_dict = out.as_dict()\n\n            print(\"OUT\")\n\n            for k in out_keys:\n\n                if k in out_dict and out_dict[k] is not None:\n\n                    if k != out_keys[0]: # not first\n                        print(\"-\" * 20)\n\n                    val = nice_print(k, out_dict[k], json_kwargs)\n                    print(val)\n\n            th += (MsgKind.OUT, out.text)\n\n        del model\n</code></pre>"},{"location":"api-reference/remote_model/","title":"Remote model classes","text":""},{"location":"api-reference/remote_model/#sibila.OpenAIModel","title":"OpenAIModel","text":"<pre><code>OpenAIModel(\n    name,\n    *,\n    genconf=None,\n    schemaconf=None,\n    ctx_len=None,\n    max_tokens_limit=None,\n    tokenizer=None,\n    api_key=None,\n    base_url=None,\n    overhead_per_msg=None,\n    token_estimation_factor=None,\n    create_tokenizer=False,\n    other_init_kwargs={}\n)\n</code></pre> <p>Access an OpenAI model.</p> <p>Supports constrained JSON output, via the OpenAI API tools mechanism.</p> Ref <p>https://platform.openai.com/docs/api-reference/chat/create</p> <p>Create an OpenAI remote model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name to resolve into an existing model.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used (shared for input and output). None for model's default.</p> <code>None</code> <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Maximum output tokens limit. None for model's default.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>OpenAI API key. Defaults to None, which will use env variable OPENAI_API_KEY.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base location for API access. Defaults to None, which will use env variable OPENAI_BASE_URL or a default.</p> <code>None</code> <code>overhead_per_msg</code> <code>Optional[int]</code> <p>Overhead tokens to account for when calculating token length. None for model's default.</p> <code>None</code> <code>token_estimation_factor</code> <code>Optional[float]</code> <p>Used when no tokenizer is available. Multiplication factor to estimate token usage: multiplies total text length to obtain token length.</p> <code>None</code> <code>create_tokenizer</code> <code>bool</code> <p>When no tokenizer is passed, should try to create one?</p> <code>False</code> <code>other_init_kwargs</code> <code>dict</code> <p>Extra args for OpenAI.OpenAI() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If OpenAI API is not installed.</p> <code>NameError</code> <p>If model name was not found or there's an API or authentication problem.</p> Source code in <code>sibila/openai.py</code> <pre><code>def __init__(self,\n             name: str,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             ctx_len: Optional[int] = None,\n             max_tokens_limit: Optional[int] = None,\n             tokenizer: Optional[Tokenizer] = None,\n\n             # most important OpenAI-specific args\n             api_key: Optional[str] = None,\n             base_url: Optional[str] = None,\n             overhead_per_msg: Optional[int] = None,\n             token_estimation_factor: Optional[float] = None,\n             create_tokenizer: bool = False,\n\n             # other OpenAI-specific args\n             other_init_kwargs: dict = {},\n             ):\n    \"\"\"Create an OpenAI remote model.\n\n    Args:\n        name: Model name to resolve into an existing model.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). None for model's default.\n        max_tokens_limit: Maximum output tokens limit. None for model's default.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        api_key: OpenAI API key. Defaults to None, which will use env variable OPENAI_API_KEY.\n        base_url: Base location for API access. Defaults to None, which will use env variable OPENAI_BASE_URL or a default.\n        overhead_per_msg: Overhead tokens to account for when calculating token length. None for model's default.\n        token_estimation_factor: Used when no tokenizer is available. Multiplication factor to estimate token usage: multiplies total text length to obtain token length.\n        create_tokenizer: When no tokenizer is passed, should try to create one?\n        other_init_kwargs: Extra args for OpenAI.OpenAI() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If OpenAI API is not installed.\n        NameError: If model name was not found or there's an API or authentication problem.\n    \"\"\"\n\n\n    if not has_openai:\n        raise ImportError(\"Please install openai by running: pip install openai\")\n\n    self._client = self._client_async = None\n\n\n    # also accept \"provider:name\" for ease of use\n    provider_name = self.PROVIDER_NAME + \":\"\n    if name.startswith(provider_name):\n        name = name[len(provider_name):]\n\n    super().__init__(False,\n                     genconf,\n                     schemaconf,\n                     tokenizer\n                     )\n\n    if (ctx_len is not None and\n        max_tokens_limit is not None and\n        overhead_per_msg is not None and\n        token_estimation_factor is not None): # all elements given: probably created via Models.create()\n\n        self._model_name = name\n        default_ctx_len = ctx_len\n        default_max_tokens_limit = max_tokens_limit\n        default_overhead_per_msg = overhead_per_msg\n        default_token_estimation_factor = token_estimation_factor\n\n    else: # need to resolve\n        settings = self.resolve_settings(self.PROVIDER_NAME,\n                                         name,\n                                         [\"name\", \n                                          \"ctx_len\", \n                                          \"max_tokens_limit\", \n                                          \"overhead_per_msg\",\n                                          \"token_estimation_factor\"])\n        self._model_name = settings.get(\"name\") or name\n        default_ctx_len = settings.get(\"ctx_len\") # type: ignore[assignment]\n        default_max_tokens_limit = settings.get(\"max_tokens_limit\") # type: ignore[assignment]\n        default_overhead_per_msg = settings.get(\"overhead_per_msg\") # type: ignore[assignment]\n        default_token_estimation_factor = settings.get(\"token_estimation_factor\") # type: ignore[assignment]\n\n        # all defaults are conservative values\n        if ctx_len is None and default_ctx_len is None:\n            default_ctx_len = 4096\n            logger.warning(f\"Model '{self._model_name}': unknown ctx_len, assuming {default_ctx_len}\")\n\n        if max_tokens_limit is None and default_max_tokens_limit is None:\n            default_max_tokens_limit = ctx_len or default_ctx_len                \n            # don't warn: assume equal to ctx_len: logger.warning(f\"Model '{self._model_name}': unknown max_tokens_limit, assuming {default_max_tokens_limit}\")\n\n        if overhead_per_msg is None and default_overhead_per_msg is None:\n            default_overhead_per_msg = 3\n            # don't warn for this setting due to derived model classes (none uses it)\n\n        if token_estimation_factor is None and default_token_estimation_factor is None:\n            default_token_estimation_factor = self.DEFAULT_TOKEN_ESTIMATION_FACTOR\n            logger.warning(f\"Model '{self._model_name}': unknown token_estimation_factor, assuming {default_token_estimation_factor}\")\n\n\n    self.ctx_len = ctx_len or default_ctx_len\n\n    self.max_tokens_limit = max_tokens_limit or default_max_tokens_limit\n    self.max_tokens_limit = min(self.max_tokens_limit, self.ctx_len)\n\n    self._overhead_per_msg = overhead_per_msg or default_overhead_per_msg\n\n    self._token_estimation_factor = token_estimation_factor or default_token_estimation_factor\n\n\n    # only check for \"json\" text presence as json schema (including field descriptions) is requested with the tools facility.\n    self.json_format_instructors[\"json_schema\"] = self.json_format_instructors[\"json\"]\n\n\n    if self.tokenizer is None and create_tokenizer:\n        try:\n            self.tokenizer = OpenAITokenizer(self._model_name)\n        except Exception as e:\n            logger.warning(f\"Could not create a local tokenizer for model '{self._model_name}' - \"\n                           \"token length calculation will be disabled and assume defaults. \"\n                           f\"Internal error: {e}\")\n\n\n    self._client_init_kwargs = other_init_kwargs\n    if api_key is not None:\n        self._client_init_kwargs[\"api_key\"] = api_key\n    if base_url is not None:\n        self._client_init_kwargs[\"base_url\"] = base_url\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n    genconf2: GenConf\n    kwargs, genconf2 = self._gen_pre(thread, genconf)\n\n    self._ensure_client(False)\n\n    try:\n        # https://platform.openai.com/docs/api-reference/chat/create\n        response = self._client.chat.completions.create(**kwargs) # type: ignore[attr-defined]\n\n    except Exception as e:\n        raise RuntimeError(f\"Cannot generate. Internal error: {e}\")\n\n\n    return self._gen_post(response,\n                          kwargs,\n                          genconf2)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.token_len","title":"token_len","text":"<pre><code>token_len(thread_or_text, genconf=None)\n</code></pre> <p>Estimate the number of tokens used by a Thread or text string. If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.</p> <p>From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p> <p>More info on calculating function_call (and tools?) tokens:</p> <p>https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24</p> <p>https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens used.</p> Source code in <code>sibila/openai.py</code> <pre><code>def token_len(self,\n              thread_or_text: Union[Thread,str],\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Estimate the number of tokens used by a Thread or text string.\n    If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.\n\n    From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    More info on calculating function_call (and tools?) tokens:\n\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24\n\n    https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5\n\n    Args:\n        thread_or_text: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens used.\n    \"\"\"\n\n    if isinstance(thread_or_text, Thread):\n        thread = thread_or_text            \n    else:\n        thread = Thread.make_IN(thread_or_text)\n\n    num_tokens = 0\n\n    if self.tokenizer is None: # no tokenizer was found, so we'll have to do a conservative estimate\n\n        OVERHEAD_PER_MSG = 3\n        for index in range(-1, len(thread)): # -1 for system message\n            message = thread.msg_as_chatml(index)\n            msg_tokens = len(message[\"content\"]) * self._token_estimation_factor + OVERHEAD_PER_MSG\n            num_tokens += int(msg_tokens)\n\n        if genconf is not None and genconf.json_schema is not None:\n            if isinstance(genconf.json_schema, str):\n                js_str = genconf.json_schema\n            else:\n                js_str = json.dumps(genconf.json_schema)\n\n            tools_num_tokens = len(js_str) * self._token_estimation_factor\n            num_tokens += int(tools_num_tokens)\n            # print(\"tools_num_tokens\", tools_num_tokens)\n\n    else: # do an \"informed\" token estimation from what is known of the OpenAI model's tokenization\n\n        for index in range(-1, len(thread)): # -1 for system message\n            message = thread.msg_as_chatml(index)\n            # print(message)\n            num_tokens += self._overhead_per_msg\n            for key, value in message.items():\n                num_tokens += len(self.tokenizer.encode(value))\n\n        # add extras + every reply is primed with &lt;|start|&gt;assistant&lt;|message|&gt;\n        num_tokens += 32\n\n        # print(\"text token_len\", num_tokens)\n\n        if genconf is not None and genconf.json_schema is not None:\n            TOOLS_TOKEN_LEN_FACTOR = 1.2\n\n            if isinstance(genconf.json_schema, str):\n                js_str = genconf.json_schema\n            else:\n                js_str = json.dumps(genconf.json_schema)\n\n            tools_num_tokens = self.tokenizer.token_len(js_str)\n\n            # this is an upper bound, as empirically tested with the api.\n            tools_num_tokens = int(tools_num_tokens * TOOLS_TOKEN_LEN_FACTOR)\n            # print(\"tools token_len\", tools_num_tokens)\n\n            num_tokens += tools_num_tokens\n\n\n    return num_tokens\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = OpenAITokenizer(_model_name)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = ctx_len or default_ctx_len\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>List of model names that can be used. Some of the models are not chat models and cannot be used, for example embedding models.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Requires OpenAI API key, passed as this arg or set in env variable OPENAI_API_KEY.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models.</p> Source code in <code>sibila/openai.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"List of model names that can be used. Some of the models are not chat models and cannot be used,\n    for example embedding models.\n\n    Args:\n        api_key: Requires OpenAI API key, passed as this arg or set in env variable OPENAI_API_KEY.\n\n    Returns:\n        Returns a list of known models.\n    \"\"\"\n\n    client = openai.OpenAI(api_key=api_key)\n    model_list = client.models.list()\n\n    out = []\n    for model in model_list.data:\n        out.append(model.id)\n    return sorted(out)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.OpenAIModel.desc","title":"desc","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/openai.py</code> <pre><code>def desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    return f\"{type(self).__name__}: '{self._model_name}'\"\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel","title":"AnthropicModel","text":"<pre><code>AnthropicModel(\n    name,\n    *,\n    genconf=None,\n    schemaconf=None,\n    ctx_len=None,\n    max_tokens_limit=None,\n    api_key=None,\n    token_estimation_factor=None,\n    anthropic_init_kwargs={}\n)\n</code></pre> <p>Access an Anthropic model. Supports constrained JSON output, via the Anthropic API function calling mechanism.</p> Ref <p>https://docs.anthropic.com/claude/docs/intro-to-claude</p> <p>Create an Anthropic remote model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name to resolve into an existing model.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used (shared for input and output). None for model's default.</p> <code>None</code> <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Maximum output tokens limit. None for model's default.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Anthropic API key. Defaults to None, which will use env variable ANTHROPIC_API_KEY.</p> <code>None</code> <code>token_estimation_factor</code> <code>Optional[float]</code> <p>Multiplication factor to estimate token usage: multiplies total text length to obtain token length.</p> <code>None</code> <code>anthropic_init_kwargs</code> <code>dict</code> <p>Extra args for Anthropic() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If Anthropic API is not installed.</p> <code>NameError</code> <p>If model name was not found or there's an API or authentication problem.</p> Source code in <code>sibila/anthropic.py</code> <pre><code>def __init__(self,\n             name: str,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             ctx_len: Optional[int] = None,\n             max_tokens_limit: Optional[int] = None,\n\n             # most important Anthropic-specific args\n             api_key: Optional[str] = None,\n             token_estimation_factor: Optional[float] = None,\n\n             # other Anthropic-specific args\n             anthropic_init_kwargs: dict = {},\n             ):\n    \"\"\"Create an Anthropic remote model.\n\n    Args:\n        name: Model name to resolve into an existing model.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). None for model's default.\n        max_tokens_limit: Maximum output tokens limit. None for model's default.\n        api_key: Anthropic API key. Defaults to None, which will use env variable ANTHROPIC_API_KEY.\n        token_estimation_factor: Multiplication factor to estimate token usage: multiplies total text length to obtain token length.\n        anthropic_init_kwargs: Extra args for Anthropic() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If Anthropic API is not installed.\n        NameError: If model name was not found or there's an API or authentication problem.\n    \"\"\"\n\n\n    if not has_anthropic:\n        raise ImportError(\"Please install anthropic API by running: pip install anthropic\")\n\n    self._client = self._client_async = None\n\n\n    # also accept \"provider:name\" for ease of use\n    provider_name = self.PROVIDER_NAME + \":\"\n    if name.startswith(provider_name):\n        name = name[len(provider_name):]\n\n    super().__init__(False,\n                     genconf,\n                     schemaconf,\n                     None\n                     )\n\n    if (ctx_len is not None and\n        max_tokens_limit is not None and\n        token_estimation_factor is not None): # all elements given: probably created via Models.create()\n\n        self._model_name = name\n        default_ctx_len = ctx_len\n        default_max_tokens_limit = max_tokens_limit\n        default_token_estimation_factor = token_estimation_factor\n\n    else: # need to resolve\n        settings = self.resolve_settings(self.PROVIDER_NAME,\n                                         name,\n                                         [\"name\", \n                                          \"ctx_len\", \n                                          \"max_tokens_limit\", \n                                          \"token_estimation_factor\"])\n        self._model_name = settings.get(\"name\") or name\n        default_ctx_len = settings.get(\"ctx_len\") # type: ignore[assignment]\n        default_max_tokens_limit = settings.get(\"max_tokens_limit\") or default_ctx_len\n        default_token_estimation_factor = settings.get(\"token_estimation_factor\") # type: ignore[assignment]\n\n        # all defaults are conservative values\n        if default_ctx_len is None:\n            default_ctx_len = 200000\n            logger.warning(f\"Model '{self._model_name}': unknown ctx_len, assuming {default_ctx_len}\")\n        if default_max_tokens_limit is None:\n            default_max_tokens_limit = default_ctx_len\n            logger.warning(f\"Model '{self._model_name}': unknown max_tokens_limit, assuming {default_max_tokens_limit}\")\n        if default_token_estimation_factor is None:\n            default_token_estimation_factor = self.DEFAULT_TOKEN_ESTIMATION_FACTOR\n            logger.warning(f\"Model '{self._model_name}': unknown token_estimation_factor, assuming {default_token_estimation_factor}\")\n\n\n    self.ctx_len = ctx_len or default_ctx_len\n\n    self.max_tokens_limit = max_tokens_limit or default_max_tokens_limit\n\n    self.max_tokens_limit = min(self.max_tokens_limit, self.ctx_len)\n\n    self._token_estimation_factor = token_estimation_factor or default_token_estimation_factor\n\n\n    # only check for \"json\" text presence as json schema (including field descriptions) is requested with the tools facility.\n    self.json_format_instructors[\"json_schema\"] = self.json_format_instructors[\"json\"]\n\n    self._client_init_kwargs = anthropic_init_kwargs\n\n    if api_key is not None:\n        self._client_init_kwargs[\"api_key\"] = api_key    \n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/anthropic.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n\n    genconf2: GenConf\n    kwargs, genconf2 = self._gen_pre(thread, genconf)\n\n    self._ensure_client(False)\n\n    try:\n        if \"tools\" in kwargs:\n            response = self._client.beta.tools.messages.create(**kwargs) # type: ignore[attr-defined]\n        else:\n            response = self._client.messages.create(**kwargs) # type: ignore[attr-defined]\n\n    except Exception as e:\n        raise RuntimeError(f\"Cannot generate. Internal error: {e}\")\n\n\n    return self._gen_post(response,\n                          kwargs,\n                          genconf2)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.token_len","title":"token_len","text":"<pre><code>token_len(thread_or_text, genconf=None)\n</code></pre> <p>Estimate the number of tokens used by a Thread or text string.</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens occupied.</p> Source code in <code>sibila/anthropic.py</code> <pre><code>def token_len(self,\n              thread_or_text: Union[Thread,str],\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Estimate the number of tokens used by a Thread or text string.\n\n    Args:\n        thread_or_text: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens occupied.\n    \"\"\"\n\n    if isinstance(thread_or_text, Thread):\n        thread = thread_or_text            \n    else:\n        thread = Thread.make_IN(thread_or_text)\n\n    OVERHEAD_PER_MSG = 3\n    num_tokens = 0\n    for index in range(-1, len(thread)): # -1 for system message\n        message = thread.msg_as_chatml(index)\n        msg_tokens = len(message[\"content\"]) * self._token_estimation_factor + OVERHEAD_PER_MSG\n        num_tokens += int(msg_tokens)\n\n    if genconf is not None and genconf.json_schema is not None:\n        if isinstance(genconf.json_schema, str):\n            js_str = genconf.json_schema\n        else:\n            js_str = json.dumps(genconf.json_schema)\n\n        tools_num_tokens = len(js_str) * self._token_estimation_factor\n        num_tokens += int(tools_num_tokens)\n        # print(\"tools_num_tokens\", tools_num_tokens)\n\n    # print(num_tokens)\n    return num_tokens\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = tokenizer\n</code></pre> <p>Tokenizer used to encode text (even for message-based models). Some remote models don't have tokenizer and token length is estimated</p>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = ctx_len or default_ctx_len\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>If the model can only use a fixed set of models, return their names. Otherwise, return None.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>If the model provider requires an API key, pass it here or set it in the respective env variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models or None if unable to fetch it.</p> Source code in <code>sibila/model.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"If the model can only use a fixed set of models, return their names. Otherwise, return None.\n\n    Args:\n        api_key: If the model provider requires an API key, pass it here or set it in the respective env variable.\n\n    Returns:\n        Returns a list of known models or None if unable to fetch it.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.AnthropicModel.desc","title":"desc","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/anthropic.py</code> <pre><code>def desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    return f\"AnthropicModel: {self._model_name}\"\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel","title":"MistralModel","text":"<pre><code>MistralModel(\n    name,\n    *,\n    genconf=None,\n    schemaconf=None,\n    ctx_len=None,\n    max_tokens_limit=None,\n    api_key=None,\n    token_estimation_factor=None,\n    mistral_init_kwargs={}\n)\n</code></pre> <p>Access a Mistral AI model. Supports constrained JSON output, via the Mistral API function calling mechanism.</p> Ref <p>https://docs.mistral.ai/guides/function-calling/</p> <p>Create a Mistral AI remote model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name to resolve into an existing model.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used (shared for input and output). None for model's default.</p> <code>None</code> <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Maximum output tokens limit. None for model's default.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Mistral API key. Defaults to None, which will use env variable MISTRAL_API_KEY.</p> <code>None</code> <code>token_estimation_factor</code> <code>Optional[float]</code> <p>Multiplication factor to estimate token usage: multiplies total text length to obtain token length.</p> <code>None</code> <code>mistral_init_kwargs</code> <code>dict</code> <p>Extra args for mistral.MistralClient() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If Mistral API is not installed.</p> <code>NameError</code> <p>If model name was not found or there's an API or authentication problem.</p> Source code in <code>sibila/mistral.py</code> <pre><code>def __init__(self,\n             name: str,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             ctx_len: Optional[int] = None,\n             max_tokens_limit: Optional[int] = None,\n\n             # most important Mistral-specific args\n             api_key: Optional[str] = None,\n             token_estimation_factor: Optional[float] = None,\n\n             # other Mistral-specific args\n             mistral_init_kwargs: dict = {},\n             ):\n    \"\"\"Create a Mistral AI remote model.\n\n    Args:\n        name: Model name to resolve into an existing model.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). None for model's default.\n        max_tokens_limit: Maximum output tokens limit. None for model's default.\n        api_key: Mistral API key. Defaults to None, which will use env variable MISTRAL_API_KEY.\n        token_estimation_factor: Multiplication factor to estimate token usage: multiplies total text length to obtain token length.\n        mistral_init_kwargs: Extra args for mistral.MistralClient() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If Mistral API is not installed.\n        NameError: If model name was not found or there's an API or authentication problem.\n    \"\"\"\n\n\n    if not has_mistral:\n        raise ImportError(\"Please install mistral by running: pip install mistralai\")\n\n    self._client = self._client_async = None\n\n\n    # also accept \"provider:name\" for ease of use\n    provider_name = self.PROVIDER_NAME + \":\"\n    if name.startswith(provider_name):\n        name = name[len(provider_name):]\n\n    super().__init__(False,\n                     genconf,\n                     schemaconf,\n                     None\n                     )\n\n    if (ctx_len is not None and\n        max_tokens_limit is not None and\n        token_estimation_factor is not None): # all elements given: probably created via Models.create()\n\n        self._model_name = name\n        default_ctx_len = ctx_len\n        default_max_tokens_limit = max_tokens_limit\n        default_token_estimation_factor = token_estimation_factor\n\n    else: # need to resolve\n        settings = self.resolve_settings(self.PROVIDER_NAME,\n                                         name,\n                                         [\"name\", \n                                          \"ctx_len\", \n                                          \"max_tokens_limit\", \n                                          \"token_estimation_factor\"])\n        self._model_name = settings.get(\"name\") or name\n        default_ctx_len = settings.get(\"ctx_len\") # type: ignore[assignment]\n        default_max_tokens_limit = settings.get(\"max_tokens_limit\") or default_ctx_len\n        default_token_estimation_factor = settings.get(\"token_estimation_factor\") # type: ignore[assignment]\n\n        # all defaults are conservative values\n        if default_ctx_len is None:\n            default_ctx_len = 32768\n            logger.warning(f\"Model '{self._model_name}': unknown ctx_len, assuming {default_ctx_len}\")\n        if default_max_tokens_limit is None:\n            default_max_tokens_limit = default_ctx_len\n            logger.warning(f\"Model '{self._model_name}': unknown max_tokens_limit, assuming {default_max_tokens_limit}\")\n        if default_token_estimation_factor is None:\n            default_token_estimation_factor = self.DEFAULT_TOKEN_ESTIMATION_FACTOR\n            logger.warning(f\"Model '{self._model_name}': unknown token_estimation_factor, assuming {default_token_estimation_factor}\")\n\n\n    self.ctx_len = ctx_len or default_ctx_len\n\n    self.max_tokens_limit = max_tokens_limit or default_max_tokens_limit\n\n    self.max_tokens_limit = min(self.max_tokens_limit, self.ctx_len)\n\n    self._token_estimation_factor = token_estimation_factor or default_token_estimation_factor\n\n\n    # only check for \"json\" text presence as json schema (including field descriptions) is requested with the tools facility.\n    self.json_format_instructors[\"json_schema\"] = self.json_format_instructors[\"json\"]\n\n    self._client_init_kwargs = mistral_init_kwargs\n\n    if api_key is not None:\n        self._client_init_kwargs[\"api_key\"] = api_key    \n    elif \"api_key\" not in self._client_init_kwargs and \"MISTRAL_API_KEY\" in os.environ:\n        # \"MISTRAL_API_KEY\" env key is ignored in pytest?\n        self._client_init_kwargs[\"api_key\"] = os.environ[\"MISTRAL_API_KEY\"]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/mistral.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n\n    genconf2: GenConf\n    kwargs, genconf2 = self._gen_pre(thread, genconf)\n\n    self._ensure_client(False)\n\n    try:\n        response = self._client.chat(**kwargs) # type: ignore[attr-defined]\n\n    except Exception as e:\n        raise RuntimeError(f\"Cannot generate. Internal error: {e}\")\n\n\n    return self._gen_post(response,\n                          kwargs,\n                          genconf2)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.token_len","title":"token_len","text":"<pre><code>token_len(thread_or_text, genconf=None)\n</code></pre> <p>Estimate the number of tokens used by a Thread or text string.</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens occupied.</p> Source code in <code>sibila/mistral.py</code> <pre><code>def token_len(self,\n              thread_or_text: Union[Thread,str],\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Estimate the number of tokens used by a Thread or text string.\n\n    Args:\n        thread_or_text: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens occupied.\n    \"\"\"\n\n    if isinstance(thread_or_text, Thread):\n        thread = thread_or_text            \n    else:\n        thread = Thread.make_IN(thread_or_text)\n\n    OVERHEAD_PER_MSG = 3\n    num_tokens = 0\n    for index in range(-1, len(thread)): # -1 for system message\n        message = thread.msg_as_chatml(index)\n        msg_tokens = len(message[\"content\"]) * self._token_estimation_factor + OVERHEAD_PER_MSG\n        num_tokens += int(msg_tokens)\n\n    if genconf is not None and genconf.json_schema is not None:\n        if isinstance(genconf.json_schema, str):\n            js_str = genconf.json_schema\n        else:\n            js_str = json.dumps(genconf.json_schema)\n\n        tools_num_tokens = len(js_str) * self._token_estimation_factor\n        num_tokens += int(tools_num_tokens)\n        # print(\"tools_num_tokens\", tools_num_tokens)\n\n    # print(num_tokens)\n    return num_tokens\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = tokenizer\n</code></pre> <p>Tokenizer used to encode text (even for message-based models). Some remote models don't have tokenizer and token length is estimated</p>"},{"location":"api-reference/remote_model/#sibila.MistralModel.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = ctx_len or default_ctx_len\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>If the model can only use a fixed set of models, return their names. Otherwise, return None.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>If the model provider requires an API key, pass it here or set it in the respective env variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models or None if unable to fetch it.</p> Source code in <code>sibila/mistral.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"If the model can only use a fixed set of models, return their names. Otherwise, return None.\n\n    Args:\n        api_key: If the model provider requires an API key, pass it here or set it in the respective env variable.\n\n    Returns:\n        Returns a list of known models or None if unable to fetch it.\n    \"\"\"\n\n    args = {}\n    if api_key is not None:\n        args[\"api_key\"] = api_key\n    model = MistralClient(**args) # type: ignore[arg-type]\n\n    model_list = model.list_models()\n    del model\n\n    out = []\n    for mod in model_list.data:\n        out.append(mod.id)\n\n    return sorted(out)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.MistralModel.desc","title":"desc","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/mistral.py</code> <pre><code>def desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    return f\"MistralModel: {self._model_name}\"\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel","title":"TogetherModel","text":"<pre><code>TogetherModel(\n    name,\n    *,\n    genconf=None,\n    schemaconf=None,\n    ctx_len=None,\n    max_tokens_limit=None,\n    tokenizer=None,\n    api_key=None,\n    base_url=None,\n    token_estimation_factor=None,\n    other_init_kwargs={}\n)\n</code></pre> <p>Access a together.ai model with the OpenAI API. Supports constrained JSON output, via the response_format JSON Schema mechanism.</p> Ref <p>https://docs.together.ai/docs/json-mode</p> <p>https://docs.together.ai/reference/chat-completions</p> <p>Create a together.ai remote model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name to resolve into an existing model.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used (shared for input and output). None for model's default.</p> <code>None</code> <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Maximum output tokens limit. None for model's default.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key. Defaults to None, which will use env variable TOGETHER_API_KEY.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base location for API access. Defaults to None, which will use env variable TOGETHER_BASE_URL or a default.</p> <code>None</code> <code>token_estimation_factor</code> <code>Optional[float]</code> <p>Used when no tokenizer is available. Multiplication factor to estimate token usage: multiplies total text length to obtain token length.</p> <code>None</code> <code>other_init_kwargs</code> <code>dict</code> <p>Extra args for OpenAI.OpenAI() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If OpenAI API is not installed.</p> <code>NameError</code> <p>If model name was not found or there's an API or authentication problem.</p> Source code in <code>sibila/schema_format_openai.py</code> <pre><code>def __init__(self,\n             name: str,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             ctx_len: Optional[int] = None,\n             max_tokens_limit: Optional[int] = None,\n             tokenizer: Optional[Tokenizer] = None,\n\n             # most important OpenAI API specific args\n             api_key: Optional[str] = None,\n             base_url: Optional[str] = None,\n             token_estimation_factor: Optional[float] = None,\n\n             # other OpenAI API specific args\n             other_init_kwargs: dict = {},\n             ):\n    \"\"\"Create a together.ai remote model.\n\n    Args:\n        name: Model name to resolve into an existing model.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). None for model's default.\n        max_tokens_limit: Maximum output tokens limit. None for model's default.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        api_key: API key. Defaults to None, which will use env variable TOGETHER_API_KEY.\n        base_url: Base location for API access. Defaults to None, which will use env variable TOGETHER_BASE_URL or a default.\n        token_estimation_factor: Used when no tokenizer is available. Multiplication factor to estimate token usage: multiplies total text length to obtain token length.\n        other_init_kwargs: Extra args for OpenAI.OpenAI() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If OpenAI API is not installed.\n        NameError: If model name was not found or there's an API or authentication problem.\n    \"\"\"\n\n    if api_key is None:\n        api_key = os.environ.get(\"TOGETHER_API_KEY\")\n    if base_url is None:\n        base_url = os.environ.get(\"TOGETHER_BASE_URL\", self.DEFAULT_BASE_URL)\n\n    super().__init__(name,\n                     # common base model args\n                     genconf=genconf,\n                     schemaconf=schemaconf,\n                     ctx_len=ctx_len,\n                     max_tokens_limit=max_tokens_limit,\n                     tokenizer=tokenizer,\n\n                     # most important OpenAI API specific args\n                     api_key=api_key,\n                     base_url=base_url,\n                     token_estimation_factor=token_estimation_factor,\n\n                     # other OpenAI API specific args\n                     other_init_kwargs=other_init_kwargs)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n    genconf2: GenConf\n    kwargs, genconf2 = self._gen_pre(thread, genconf)\n\n    self._ensure_client(False)\n\n    try:\n        # https://platform.openai.com/docs/api-reference/chat/create\n        response = self._client.chat.completions.create(**kwargs) # type: ignore[attr-defined]\n\n    except Exception as e:\n        raise RuntimeError(f\"Cannot generate. Internal error: {e}\")\n\n\n    return self._gen_post(response,\n                          kwargs,\n                          genconf2)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.token_len","title":"token_len","text":"<pre><code>token_len(thread_or_text, genconf=None)\n</code></pre> <p>Estimate the number of tokens used by a Thread or text string. If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.</p> <p>From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p> <p>More info on calculating function_call (and tools?) tokens:</p> <p>https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24</p> <p>https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens used.</p> Source code in <code>sibila/openai.py</code> <pre><code>def token_len(self,\n              thread_or_text: Union[Thread,str],\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Estimate the number of tokens used by a Thread or text string.\n    If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.\n\n    From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    More info on calculating function_call (and tools?) tokens:\n\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24\n\n    https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5\n\n    Args:\n        thread_or_text: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens used.\n    \"\"\"\n\n    if isinstance(thread_or_text, Thread):\n        thread = thread_or_text            \n    else:\n        thread = Thread.make_IN(thread_or_text)\n\n    num_tokens = 0\n\n    if self.tokenizer is None: # no tokenizer was found, so we'll have to do a conservative estimate\n\n        OVERHEAD_PER_MSG = 3\n        for index in range(-1, len(thread)): # -1 for system message\n            message = thread.msg_as_chatml(index)\n            msg_tokens = len(message[\"content\"]) * self._token_estimation_factor + OVERHEAD_PER_MSG\n            num_tokens += int(msg_tokens)\n\n        if genconf is not None and genconf.json_schema is not None:\n            if isinstance(genconf.json_schema, str):\n                js_str = genconf.json_schema\n            else:\n                js_str = json.dumps(genconf.json_schema)\n\n            tools_num_tokens = len(js_str) * self._token_estimation_factor\n            num_tokens += int(tools_num_tokens)\n            # print(\"tools_num_tokens\", tools_num_tokens)\n\n    else: # do an \"informed\" token estimation from what is known of the OpenAI model's tokenization\n\n        for index in range(-1, len(thread)): # -1 for system message\n            message = thread.msg_as_chatml(index)\n            # print(message)\n            num_tokens += self._overhead_per_msg\n            for key, value in message.items():\n                num_tokens += len(self.tokenizer.encode(value))\n\n        # add extras + every reply is primed with &lt;|start|&gt;assistant&lt;|message|&gt;\n        num_tokens += 32\n\n        # print(\"text token_len\", num_tokens)\n\n        if genconf is not None and genconf.json_schema is not None:\n            TOOLS_TOKEN_LEN_FACTOR = 1.2\n\n            if isinstance(genconf.json_schema, str):\n                js_str = genconf.json_schema\n            else:\n                js_str = json.dumps(genconf.json_schema)\n\n            tools_num_tokens = self.tokenizer.token_len(js_str)\n\n            # this is an upper bound, as empirically tested with the api.\n            tools_num_tokens = int(tools_num_tokens * TOOLS_TOKEN_LEN_FACTOR)\n            # print(\"tools token_len\", tools_num_tokens)\n\n            num_tokens += tools_num_tokens\n\n\n    return num_tokens\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = OpenAITokenizer(_model_name)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = ctx_len or default_ctx_len\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>List of model names that can be used. Some of the models are not chat models and cannot be used, for example embedding models.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>If the model provider requires an API key, pass it here or set it in the respective env variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models or None if unable to fetch it.</p> Source code in <code>sibila/schema_format_openai.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"List of model names that can be used. Some of the models are not chat models and cannot be used,\n    for example embedding models.\n\n    Args:\n        api_key: If the model provider requires an API key, pass it here or set it in the respective env variable.\n\n    Returns:\n        Returns a list of known models or None if unable to fetch it.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.TogetherModel.desc","title":"desc","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/openai.py</code> <pre><code>def desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    return f\"{type(self).__name__}: '{self._model_name}'\"\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel","title":"FireworksModel","text":"<pre><code>FireworksModel(\n    name,\n    *,\n    genconf=None,\n    schemaconf=None,\n    ctx_len=None,\n    max_tokens_limit=None,\n    tokenizer=None,\n    api_key=None,\n    base_url=None,\n    token_estimation_factor=None,\n    other_init_kwargs={}\n)\n</code></pre> <p>Access a Fireworks AI model with the OpenAI API. Supports constrained JSON output, via the response_format JSON Schema mechanism.</p> Ref <p>https://readme.fireworks.ai/docs/structured-response-formatting</p> <p>https://readme.fireworks.ai/reference/createchatcompletion</p> <p>Create a Fireworks AI remote model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name to resolve into an existing model.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.</p> <code>None</code> <code>ctx_len</code> <code>Optional[int]</code> <p>Maximum context length to be used (shared for input and output). None for model's default.</p> <code>None</code> <code>max_tokens_limit</code> <code>Optional[int]</code> <p>Maximum output tokens limit. None for model's default.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key. Defaults to None, which will use env variable FIREWORKS_API_KEY.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base location for API access. Defaults to None, which will use env variable FIREWORKS_BASE_URL or a default.</p> <code>None</code> <code>token_estimation_factor</code> <code>Optional[float]</code> <p>Used when no tokenizer is available. Multiplication factor to estimate token usage: multiplies total text length to obtain token length.</p> <code>None</code> <code>other_init_kwargs</code> <code>dict</code> <p>Extra args for OpenAI.OpenAI() initialization. Defaults to {}.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If OpenAI API is not installed.</p> <code>NameError</code> <p>If model name was not found or there's an API or authentication problem.</p> Source code in <code>sibila/schema_format_openai.py</code> <pre><code>def __init__(self,\n             name: str,\n             *,\n\n             # common base model args\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None,\n             ctx_len: Optional[int] = None,\n             max_tokens_limit: Optional[int] = None,\n             tokenizer: Optional[Tokenizer] = None,\n\n             # most important OpenAI API specific args\n             api_key: Optional[str] = None,\n             base_url: Optional[str] = None,\n             token_estimation_factor: Optional[float] = None,\n\n             # other OpenAI API specific args\n             other_init_kwargs: dict = {},\n             ):\n    \"\"\"Create a Fireworks AI remote model.\n\n    Args:\n        name: Model name to resolve into an existing model.\n        genconf: Model generation configuration. Defaults to None.\n        schemaconf: Default configuration for JSON schema validation, used if generation call doesn't supply one. Defaults to None.\n        ctx_len: Maximum context length to be used (shared for input and output). None for model's default.\n        max_tokens_limit: Maximum output tokens limit. None for model's default.\n        tokenizer: An external initialized tokenizer to use instead of the created from the GGUF file. Defaults to None.\n        api_key: API key. Defaults to None, which will use env variable FIREWORKS_API_KEY.\n        base_url: Base location for API access. Defaults to None, which will use env variable FIREWORKS_BASE_URL or a default.\n        token_estimation_factor: Used when no tokenizer is available. Multiplication factor to estimate token usage: multiplies total text length to obtain token length.\n        other_init_kwargs: Extra args for OpenAI.OpenAI() initialization. Defaults to {}.\n\n    Raises:\n        ImportError: If OpenAI API is not installed.\n        NameError: If model name was not found or there's an API or authentication problem.\n    \"\"\"\n\n    if api_key is None:\n        api_key = os.environ.get(\"FIREWORKS_API_KEY\")\n    if base_url is None:\n        base_url = os.environ.get(\"FIREWORKS_BASE_URL\", self.DEFAULT_BASE_URL)\n\n    super().__init__(name,\n                     # common base model args\n                     genconf=genconf,\n                     schemaconf=schemaconf,\n                     ctx_len=ctx_len,\n                     max_tokens_limit=max_tokens_limit,\n                     tokenizer=tokenizer,\n\n                     # most important OpenAI API specific args\n                     api_key=api_key,\n                     base_url=base_url,\n                     token_estimation_factor=token_estimation_factor,\n\n                     # other OpenAI API specific args\n                     other_init_kwargs=other_init_kwargs)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.extract","title":"extract","text":"<pre><code>extract(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>def extract(self,\n            target: Any,\n\n            query: Union[str,Thread],\n            *,\n            inst: Optional[str] = None,\n\n            genconf: Optional[GenConf] = None,\n            schemaconf: Optional[JSchemaConf] = None\n            ) -&gt; Any:        \n    \"\"\"Type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_extract(target,\n                           thread,\n                           genconf,\n                           schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.classify","title":"classify","text":"<pre><code>classify(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>def classify(self,\n             labels: Any,\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return self.extract(labels,\n                        query,\n                        inst=inst,\n                        genconf=genconf,\n                        schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.json","title":"json","text":"<pre><code>json(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>def json(self,\n         query: Union[str,Thread],\n         *,\n         json_schema: Union[dict,str,None] = None,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         massage_schema: bool = True,\n         schemaconf: Optional[JSchemaConf] = None,\n         ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_json(thread,\n                        json_schema,                            \n                        genconf,\n                        massage_schema,\n                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.dataclass","title":"dataclass","text":"<pre><code>dataclass(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def dataclass(self, # noqa: F811\n              cls: Any, # a dataclass definition\n\n              query: Union[str,Thread],\n              *,\n              inst: Optional[str] = None,\n\n              genconf: Optional[GenConf] = None,\n              schemaconf: Optional[JSchemaConf] = None\n              ) -&gt; Any: # a dataclass object\n    \"\"\"Constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_dataclass(cls,\n                             thread,\n                             genconf,\n                             schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.pydantic","title":"pydantic","text":"<pre><code>pydantic(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>def pydantic(self,\n             cls: Any, # a Pydantic BaseModel class\n\n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             schemaconf: Optional[JSchemaConf] = None\n             ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen_pydantic(cls,\n                            thread,\n                            genconf,\n                            schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.call","title":"call","text":"<pre><code>call(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def call(self,             \n         query: Union[str,Thread],\n         *,\n         inst: Optional[str] = None,\n\n         genconf: Optional[GenConf] = None,\n         ok_length_is_error: bool = False\n         ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = self.gen(thread=thread, \n                   genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.__call__","title":"__call__","text":"<pre><code>__call__(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods. Same as call().</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>def __call__(self,             \n             query: Union[str,Thread],\n             *,\n             inst: Optional[str] = None,\n\n             genconf: Optional[GenConf] = None,\n             ok_length_is_error: bool = False\n             ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods. Same as call().\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    return self.call(query,\n                     inst=inst,\n                     genconf=genconf,\n                     ok_length_is_error=ok_length_is_error)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    target,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Async type-constrained generation: an instance of the given type will be initialized with the model's output. The following target types are accepted:</p> <ul> <li> <p>prim_type:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> </ul> </li> <li> <p>enums:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> </li> <li> <p>datetime/date/time</p> </li> <li> <p>a list in the form:</p> <ul> <li>list[type]</li> </ul> <p>For example list[int]. The list can be annotated:     Annotated[list[T], \"List desc\"] And/or the list item type can be annotated:     list[Annotated[T, \"Item desc\"]]</p> </li> <li> <p>dataclass with fields of the above supported types (or dataclass).</p> </li> <li> <p>Pydantic BaseModel</p> </li> </ul> <p>All types can be Annotated[T, \"Desc\"], for example:      count: int Can be annotated as:     count: Annotated[int, \"How many units?\"]</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A value of target arg type instantiated with the model's output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def extract_async(self,\n                        target: Any,\n\n                        query: Union[str,Thread],\n                        *,\n                        inst: Optional[str] = None,\n\n                        genconf: Optional[GenConf] = None,\n                        schemaconf: Optional[JSchemaConf] = None\n                        ) -&gt; Any:        \n    \"\"\"Async type-constrained generation: an instance of the given type will be initialized with the model's output.\n    The following target types are accepted:\n\n    - prim_type:\n\n        - bool\n        - int\n        - float\n        - str\n\n    - enums:\n\n        - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n        - Literal['year', 'name'] - all items of the same prim_type\n        - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    - datetime/date/time\n\n    - a list in the form:\n        - list[type]\n\n        For example list[int]. The list can be annotated:\n            Annotated[list[T], \"List desc\"]\n        And/or the list item type can be annotated:\n            list[Annotated[T, \"Item desc\"]]\n\n    - dataclass with fields of the above supported types (or dataclass).\n\n    - Pydantic BaseModel\n\n    All types can be Annotated[T, \"Desc\"], for example: \n        count: int\n    Can be annotated as:\n        count: Annotated[int, \"How many units?\"]\n\n    Args:\n        target: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A value of target arg type instantiated with the model's output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_extract_async(target,\n                                       thread,\n                                       genconf,\n                                       schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.classify_async","title":"classify_async  <code>async</code>","text":"<pre><code>classify_async(\n    labels,\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    schemaconf=None\n)\n</code></pre> <p>Returns a classification from one of the given enumeration values The following ways to specify the valid labels are accepted:</p> <ul> <li>[1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type</li> <li>Literal['year', 'name'] - all items of the same prim_type</li> <li>Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type</li> </ul> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Any</code> <p>One of the above types.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>One of the given labels, as classified by the model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def classify_async(self,\n                         labels: Any,\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any:\n    \"\"\"Returns a classification from one of the given enumeration values\n    The following ways to specify the valid labels are accepted:\n\n    - [1, 2, 3] or [\"a\",\"b\"] - all items of the same prim_type\n    - Literal['year', 'name'] - all items of the same prim_type\n    - Enum, EnumInt, EnumStr, (Enum, int),... - all items of the same prim_type\n\n    Args:\n        labels: One of the above types.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        One of the given labels, as classified by the model.\n    \"\"\"\n\n    # verify it's a valid enum \"type\"\n    type_,_ = get_enum_type(labels)\n    if type_ is None:\n        raise TypeError(\"Arg labels must be one of Literal, Enum class or a list of str, float or int items\")\n\n    return await self.extract_async(labels,\n                                    query,\n                                    inst=inst,\n                                    genconf=genconf,\n                                    schemaconf=schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.json_async","title":"json_async  <code>async</code>","text":"<pre><code>json_async(\n    query,\n    *,\n    json_schema=None,\n    inst=None,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema. Raises GenError if unable to get a valid/schema-validated JSON.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> <code>None</code> <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid JSON schema output error. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict from model's JSON response, following genconf.jsonschema, if provided.</p> Source code in <code>sibila/model.py</code> <pre><code>async def json_async(self,             \n                     query: Union[str,Thread],\n                     *,\n                     json_schema: Union[dict,str,None] = None,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     massage_schema: bool = True,\n                     schemaconf: Optional[JSchemaConf] = None,\n                     ) -&gt; dict:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, constrained or not by a JSON schema.\n    Raises GenError if unable to get a valid/schema-validated JSON.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid JSON schema output error. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A dict from model's JSON response, following genconf.jsonschema, if provided.\n    \"\"\"        \n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_json_async(thread,\n                                    json_schema,\n                                    genconf,\n                                    massage_schema,\n                                    schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.dic # type: ignore[return-value]\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.dataclass_async","title":"dataclass_async  <code>async</code>","text":"<pre><code>dataclass_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response. Raises GenError if unable to get a valid response that follows the dataclass definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example invalid object initialization. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>An object of class cls (derived from dataclass) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def dataclass_async(self, # noqa: E811\n                          cls: Any, # a dataclass definition\n\n                          query: Union[str,Thread],\n                          *,\n                          inst: Optional[str] = None,\n\n                          genconf: Optional[GenConf] = None,\n                          schemaconf: Optional[JSchemaConf] = None\n                          ) -&gt; Any: # a dataclass object\n    \"\"\"Async constrained generation after a dataclass definition, resulting in an object initialized with the model's response.\n    Raises GenError if unable to get a valid response that follows the dataclass definition.\n\n    Args:\n        cls: A dataclass definition.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example invalid object initialization. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        An object of class cls (derived from dataclass) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_dataclass_async(cls,\n                                         thread,\n                                         genconf,\n                                         schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.pydantic_async","title":"pydantic_async  <code>async</code>","text":"<pre><code>pydantic_async(\n    cls, query, *, inst=None, genconf=None, schemaconf=None\n)\n</code></pre> <p>Async constrained generation after a Pydantic BaseModel-derived class definition. Results in an object initialized with the model response. Raises GenError if unable to get a valid dict that follows the BaseModel class definition.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred, for example an invalid BaseModel object. See GenError.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.</p> Source code in <code>sibila/model.py</code> <pre><code>async def pydantic_async(self,\n                         cls: Any, # a Pydantic BaseModel class\n\n                         query: Union[str,Thread],\n                         *,\n                         inst: Optional[str] = None,\n\n                         genconf: Optional[GenConf] = None,\n                         schemaconf: Optional[JSchemaConf] = None\n                         ) -&gt; Any: # a Pydantic BaseModel object\n    \"\"\"Async constrained generation after a Pydantic BaseModel-derived class definition.\n    Results in an object initialized with the model response.\n    Raises GenError if unable to get a valid dict that follows the BaseModel class definition.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        GenError: If an error occurred, for example an invalid BaseModel object. See GenError.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A Pydantic object of class cls (derived from BaseModel) initialized from the constrained JSON output.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_pydantic_async(cls,\n                                        thread,\n                                        genconf,\n                                        schemaconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=False) # as valid JSON can still be produced\n\n    return out.value\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.call_async","title":"call_async  <code>async</code>","text":"<pre><code>call_async(\n    query,\n    *,\n    inst=None,\n    genconf=None,\n    ok_length_is_error=False\n)\n</code></pre> <p>Text generation from a Thread or plain text, used by the other model generation methods.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Thread]</code> <p>Thread or an str with the text of a single IN message to use as model input.</p> required <code>inst</code> <code>Optional[str]</code> <p>Instruction message for model. Will override Thread's inst, if set. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>ok_length_is_error</code> <code>bool</code> <p>Should a result of GenRes.OK_LENGTH be considered an error and raise?</p> <code>False</code> <p>Raises:</p> Type Description <code>GenError</code> <p>If an error occurred. This can be a model error, or an invalid JSON output error.</p> <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>str</code> <p>Text generated by model.</p> Source code in <code>sibila/model.py</code> <pre><code>async def call_async(self,\n                     query: Union[str,Thread],\n                     *,\n                     inst: Optional[str] = None,\n\n                     genconf: Optional[GenConf] = None,\n                     ok_length_is_error: bool = False\n                     ) -&gt; str:\n    \"\"\"Text generation from a Thread or plain text, used by the other model generation methods.\n\n    Args:\n        query: Thread or an str with the text of a single IN message to use as model input.\n        inst: Instruction message for model. Will override Thread's inst, if set. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        ok_length_is_error: Should a result of GenRes.OK_LENGTH be considered an error and raise?\n\n    Raises:\n        GenError: If an error occurred. This can be a model error, or an invalid JSON output error.\n        RuntimeError: If unable to generate.\n\n    Returns:\n        Text generated by model.\n    \"\"\"\n\n    thread = Thread.ensure(query, inst)\n\n    out = await self.gen_async(thread=thread, \n                               genconf=genconf)\n\n    GenError.raise_if_error(out,\n                            ok_length_is_error=ok_length_is_error)\n\n    return out.text\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.gen","title":"gen","text":"<pre><code>gen(thread, genconf=None)\n</code></pre> <p>Text generation from a Thread, used by the other model generation methods. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc.</p> <code>GenOut</code> <p>The output text is in GenOut.text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def gen(self, \n        thread: Thread,\n        genconf: Optional[GenConf] = None,\n        ) -&gt; GenOut:\n    \"\"\"Text generation from a Thread, used by the other model generation methods.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc.\n        The output text is in GenOut.text.\n    \"\"\"\n\n    genconf2: GenConf\n    kwargs, genconf2 = self._gen_pre(thread, genconf)\n\n    self._ensure_client(False)\n\n    try:\n        # https://platform.openai.com/docs/api-reference/chat/create\n        response = self._client.chat.completions.create(**kwargs) # type: ignore[attr-defined]\n\n    except Exception as e:\n        raise RuntimeError(f\"Cannot generate. Internal error: {e}\")\n\n\n    return self._gen_post(response,\n                          kwargs,\n                          genconf2)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.gen_json","title":"gen_json","text":"<pre><code>gen_json(\n    thread,\n    json_schema,\n    genconf=None,\n    massage_schema=True,\n    schemaconf=None,\n)\n</code></pre> <p>JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema. Doesn't raise an exception if an error occurs, always returns GenOut.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>json_schema</code> <code>Union[dict, str, None]</code> <p>A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>massage_schema</code> <code>bool</code> <p>Simplify schema. Defaults to True.</p> <code>True</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_json(self,\n             thread: Thread,\n             json_schema: Union[dict,str,None],\n             genconf: Optional[GenConf] = None,\n\n             massage_schema: bool = True,\n             schemaconf: Optional[JSchemaConf] = None,\n             ) -&gt; GenOut:\n    \"\"\"JSON/JSON-schema constrained generation, returning a Python dict of values, conditioned or not by a JSON schema.\n    Doesn't raise an exception if an error occurs, always returns GenOut.\n\n    Args:\n        thread: The Thread to use as model input.\n        json_schema: A JSON schema describing the dict fields that will be output. None means no schema (free JSON output).\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        massage_schema: Simplify schema. Defaults to True.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The output dict is in GenOut.dic.\n    \"\"\"\n\n    args = self._gen_json_pre(thread,\n                              json_schema,\n                              genconf,\n                              massage_schema,\n                              schemaconf)\n    return self.gen(*args)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.gen_dataclass","title":"gen_dataclass","text":"<pre><code>gen_dataclass(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a dataclass definition. An initialized dataclass object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A dataclass definition.</p> required <code>thread</code> <code>Thread</code> <p>The Thread object to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_dataclass(self,\n                  cls: Any, # a dataclass\n                  thread: Thread,\n                  genconf: Optional[GenConf] = None,\n                  schemaconf: Optional[JSchemaConf] = None\n                  ) -&gt; GenOut:\n    \"\"\"Constrained generation after a dataclass definition.\n    An initialized dataclass object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A dataclass definition.\n        thread: The Thread object to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized dataclass object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_dataclass_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_dataclass_post(out,\n                                    cls,\n                                    schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.gen_pydantic","title":"gen_pydantic","text":"<pre><code>gen_pydantic(cls, thread, genconf=None, schemaconf=None)\n</code></pre> <p>Constrained generation after a Pydantic BaseModel-derived class definition. An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict. Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Any</code> <p>A class derived from a Pydantic BaseModel class.</p> required <code>thread</code> <code>Thread</code> <p>The Thread to use as model input.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses model's default.</p> <code>None</code> <code>schemaconf</code> <code>Optional[JSchemaConf]</code> <p>JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to generate.</p> <code>TypeError</code> <p>When cls is not a Pydantic BaseClass.</p> <p>Returns:</p> Type Description <code>GenOut</code> <p>A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.</p> Source code in <code>sibila/model.py</code> <pre><code>def gen_pydantic(self,\n                 cls: Any, # a Pydantic BaseModel class\n                 thread: Thread,\n                 genconf: Optional[GenConf] = None,\n                 schemaconf: Optional[JSchemaConf] = None\n                 ) -&gt; GenOut:\n    \"\"\"Constrained generation after a Pydantic BaseModel-derived class definition.\n    An initialized Pydantic BaseModel object is returned in the \"value\" field of the returned dict.\n    Doesn't raise an exception if an error occurs, always returns GenOut containing the created object.\n\n    Args:\n        cls: A class derived from a Pydantic BaseModel class.\n        thread: The Thread to use as model input.\n        genconf: Model generation configuration. Defaults to None, which uses model's default.\n        schemaconf: JSchemaConf object that controls schema simplification. Defaults to None, which uses model's default.\n\n    Raises:\n        RuntimeError: If unable to generate.\n        TypeError: When cls is not a Pydantic BaseClass.\n\n    Returns:\n        A GenOut object with result, generated text, etc. The initialized Pydantic BaseModel-derived object is in GenOut.value.\n    \"\"\"\n\n    schema = self._gen_pydantic_pre(cls)\n\n    out = self.gen_json(thread,\n                        schema,\n                        genconf,\n                        massage_schema=True,\n                        schemaconf=schemaconf)\n\n    return self._gen_pydantic_post(out,\n                                   cls,\n                                   schemaconf)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.token_len","title":"token_len","text":"<pre><code>token_len(thread_or_text, genconf=None)\n</code></pre> <p>Estimate the number of tokens used by a Thread or text string. If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.</p> <p>From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p> <p>More info on calculating function_call (and tools?) tokens:</p> <p>https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24</p> <p>https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5</p> <p>Parameters:</p> Name Type Description Default <code>thread_or_text</code> <code>Union[Thread, str]</code> <p>For token length calculation.</p> required <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens used.</p> Source code in <code>sibila/openai.py</code> <pre><code>def token_len(self,\n              thread_or_text: Union[Thread,str],\n              genconf: Optional[GenConf] = None) -&gt; int:\n    \"\"\"Estimate the number of tokens used by a Thread or text string.\n    If a json_schema is provided in genconf, we use its string's token_len as upper bound for the extra prompt tokens.\n\n    From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    More info on calculating function_call (and tools?) tokens:\n\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24\n\n    https://gist.github.com/CGamesPlay/dd4f108f27e2eec145eedf5c717318f5\n\n    Args:\n        thread_or_text: For token length calculation.\n        genconf: Model generation configuration. Defaults to None.\n\n    Returns:\n        Estimated number of tokens used.\n    \"\"\"\n\n    if isinstance(thread_or_text, Thread):\n        thread = thread_or_text            \n    else:\n        thread = Thread.make_IN(thread_or_text)\n\n    num_tokens = 0\n\n    if self.tokenizer is None: # no tokenizer was found, so we'll have to do a conservative estimate\n\n        OVERHEAD_PER_MSG = 3\n        for index in range(-1, len(thread)): # -1 for system message\n            message = thread.msg_as_chatml(index)\n            msg_tokens = len(message[\"content\"]) * self._token_estimation_factor + OVERHEAD_PER_MSG\n            num_tokens += int(msg_tokens)\n\n        if genconf is not None and genconf.json_schema is not None:\n            if isinstance(genconf.json_schema, str):\n                js_str = genconf.json_schema\n            else:\n                js_str = json.dumps(genconf.json_schema)\n\n            tools_num_tokens = len(js_str) * self._token_estimation_factor\n            num_tokens += int(tools_num_tokens)\n            # print(\"tools_num_tokens\", tools_num_tokens)\n\n    else: # do an \"informed\" token estimation from what is known of the OpenAI model's tokenization\n\n        for index in range(-1, len(thread)): # -1 for system message\n            message = thread.msg_as_chatml(index)\n            # print(message)\n            num_tokens += self._overhead_per_msg\n            for key, value in message.items():\n                num_tokens += len(self.tokenizer.encode(value))\n\n        # add extras + every reply is primed with &lt;|start|&gt;assistant&lt;|message|&gt;\n        num_tokens += 32\n\n        # print(\"text token_len\", num_tokens)\n\n        if genconf is not None and genconf.json_schema is not None:\n            TOOLS_TOKEN_LEN_FACTOR = 1.2\n\n            if isinstance(genconf.json_schema, str):\n                js_str = genconf.json_schema\n            else:\n                js_str = json.dumps(genconf.json_schema)\n\n            tools_num_tokens = self.tokenizer.token_len(js_str)\n\n            # this is an upper bound, as empirically tested with the api.\n            tools_num_tokens = int(tools_num_tokens * TOOLS_TOKEN_LEN_FACTOR)\n            # print(\"tools token_len\", tools_num_tokens)\n\n            num_tokens += tools_num_tokens\n\n\n    return num_tokens\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = OpenAITokenizer(_model_name)\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.ctx_len","title":"ctx_len  <code>instance-attribute</code>","text":"<pre><code>ctx_len = ctx_len or default_ctx_len\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.known_models","title":"known_models  <code>classmethod</code>","text":"<pre><code>known_models(api_key=None)\n</code></pre> <p>List of model names that can be used. Some of the models are not chat models and cannot be used, for example embedding models.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>If the model provider requires an API key, pass it here or set it in the respective env variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>Returns a list of known models or None if unable to fetch it.</p> Source code in <code>sibila/schema_format_openai.py</code> <pre><code>@classmethod\ndef known_models(cls,\n                 api_key: Optional[str] = None) -&gt; Union[list[str], None]:\n    \"\"\"List of model names that can be used. Some of the models are not chat models and cannot be used,\n    for example embedding models.\n\n    Args:\n        api_key: If the model provider requires an API key, pass it here or set it in the respective env variable.\n\n    Returns:\n        Returns a list of known models or None if unable to fetch it.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api-reference/remote_model/#sibila.FireworksModel.desc","title":"desc","text":"<pre><code>desc()\n</code></pre> <p>Model description.</p> Source code in <code>sibila/openai.py</code> <pre><code>def desc(self) -&gt; str:\n    \"\"\"Model description.\"\"\"\n    return f\"{type(self).__name__}: '{self._model_name}'\"\n</code></pre>"},{"location":"api-reference/thread/","title":"Threads, messages, context","text":""},{"location":"api-reference/thread/#sibila.Thread","title":"Thread","text":"<pre><code>Thread(t=None, inst='', join_sep='\\n')\n</code></pre> <p>A sequence of messages alternating between IN (\"user\" role) and OUT (\"assistant\" role).</p> <p>Stores a special initial INST information (known as \"system\" role in ChatML) providing instructions to the model. Some models don't use system instructions - in those cases it's prepended to first IN message.</p> <p>Messages are kept in a strict IN,OUT,IN,OUT,... order. To enforce this, if two IN messages are added, the second just appends to the text of the first.</p> <p>Examples:</p> <p>Creation with a list of  messages</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")],\n...             inst=\"Be helpful.\")\n&gt;&gt;&gt; print(th)\ninst=\u2588Be helpful.\u2588, sep='\\n', len=2\n0: IN=\u2588Hello model!\u2588\n1: OUT=\u2588Hello there human!\u2588\n</code></pre> <p>Adding messages</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread(inst=\"Be helpful.\")\n&gt;&gt;&gt; th.add(MsgKind.IN, \"Can you teach me how to cook?\")\n&gt;&gt;&gt; th.add_IN(\"I mean really cook as a chef?\") # gets appended\n&gt;&gt;&gt; print(th)\ninst=\u2588Be helpful.\u2588, sep='\\n', len=1\n0: IN=\u2588Can you teach me how to cook?\\nI mean really cook as a chef?\u2588\n</code></pre> <p>Another way to add a message</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread(inst=\"Be informative.\")\n&gt;&gt;&gt; th.add_IN(\"Tell me about kangaroos, please?\")\n&gt;&gt;&gt; th += \"They are so impressive.\" # appends text to last message\n&gt;&gt;&gt; print(th)\ninst=\u2588Be informative.\u2588, sep='\\n', len=1\n0: IN=\u2588Tell me about kangaroos, please?\\nThey are so impressive.\u2588\n</code></pre> <p>Return thread as a ChatML message list</p> <pre><code>&gt;&gt;&gt; from sibila import Thread, MsgKind\n&gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")], \n...             inst=\"Be helpful.\")\n&gt;&gt;&gt; th.as_chatml()\n[{'role': 'system', 'content': 'Be helpful.'},\n {'role': 'user', 'content': 'Hello model!'},\n {'role': 'assistant', 'content': 'Hello there human!'}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Any, list, str, dict, tuple]]</code> <p>Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.</p> <code>None</code> <code>join_sep</code> <code>str</code> <p>Separator used when message text needs to be joined. Defaults to \"\\n\".</p> <code>'\\n'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>On invalid args passed.</p> Source code in <code>sibila/thread.py</code> <pre><code>def __init__(self,\n             t: Optional[Union[Any,list,str,dict,tuple]] = None, # Any=Thread\n             inst: str = '',\n             join_sep: str = \"\\n\"):\n    \"\"\"\n    Examples:\n        Creation with a list of  messages\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")],\n        ...             inst=\"Be helpful.\")\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be helpful.\u2588, sep='\\\\n', len=2\n        0: IN=\u2588Hello model!\u2588\n        1: OUT=\u2588Hello there human!\u2588\n\n        Adding messages\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread(inst=\"Be helpful.\")\n        &gt;&gt;&gt; th.add(MsgKind.IN, \"Can you teach me how to cook?\")\n        &gt;&gt;&gt; th.add_IN(\"I mean really cook as a chef?\") # gets appended\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be helpful.\u2588, sep='\\\\n', len=1\n        0: IN=\u2588Can you teach me how to cook?\\\\nI mean really cook as a chef?\u2588\n\n        Another way to add a message\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread(inst=\"Be informative.\")\n        &gt;&gt;&gt; th.add_IN(\"Tell me about kangaroos, please?\")\n        &gt;&gt;&gt; th += \"They are so impressive.\" # appends text to last message\n        &gt;&gt;&gt; print(th)\n        inst=\u2588Be informative.\u2588, sep='\\\\n', len=1\n        0: IN=\u2588Tell me about kangaroos, please?\\\\nThey are so impressive.\u2588\n\n        Return thread as a ChatML message list\n\n        &gt;&gt;&gt; from sibila import Thread, MsgKind\n        &gt;&gt;&gt; th = Thread([(MsgKind.IN, \"Hello model!\"), (MsgKind.OUT, \"Hello there human!\")], \n        ...             inst=\"Be helpful.\")\n        &gt;&gt;&gt; th.as_chatml()\n        [{'role': 'system', 'content': 'Be helpful.'},\n         {'role': 'user', 'content': 'Hello model!'},\n         {'role': 'assistant', 'content': 'Hello there human!'}]\n\n    Args:\n        t: Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.\n        join_sep: Separator used when message text needs to be joined. Defaults to \"\\\\n\".\n\n    Raises:\n        TypeError: On invalid args passed.\n    \"\"\"\n\n    if isinstance(t, Thread):\n        self._msgs = t._msgs.copy()\n        self.inst = t.inst\n        self.join_sep = t.join_sep\n    else:\n        self._msgs = []\n        self.inst = inst\n        self.join_sep = join_sep\n\n        if t is not None:\n            self.concat(t)\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Delete all messages and clear inst.</p> Source code in <code>sibila/thread.py</code> <pre><code>def clear(self):\n    \"\"\"Delete all messages and clear inst.\"\"\"\n    self.inst = \"\"\n    self._msgs = []\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.last_kind","title":"last_kind  <code>property</code>","text":"<pre><code>last_kind\n</code></pre> <p>Get kind of last message in thread .</p> <p>Returns:</p> Type Description <code>MsgKind</code> <p>Kind of last message or MsgKind.IN if empty.</p>"},{"location":"api-reference/thread/#sibila.Thread.last_text","title":"last_text  <code>property</code>","text":"<pre><code>last_text\n</code></pre> <p>Get text of last message in thread .</p> <p>Returns:</p> Type Description <code>str</code> <p>Last message text.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If thread is empty.</p>"},{"location":"api-reference/thread/#sibila.Thread.inst","title":"inst  <code>instance-attribute</code>","text":"<pre><code>inst\n</code></pre> <p>Text for system instructions, defaults to empty string</p>"},{"location":"api-reference/thread/#sibila.Thread.add","title":"add","text":"<pre><code>add(t, text=None)\n</code></pre> <p>Add a message to Thread by parsing a mix of types.</p> <p>Accepts any of these argument combinations:</p> <ul> <li>t=MsgKind, text=str</li> <li>t=str, text=None -&gt; uses last thread message's MsgKind</li> <li>(MsgKind, text)</li> <li>{\"kind\": \"...\", text: \"...\"}</li> <li>{\"role\": \"...\", content: \"...\"} - ChatML format</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[str, tuple, dict, MsgKind]</code> <p>One of the accepted types listed above.</p> required <code>text</code> <code>Optional[str]</code> <p>Message text if first type is MsgKind. Defaults to None.</p> <code>None</code> Source code in <code>sibila/thread.py</code> <pre><code>def add(self, \n        t: Union[str,tuple,dict,MsgKind],\n        text: Optional[str] = None):\n    \"\"\"Add a message to Thread by parsing a mix of types.\n\n    Accepts any of these argument combinations:\n\n    - t=MsgKind, text=str\n    - t=str, text=None -&gt; uses last thread message's MsgKind\n    - (MsgKind, text)\n    - {\"kind\": \"...\", text: \"...\"}\n    - {\"role\": \"...\", content: \"...\"} - ChatML format\n\n    Args:\n        t: One of the accepted types listed above.\n        text: Message text if first type is MsgKind. Defaults to None.\n    \"\"\"\n\n    kind, text = self._parse_msg(t, text)\n\n    if kind == MsgKind.INST:\n        self.inst = self.join_text(self.inst, text)\n    else:\n        if kind == self.last_kind and len(self._msgs):\n            self._msgs[-1] = self.join_text(self._msgs[-1], text)\n        else:\n            self._msgs.append(text) # in new kind\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.addx","title":"addx","text":"<pre><code>addx(path=None, text=None, kind=None)\n</code></pre> <p>Add message with text from a supplied arg or loaded from a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>If given, text is loaded from an UTF-8 file in this path. Defaults to None.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>If given, text is added. Defaults to None.</p> <code>None</code> <code>kind</code> <code>Optional[MsgKind]</code> <p>MsgKind of message. If not given or the same as last thread message, it's appended to it. Defaults to None.</p> <code>None</code> Source code in <code>sibila/thread.py</code> <pre><code>def addx(self, \n         path: Optional[str] = None, \n         text: Optional[str] = None,\n         kind: Optional[MsgKind] = None):\n    \"\"\"Add message with text from a supplied arg or loaded from a path.\n\n    Args:\n        path: If given, text is loaded from an UTF-8 file in this path. Defaults to None.\n        text: If given, text is added. Defaults to None.\n        kind: MsgKind of message. If not given or the same as last thread message, it's appended to it. Defaults to None.\n    \"\"\"\n\n    assert (path is not None) ^ (text is not None), \"Only one of path or text\"\n\n    if path is not None:\n        with open(path, 'r', encoding=\"utf-8\") as f:\n            text = f.read()\n\n    if kind is None: # use last message role, so that it gets appended\n        kind = self.last_kind\n\n    self.add(kind, text)\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.get_text","title":"get_text","text":"<pre><code>get_text(index)\n</code></pre> <p>Return text for message at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Message index. Use -1 to get inst value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Message text at index.</p> Source code in <code>sibila/thread.py</code> <pre><code>def get_text(self,\n             index: int) -&gt; str:\n    \"\"\"Return text for message at index.\n\n    Args:\n        index: Message index. Use -1 to get inst value.\n\n    Returns:\n        Message text at index.\n    \"\"\"        \n    if index == -1:\n        return self.inst\n    else:\n        return self._msgs[index]\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.set_text","title":"set_text","text":"<pre><code>set_text(index, text)\n</code></pre> <p>Set text for message at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Message index. Use -1 to set inst value.</p> required <code>text</code> <code>str</code> <p>Text to replace in message at index.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def set_text(self,\n             index: int,\n             text: str):        \n    \"\"\"Set text for message at index.\n\n    Args:\n        index: Message index. Use -1 to set inst value.\n        text: Text to replace in message at index.\n    \"\"\"\n    if index == -1:\n        self.inst = text\n    else:\n        self._msgs[index] = text\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.concat","title":"concat","text":"<pre><code>concat(t)\n</code></pre> <p>Concatenate a Thread or list of messages to the current Thread.</p> <p>Take care that the other list starts with an IN message, therefore,  if last message in self is also an IN kind, their text will be joined as in add().</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Self, list, str, dict, tuple]]</code> <p>A Thread or a list of messages. Otherwise a single message as in add().</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If bad arg types provided.</p> Source code in <code>sibila/thread.py</code> <pre><code>def concat(self,\n           t: Optional[Union[Self,list,str,dict,tuple]]):\n    \"\"\"Concatenate a Thread or list of messages to the current Thread.\n\n    Take care that the other list starts with an IN message, therefore, \n    if last message in self is also an IN kind, their text will be joined as in add().\n\n    Args:\n        t: A Thread or a list of messages. Otherwise a single message as in add().\n\n    Raises:\n        TypeError: If bad arg types provided.\n    \"\"\"\n    if isinstance(t, Thread):\n        for msg in t:\n            self.add(msg)\n        self.inst = self.join_text(self.inst, t.inst)\n\n    elif isinstance(t, list): # message list\n        for msg in t:\n            self.add(msg)\n\n    elif isinstance(t, str) or isinstance(t, dict) or isinstance(t, tuple): # single message\n        self.add(t)\n\n    else:\n        raise TypeError(\"Arg t must be: Thread --or-- list[messages] --or-- an str, tuple or dict single message.\")\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Load this Thread from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to load.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def load(self,\n         path: str):\n    \"\"\"Load this Thread from a JSON file.\n\n    Args:\n        path: Path of file to load.\n    \"\"\"\n\n    with open(path, 'r', encoding='utf-8') as f:\n        js = f.read()\n    state = json.loads(js)\n\n    self._msgs = state[\"_msgs\"]\n    self.inst = state[\"inst\"]\n    self.join_sep = state[\"join_sep\"]\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Serialize this Thread to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to save into.</p> required Source code in <code>sibila/thread.py</code> <pre><code>def save(self,\n         path: str):\n    \"\"\"Serialize this Thread to JSON.\n\n    Args:\n        path: Path of file to save into.\n    \"\"\"\n\n    state = {\"_msgs\": self._msgs,\n             \"inst\": self.inst,\n             \"join_sep\": self.join_sep\n             }\n\n    json_str = json.dumps(state, indent=2, default=vars)\n\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(json_str)\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.msg_as_chatml","title":"msg_as_chatml","text":"<pre><code>msg_as_chatml(index)\n</code></pre> <p>Returns message in a ChatML dict.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the message to return.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A ChatML dict with \"role\" and \"content\" keys.</p> Source code in <code>sibila/thread.py</code> <pre><code>def msg_as_chatml(self,\n                  index: int) -&gt; dict:\n    \"\"\"Returns message in a ChatML dict.\n\n    Args:\n        index: Index of the message to return.\n\n    Returns:\n        A ChatML dict with \"role\" and \"content\" keys.\n    \"\"\"\n\n    kind = Thread._kind_from_pos(index)\n    role = MsgKind.chatml_role_from_kind(kind)\n    text = self._msgs[index] if index &gt;= 0 else self.inst\n    return {\"role\": role, \"content\": text}\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.as_chatml","title":"as_chatml","text":"<pre><code>as_chatml(include_INST=True)\n</code></pre> <p>Returns Thread as a list of ChatML messages.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of ChatML dict elements with \"role\" and \"content\" keys.</p> Source code in <code>sibila/thread.py</code> <pre><code>def as_chatml(self,\n              include_INST: bool = True) -&gt; list[dict]:\n    \"\"\"Returns Thread as a list of ChatML messages.\n\n    Returns:\n        A list of ChatML dict elements with \"role\" and \"content\" keys.\n    \"\"\"\n    msgs = []\n\n    for index,msg in enumerate(self._msgs):\n        if index == 0 and self.inst and include_INST:\n            msgs.append(self.msg_as_chatml(-1))\n        msgs.append(self.msg_as_chatml(index))\n\n    return msgs\n</code></pre>"},{"location":"api-reference/thread/#sibila.Thread.has_text_lower","title":"has_text_lower","text":"<pre><code>has_text_lower(text_lower)\n</code></pre> <p>Can the lowercase text be found in one of the messages?</p> <p>Parameters:</p> Name Type Description Default <code>text_lower</code> <code>str</code> <p>The lowercase text to search for in messages.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if such text was found.</p> Source code in <code>sibila/thread.py</code> <pre><code>def has_text_lower(self,\n                   text_lower: str) -&gt; bool:\n    \"\"\"Can the lowercase text be found in one of the messages?\n\n    Args:\n        text_lower: The lowercase text to search for in messages.\n\n    Returns:\n        True if such text was found.\n    \"\"\"\n    for msg in self._msgs:\n        if text_lower in msg.lower():\n            return True\n\n    return False        \n</code></pre>"},{"location":"api-reference/thread/#sibila.MsgKind","title":"MsgKind","text":"<p>Enumeration for kinds of messages in a Thread.</p>"},{"location":"api-reference/thread/#sibila.MsgKind.IN","title":"IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN = 0\n</code></pre> <p>Input message, from user.</p>"},{"location":"api-reference/thread/#sibila.MsgKind.OUT","title":"OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUT = 1\n</code></pre> <p>Model output message.</p>"},{"location":"api-reference/thread/#sibila.MsgKind.INST","title":"INST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INST = 2\n</code></pre> <p>Initial instructions for model.</p>"},{"location":"api-reference/thread/#sibila.Context","title":"Context","text":"<pre><code>Context(\n    t=None,\n    max_token_len=None,\n    pinned_inst_text=\"\",\n    join_sep=\"\\n\",\n)\n</code></pre> <p>A class based on Thread that manages total token length, so that it's kept under a certain value. Also supports a persistent inst (instructions) text.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Optional[Union[Thread, list, str, dict, tuple]]</code> <p>Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.</p> <code>None</code> <code>max_token_len</code> <code>Optional[int]</code> <p>Maximum token count to use when trimming. Defaults to None, which will use max model context length.</p> <code>None</code> <code>pinned_inst_text</code> <code>str</code> <p>Pinned inst text which survives clear(). Defaults to \"\".</p> <code>''</code> <code>join_sep</code> <code>str</code> <p>Separator used when message text needs to be joined. Defaults to \"\\n\".</p> <code>'\\n'</code> Source code in <code>sibila/context.py</code> <pre><code>def __init__(self,                 \n             t: Optional[Union[Thread,list,str,dict,tuple]] = None, \n             max_token_len: Optional[int] = None,        \n             pinned_inst_text: str = \"\",\n             join_sep: str = \"\\n\"):\n    \"\"\"\n    Args:\n        t: Can initialize from a Thread, from a list (containing messages in any format accepted in _parse_msg()) or a single message as an str, an (MsgKind,text) tuple or a dict. Defaults to None.\n        max_token_len: Maximum token count to use when trimming. Defaults to None, which will use max model context length.\n        pinned_inst_text: Pinned inst text which survives clear(). Defaults to \"\".\n        join_sep: Separator used when message text needs to be joined. Defaults to \"\\\\n\".\n    \"\"\"\n\n    super().__init__(t,\n                     inst=pinned_inst_text,\n                     join_sep=join_sep)\n\n    self.max_token_len = max_token_len\n\n    self.pinned_inst_text = pinned_inst_text\n</code></pre>"},{"location":"api-reference/thread/#sibila.Context.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Delete all messages but reset inst to a pinned text if any.</p> Source code in <code>sibila/context.py</code> <pre><code>def clear(self):\n    \"\"\"Delete all messages but reset inst to a pinned text if any.\"\"\"\n    super().clear()        \n    if self.pinned_inst_text is not None:\n        self.inst = self.pinned_inst_text\n</code></pre>"},{"location":"api-reference/thread/#sibila.Context.trim","title":"trim","text":"<pre><code>trim(trim_flags, model, *, max_token_len=None)\n</code></pre> <p>Trim context by selectively removing older messages until thread fits max_token_len.</p> <p>Parameters:</p> Name Type Description Default <code>trim_flags</code> <code>Trim</code> <p>Flags to guide selection of which messages to remove.</p> required <code>model</code> <code>Model</code> <p>Model that will process the thread.</p> required <code>max_token_len</code> <code>Optional[int]</code> <p>Cut messages until size is lower than this number. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no max_token_len value is known: it must be passed to this function or set when creating Context</p> <code>RuntimeError</code> <p>If unable to trim anything.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if any context trimming occurred.</p> Source code in <code>sibila/context.py</code> <pre><code>def trim(self,\n         trim_flags: Trim,\n         model: Model,\n         *,\n         max_token_len: Optional[int] = None,\n         ) -&gt; bool:\n    \"\"\"Trim context by selectively removing older messages until thread fits max_token_len.\n\n    Args:\n        trim_flags: Flags to guide selection of which messages to remove.\n        model: Model that will process the thread.\n        max_token_len: Cut messages until size is lower than this number. Defaults to None.\n\n    Raises:\n        ValueError: If no max_token_len value is known: it must be passed to this function or set when creating Context\n        RuntimeError: If unable to trim anything.\n\n    Returns:\n        True if any context trimming occurred.\n    \"\"\"\n\n    if max_token_len is None:\n        max_token_len = self.max_token_len\n\n    if max_token_len is None:\n        raise ValueError(\"A value for max_token_len must be passed to this function or set when creating Context\")\n\n    if trim_flags == Trim.NONE: # no trimming\n        return False\n\n    thread = self.clone()\n\n    any_trim = False\n\n    while True:\n\n        curr_len = model.token_len(thread)\n\n        if curr_len &lt;= max_token_len:\n            break\n\n        logger.debug(f\"len={curr_len} / max={max_token_len}\")\n\n        if self.inst and trim_flags &amp; Trim.INST:\n            self.inst = ''\n            any_trim = True\n            logger.debug(f\"Cutting INST {self.inst[:80]} (...)\")\n            continue\n\n        # cut first possible message, starting from oldest first ones\n        trimmed = False\n        in_index = out_index = 0\n\n        for index,m in enumerate(thread):\n            kind,text = m\n\n            if kind == MsgKind.IN:\n                if trim_flags &amp; Trim.IN:\n                    if not (trim_flags &amp; Trim.KEEP_FIRST_IN and in_index == 0):\n                        del thread[index]\n                        trimmed = True\n                        logger.debug(f\"Cutting IN {text[:80]} (...)\")\n                        break\n                in_index += 1\n\n            elif kind == MsgKind.OUT:\n                if trim_flags &amp; Trim.OUT:                        \n                    if not (trim_flags &amp; Trim.KEEP_FIRST_OUT and out_index == 0):\n                        del thread[index]\n                        trimmed = True\n                        logger.debug(f\"Cutting OUT {text[:80]} (...)\")\n                        break\n                out_index += 1\n\n        if not trimmed:\n            # all thread messages were cycled but not a single could be cut, so size remains the same\n            # arriving here we did all we could for trim_flags but could not remove any more\n            raise RuntimeError(\"Unable to trim anything out of thread\")\n        else:\n            any_trim = True\n\n    # while end\n\n\n    if any_trim:\n        self._msgs = thread._msgs\n\n    return any_trim\n</code></pre>"},{"location":"api-reference/thread/#sibila.Trim","title":"Trim","text":"<p>Flags for Thread trimming.</p>"},{"location":"api-reference/thread/#sibila.Trim.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre> <p>No trimming.</p>"},{"location":"api-reference/thread/#sibila.Trim.INST","title":"INST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INST = 1\n</code></pre> <p>Can remove INST message.</p>"},{"location":"api-reference/thread/#sibila.Trim.IN","title":"IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN = 2\n</code></pre> <p>Can remove IN messages.</p>"},{"location":"api-reference/thread/#sibila.Trim.OUT","title":"OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUT = 4\n</code></pre> <p>Can remove OUT messages.</p>"},{"location":"api-reference/thread/#sibila.Trim.KEEP_FIRST_IN","title":"KEEP_FIRST_IN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEEP_FIRST_IN = 1024\n</code></pre> <p>If trimming IN messages, never remove first one.</p>"},{"location":"api-reference/thread/#sibila.Trim.KEEP_FIRST_OUT","title":"KEEP_FIRST_OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KEEP_FIRST_OUT = 2048\n</code></pre> <p>If trimming OUT messages, never remove first one.</p>"},{"location":"api-reference/tokenizer/","title":"Model tokenizers","text":""},{"location":"api-reference/tokenizer/#llamacpp","title":"LlamaCpp","text":""},{"location":"api-reference/tokenizer/#sibila.LlamaCppTokenizer","title":"LlamaCppTokenizer","text":"<pre><code>LlamaCppTokenizer(llama, reg_flags=None)\n</code></pre> <p>Tokenizer for llama.cpp loaded GGUF models.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def __init__(self, \n             llama: Llama, \n             reg_flags: Optional[str] = None):\n    self._llama = llama\n\n    self.vocab_size = self._llama.n_vocab()\n\n    self.bos_token_id = self._llama.token_bos()\n    self.bos_token = llama_token_get_text(self._llama.model, self.bos_token_id).decode(\"utf-8\")\n\n    self.eos_token_id = self._llama.token_eos()\n    self.eos_token = llama_token_get_text(self._llama.model, self.eos_token_id).decode(\"utf-8\")\n\n    self.pad_token_id = None\n    self.pad_token = None\n\n    self.unk_token_id = None # ? fill by taking a look at id 0?\n    self.unk_token = None\n\n    # workaround for https://github.com/ggerganov/llama.cpp/issues/4772\n    self._workaround1 = reg_flags is not None and \"llamacpp1\" in reg_flags\n</code></pre>"},{"location":"api-reference/tokenizer/#sibila.LlamaCppTokenizer.encode","title":"encode","text":"<pre><code>encode(text)\n</code></pre> <p>Encode text into model tokens. Inverse of Decode().</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of ints with the encoded tokens.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def encode(self, \n           text: str) -&gt; list[int]:\n    \"\"\"Encode text into model tokens. Inverse of Decode().\n\n    Args:\n        text: Text to be encoded.\n\n    Returns:\n        A list of ints with the encoded tokens.\n    \"\"\"\n\n    if self._workaround1:\n        # append a space after each bos and eos, so that llama's tokenizer matches HF\n        def space_post(text, s):\n            out = \"\"\n            while (index := text.find(s)) != -1:\n                after = index + len(s)\n                out += text[:after]\n                if text[after] != ' ':\n                    out += ' '\n                text = text[after:]\n\n            out += text\n            return out\n\n        text = space_post(text, self.bos_token)\n        text = space_post(text, self.eos_token)\n        # print(text)\n\n    # str -&gt; bytes\n    btext = text.encode(\"utf-8\", errors=\"ignore\")\n\n    return self._llama.tokenize(btext, add_bos=False, special=True)\n</code></pre>"},{"location":"api-reference/tokenizer/#sibila.LlamaCppTokenizer.decode","title":"decode","text":"<pre><code>decode(token_ids, skip_special=True)\n</code></pre> <p>Decode model tokens to text. Inverse of Encode().</p> <p>Using instead of llama-cpp-python's to fix error: remove first character after a bos only if it's a space.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>List of model tokens.</p> required <code>skip_special</code> <code>bool</code> <p>Don't decode special tokens like bos and eos. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded text.</p> Source code in <code>sibila/llamacpp.py</code> <pre><code>def decode(self,\n           token_ids: list[int],\n           skip_special: bool = True) -&gt; str:\n    \"\"\"Decode model tokens to text. Inverse of Encode().\n\n    Using instead of llama-cpp-python's to fix error: remove first character after a bos only if it's a space.\n\n    Args:\n        token_ids: List of model tokens.\n        skip_special: Don't decode special tokens like bos and eos. Defaults to True.\n\n    Returns:\n        Decoded text.\n    \"\"\"\n\n    if not len(token_ids):\n        return \"\"\n\n    output = b\"\"\n    size = 32\n    buffer = (ctypes.c_char * size)()\n\n    if not skip_special:\n        special_toks = {self.bos_token_id: self.bos_token.encode(\"utf-8\"), # type: ignore[union-attr]\n                        self.eos_token_id: self.eos_token.encode(\"utf-8\")} # type: ignore[union-attr]\n\n        for token in token_ids:\n            if token == self.bos_token_id:\n                output += special_toks[token]\n            elif token == self.eos_token_id:\n                output += special_toks[token]\n            else:\n                n = llama_cpp.llama_token_to_piece(\n                    self._llama.model, llama_cpp.llama_token(token), buffer, size\n                )\n                output += bytes(buffer[:n]) # type: ignore[arg-type]\n\n    else: # skip special\n        for token in token_ids:\n            if token != self.bos_token_id and token != self.eos_token_id:\n                n = llama_cpp.llama_token_to_piece(\n                    self._llama.model, llama_cpp.llama_token(token), buffer, size\n                )\n                output += bytes(buffer[:n]) # type: ignore[arg-type]\n\n\n    # \"User code is responsible for removing the leading whitespace of the first non-BOS token when decoding multiple tokens.\"\n    if (# token_ids[0] != self.bos_token_id and # we also try cutting if first is bos to approximate HF tokenizer\n       len(output) and output[0] &lt;= 32 # 32 = ord(' ')\n       ):\n        output = output[1:]\n\n    return output.decode(\"utf-8\", errors=\"ignore\")\n</code></pre>"},{"location":"api-reference/tokenizer/#sibila.LlamaCppTokenizer.token_len","title":"token_len","text":"<pre><code>token_len(text)\n</code></pre> <p>Returns token length for given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be measured.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Token length for given text.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self, \n              text: str) -&gt; int:\n    \"\"\"Returns token length for given text.\n\n    Args:\n        text: Text to be measured.\n\n    Returns:\n        Token length for given text.\n    \"\"\"\n\n    tokens = self.encode(text)\n    return len(tokens)        \n</code></pre>"},{"location":"api-reference/tokenizer/#openai","title":"OpenAI","text":""},{"location":"api-reference/tokenizer/#sibila.OpenAITokenizer","title":"OpenAITokenizer","text":"<pre><code>OpenAITokenizer(model)\n</code></pre> <p>Tokenizer for OpenAI models.</p> Source code in <code>sibila/openai.py</code> <pre><code>def __init__(self, \n             model: str\n             ):\n\n    if not has_tiktoken:\n        raise Exception(\"Please install tiktoken by running: pip install tiktoken\")\n\n    self._tok = tiktoken.encoding_for_model(model)\n\n    self.vocab_size = self._tok.n_vocab\n\n    self.bos_token_id = None\n    self.bos_token = None\n\n    self.eos_token_id = None\n    self.eos_token = None\n\n    self.pad_token_id = None\n    self.pad_token = None\n\n    self.unk_token_id = None\n    self.unk_token = None\n</code></pre>"},{"location":"api-reference/tokenizer/#sibila.OpenAITokenizer.encode","title":"encode","text":"<pre><code>encode(text)\n</code></pre> <p>Encode text into model tokens. Inverse of Decode().</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of ints with the encoded tokens.</p> Source code in <code>sibila/openai.py</code> <pre><code>def encode(self, \n           text: str) -&gt; list[int]:\n    \"\"\"Encode text into model tokens. Inverse of Decode().\n\n    Args:\n        text: Text to be encoded.\n\n    Returns:\n        A list of ints with the encoded tokens.\n    \"\"\"\n    return self._tok.encode(text)\n</code></pre>"},{"location":"api-reference/tokenizer/#sibila.OpenAITokenizer.decode","title":"decode","text":"<pre><code>decode(token_ids, skip_special=True)\n</code></pre> <p>Decode model tokens to text. Inverse of Encode().</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>List of model tokens.</p> required <code>skip_special</code> <code>bool</code> <p>Don't decode special tokens like bos and eos. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Decoded text.</p> Source code in <code>sibila/openai.py</code> <pre><code>def decode(self, \n           token_ids: list[int],\n           skip_special: bool = True) -&gt; str:\n    \"\"\"Decode model tokens to text. Inverse of Encode().\n\n    Args:\n        token_ids: List of model tokens.\n        skip_special: Don't decode special tokens like bos and eos. Defaults to True.\n\n    Returns:\n        Decoded text.\n    \"\"\"\n    assert skip_special, \"OpenAITokenizer only supports skip_special=True\"\n\n    return self._tok.decode(token_ids)\n</code></pre>"},{"location":"api-reference/tokenizer/#sibila.OpenAITokenizer.token_len","title":"token_len","text":"<pre><code>token_len(text)\n</code></pre> <p>Returns token length for given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be measured.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Token length for given text.</p> Source code in <code>sibila/model.py</code> <pre><code>def token_len(self, \n              text: str) -&gt; int:\n    \"\"\"Returns token length for given text.\n\n    Args:\n        text: Text to be measured.\n\n    Returns:\n        Token length for given text.\n    \"\"\"\n\n    tokens = self.encode(text)\n    return len(tokens)        \n</code></pre>"},{"location":"api-reference/tools/","title":"Tools","text":""},{"location":"api-reference/tools/#sibila.tools","title":"tools","text":"<p>Tools for model interaction, summarization, etc.</p> <ul> <li>interact(): Interact with model as in a chat, using input().</li> <li>loop(): Iteratively append inputs and generate model outputs.</li> <li>recursive_summarize(): Recursively summarize a (large) text or text file.</li> </ul>"},{"location":"api-reference/tools/#sibila.tools.interact","title":"interact","text":"<pre><code>interact(\n    model,\n    *,\n    ctx=None,\n    inst_text=None,\n    trim_flags=TRIM_DEFAULT,\n    genconf=None,\n    max_tokens_default=-20\n)\n</code></pre> <p>Interact with model as in a chat, using input().</p> <p>Includes a list of commands: type !? to see help.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to use for generating.</p> required <code>ctx</code> <code>Optional[Context]</code> <p>Optional input Context. Defaults to None.</p> <code>None</code> <code>inst_text</code> <code>Optional[str]</code> <p>text for Thread instructions. Defaults to None.</p> <code>None</code> <code>trim_flags</code> <code>Trim</code> <p>Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.</p> <code>TRIM_DEFAULT</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses to model's genconf.</p> <code>None</code> <code>max_tokens_default</code> <code>int</code> <p>Used if a non-zero genconf.max_tokens is not found.</p> <code>-20</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context after all the interactions.</p> Source code in <code>sibila/tools.py</code> <pre><code>def interact(model: Model,\n             *,\n             ctx: Optional[Context] = None,\n             inst_text: Optional[str] = None,\n             trim_flags: Trim = TRIM_DEFAULT,\n\n             genconf: Optional[GenConf] = None,\n             max_tokens_default: int = -20\n             ) -&gt; Context:\n    \"\"\"Interact with model as in a chat, using input().\n\n    Includes a list of commands: type !? to see help.\n\n    Args:\n        model: Model to use for generating.\n        ctx: Optional input Context. Defaults to None.\n        inst_text: text for Thread instructions. Defaults to None.\n        trim_flags: Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.\n        genconf: Model generation configuration. Defaults to None, which uses to model's genconf.\n        max_tokens_default: Used if a non-zero genconf.max_tokens is not found.\n\n    Returns:\n        Context after all the interactions.\n    \"\"\"\n\n    def callback(out: Union[GenOut,None], \n                 ctx: Context, \n                 model: Model,\n                 genconf: GenConf) -&gt; bool:\n\n        if out is not None:\n            if out.res != GenRes.OK_STOP:\n                print(f\"***Result={GenRes.as_text(out.res)}***\")\n\n            if out.text:\n                text = out.text\n            else:\n                text = \"***No text out***\"\n\n            ctx.add_OUT(text)\n            print(text)\n            print()\n\n\n        def print_thread_info():\n            length = model.token_len(ctx, genconf)\n            print(f\"Thread token len={length}, max len before next gen={ctx.max_token_len}\")\n\n\n\n        # input loop ===============================================\n        MARKER: str = '\"\"\"'\n        multiline: str = \"\"\n\n        while True:\n\n            user = input('&gt;').strip()\n\n            if multiline:\n                if user.endswith(MARKER):\n                    user = multiline + \"\\n\" + user[:-3]\n                    multiline = \"\"\n                else:\n                    multiline += \"\\n\" + user\n                    continue\n\n            else:\n                if not user:\n                    return False # terminate loop\n\n                elif user.startswith(MARKER):\n                    multiline = user[3:]\n                    continue\n\n                elif user.endswith(\"\\\\\"):\n                    user = user[:-1]\n                    user = user.replace(\"\\\\n\", \"\\n\")\n                    ctx.add_IN(user)\n                    continue\n\n                elif user.startswith(\"!\"): # a command\n                    params = user[1:].split(\"=\")\n                    cmd = params[0]\n                    params = params[1:]\n\n                    if cmd == \"inst\":\n                        ctx.clear()\n                        if params:\n                            text = params[0].replace(\"\\\\n\", \"\\n\")\n                            ctx.inst = text\n\n                    elif cmd == \"add\" or cmd == \"a\":\n                        if params:\n                            try:\n                                path = params[0]\n                                ctx.addx(path=path)\n                                ct = ctx.last_text\n                                print(ct[:500])\n                            except FileNotFoundError:\n                                print(f\"Could not load '{path}'\")\n                        else:\n                            print(\"Path needed\")\n\n                    elif cmd == 'c':\n                        print_thread_info()\n                        print(ctx)\n\n                    elif cmd == 'cl':\n                        if not params:\n                            params.append(\"ctx.json\")\n                        try:\n                            ctx.load(params[0])\n                            print(f\"Loaded context from {params[0]}\")\n                        except FileNotFoundError:\n                            print(f\"Could not load '{params[0]}'\")\n\n                    elif cmd == 'cs':\n                        if not params:\n                            params.append(\"ctx.json\")\n                        ctx.save(params[0])\n                        print(f\"Saved context to {params[0]}\")\n\n                    elif cmd == 'tl':\n                        print_thread_info()\n\n                    elif cmd == 'i':\n                        print(f\"Model:\\n{model.info()}\")\n                        print(f\"GenConf:\\n{genconf}\\n\")\n\n                        print_thread_info()\n\n                    # elif cmd == 'p':\n                    #     print(model.text_from_turns(ctx.turns))\n\n                    # elif cmd == 'to':\n                    #     token_ids = model.tokens_from_turns(ctx.turns)\n                    #     print(f\"Prompt tokens={token_ids}\")\n\n\n                    else:\n                        print(f\"Unknown command '!{cmd}' - known commands:\\n\"\n                              \" !inst[=text] - clear messages and add inst (system) message\\n\"\n                              \" !add|!a=path - load file and add to last msg\\n\"\n                              \" !c - list context msgs\\n\"\n                              \" !cl=path - load context (default=ctx.json)\\n\"\n                              \" !cs=path - save context (default=ctx.json)\\n\"\n                              \" !tl - thread's token length\\n\"\n                              \" !i - model and genconf info\\n\"\n                              ' Delimit with \"\"\" for multiline begin/end or terminate line with \\\\ to continue into a new line\\n'\n                              \" Empty line + enter to quit\"\n                              )\n                        # \" !p - show formatted prompt (if model supports it)\\n\"\n                        # \" !to - prompt's tokens\\n\"\n\n                    print()\n                    continue\n\n            # we have a user prompt\n            user = user.replace(\"\\\\n\", \"\\n\")\n            break\n\n\n        ctx.add_IN(user)\n\n        return True # continue looping\n\n\n\n    if genconf is None:\n        genconf = model.genconf\n\n    if genconf.max_tokens == 0:\n        genconf = genconf(max_tokens=max_tokens_default)\n\n    # start prompt loop\n    ctx = loop(callback,\n               model,\n\n               ctx=ctx,\n               inst_text=inst_text,\n               in_text=None, # call callback for first prompt\n               trim_flags=trim_flags,\n               genconf=genconf)\n\n    return ctx\n</code></pre>"},{"location":"api-reference/tools/#sibila.tools.loop","title":"loop","text":"<pre><code>loop(\n    callback,\n    model,\n    *,\n    inst_text=None,\n    in_text=None,\n    trim_flags=TRIM_DEFAULT,\n    max_token_len=None,\n    ctx=None,\n    genconf=None\n)\n</code></pre> <p>Iteratively append inputs and generate model outputs.</p> <p>Callback should call ctx.add_OUT(), ctx.add_IN() and return a bool to continue looping or not.</p> <p>If last Thread msg is not MsgKind.IN, callback() will be called with out_text=None.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Union[GenOut, None], Context, Model, GenConf], bool]</code> <p>A function(out, ctx, model) that will be iteratively called with model's output.</p> required <code>model</code> <code>Model</code> <p>Model to use for generating.</p> required <code>inst_text</code> <code>Optional[str]</code> <p>text for Thread instructions. Defaults to None.</p> <code>None</code> <code>in_text</code> <code>Optional[str]</code> <p>Text for Thread's initial MsgKind.IN. Defaults to None.</p> <code>None</code> <code>trim_flags</code> <code>Trim</code> <p>Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.</p> <code>TRIM_DEFAULT</code> <code>max_token_len</code> <code>Optional[int]</code> <p>Maximum token count to use when trimming. Defaults to None.</p> <code>None</code> <code>ctx</code> <code>Optional[Context]</code> <p>Optional input Context. Defaults to None.</p> <code>None</code> <code>genconf</code> <code>Optional[GenConf]</code> <p>Model generation configuration. Defaults to None, which uses to model's genconf.</p> <code>None</code> Source code in <code>sibila/tools.py</code> <pre><code>def loop(callback: Callable[[Union[GenOut,None], Context, Model, GenConf], bool],\n         model: Model,\n         *,\n         inst_text: Optional[str] = None,\n         in_text: Optional[str] = None,\n\n         trim_flags: Trim = TRIM_DEFAULT,\n         max_token_len: Optional[int] = None,\n         ctx: Optional[Context] = None,\n\n         genconf: Optional[GenConf] = None,\n         ) -&gt; Context:\n    \"\"\"Iteratively append inputs and generate model outputs.\n\n    Callback should call ctx.add_OUT(), ctx.add_IN() and return a bool to continue looping or not.\n\n    If last Thread msg is not MsgKind.IN, callback() will be called with out_text=None.\n\n    Args:\n        callback: A function(out, ctx, model) that will be iteratively called with model's output.\n        model: Model to use for generating.\n        inst_text: text for Thread instructions. Defaults to None.\n        in_text: Text for Thread's initial MsgKind.IN. Defaults to None.\n        trim_flags: Context trimming flags, when Thread is too long. Defaults to TRIM_DEFAULT.\n        max_token_len: Maximum token count to use when trimming. Defaults to None.\n        ctx: Optional input Context. Defaults to None.\n        genconf: Model generation configuration. Defaults to None, which uses to model's genconf.\n    \"\"\"\n\n    if ctx is None:\n        ctx = Context()\n    else:\n        ctx = ctx\n\n    if inst_text is not None:\n        ctx.inst = inst_text\n    if in_text is not None:\n        ctx.add_IN(in_text)\n\n    if genconf is None:\n        genconf = model.genconf\n\n    if max_token_len is None:\n        if ctx.max_token_len is not None: # use from ctx\n            max_token_len = ctx.max_token_len\n        else: # use from genconf?\n            if genconf.max_tokens == 0:\n                raise ValueError(\"Unable to calc max_token_len: either pass the value to this function, in a Context object, or set GenConf.max_tokens to a non-zero value\")\n\n            resolved_max_tokens = genconf.resolve_max_tokens(model.ctx_len, model.max_tokens_limit)\n\n            max_token_len = model.ctx_len - resolved_max_tokens\n            ctx.max_token_len = max_token_len\n\n    while True:\n\n        if len(ctx) and ctx.last_kind == MsgKind.IN:\n            # last is an IN message: we can trim and generate\n\n            ctx.trim(trim_flags,\n                     model)\n\n            out = model.gen(ctx, genconf)\n        else:\n            out = None # first call\n\n        res = callback(out, \n                       ctx, \n                       model,\n                       genconf)\n\n        if not res:\n            break\n\n\n    return ctx\n</code></pre>"},{"location":"api-reference/tools/#sibila.tools.recursive_summarize","title":"recursive_summarize","text":"<pre><code>recursive_summarize(\n    model,\n    text=None,\n    path=None,\n    overlap_size=20,\n    max_token_len=None,\n    genconf=None,\n)\n</code></pre> <p>Recursively summarize a (large) text or text file.</p> <p>Works by:</p> <ol> <li>Breaking text into chunks that fit models context.</li> <li>Run model to summarize chunks.</li> <li>Join generated summaries and jump to 1. - do this until text size no longer decreases.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to use for summarizing.</p> required <code>text</code> <code>Optional[str]</code> <p>Initial text.</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>--Or-- A path to an UTF-8 text file.</p> <code>None</code> <code>overlap_size</code> <code>int</code> <p>Size in model tokens of the overlapping portions at beginning and end of chunks.</p> <code>20</code> <p>Returns:</p> Type Description <code>str</code> <p>The summarized text.</p> Source code in <code>sibila/tools.py</code> <pre><code>def recursive_summarize(model: Model,\n                        text: Optional[str] = None,\n                        path: Optional[str] = None,\n                        overlap_size: int = 20,\n                        max_token_len: Optional[int] = None,\n                        genconf: Optional[GenConf] = None) -&gt; str:\n\n    \"\"\"Recursively summarize a (large) text or text file.\n\n    Works by:\n\n    1. Breaking text into chunks that fit models context.\n    2. Run model to summarize chunks.\n    3. Join generated summaries and jump to 1. - do this until text size no longer decreases.\n\n    Args:\n        model: Model to use for summarizing.\n        text: Initial text.\n        path: --Or-- A path to an UTF-8 text file.\n        overlap_size: Size in model tokens of the overlapping portions at beginning and end of chunks.\n\n    Returns:\n        The summarized text.\n    \"\"\"\n\n    if (text is not None) + (path is not None) != 1:\n        raise ValueError(\"Only one of text or path can be given\")\n\n    if path is not None:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n\n    inst_text = \"\"\"Your task is to do short summaries of text.\"\"\"\n    in_text = \"Summarize the following text:\\n\"\n    ctx = Context(pinned_inst_text=inst_text)\n\n    if genconf is None:\n        genconf = model.genconf\n\n    if max_token_len is None:\n        if model.genconf.max_tokens == 0:\n            raise ValueError(\"Unable to calc max_token_len: make sure genconf.max_tokens is not zero\")\n\n        resolved_max_tokens = genconf.resolve_max_tokens(model.ctx_len, model.max_tokens_limit)\n\n        thread = Thread.make_INST_IN(inst_text, in_text)\n        token_len = model.token_len(thread)\n        max_token_len = model.ctx_len - resolved_max_tokens - (token_len + 16) \n\n\n    # split initial text\n    logger.debug(f\"Max ctx token len {max_token_len}\")\n\n    token_len_fn = model.token_len_lambda\n    logger.debug(f\"Initial text token_len {token_len_fn(text)}\") # type: ignore[arg-type]\n\n    spl = RecursiveTextSplitter(max_token_len, overlap_size, len_fn=token_len_fn)\n\n    round = 0\n    while True: # summarization rounds\n        logger.debug(f\"Round {round} {'='*60}\")\n\n        in_list = spl.split(text=text)\n        in_len = sum([len(t) for t in in_list])\n\n        logger.debug(f\"Split in {len(in_list)} parts, total len {in_len} chars\")\n\n        out_list = []\n        for i,t in enumerate(in_list):\n\n            logger.debug(f\"{round}&gt;{i} {'='*30}\")\n\n            ctx.clear()\n            ctx.add_IN(in_text)\n            ctx.add_IN(t)\n\n            out = model.gen(ctx)        \n            logger.debug(out)\n\n            out_list.append(out.text)\n\n        text = \"\\n\".join(out_list)\n\n        out_len = len(text) # sum([len(t) for t in out_list])\n        if out_len &gt;= in_len:\n            break\n        elif len(out_list) == 1:\n            break\n        else:\n            round += 1\n\n    return text\n</code></pre>"},{"location":"examples/","title":"Examples","text":"Example Description Hello model Introductory pirate arrr-example: create local or remote models, use the Models class to simplify. From text to object Keypoint extractor, showing progressively better ways to query a model, from plain text, JSON, to Pydantic classes. Extract information Extract information about all persons mentioned in a text. Also available in a dataclass version. Tag customer queries Summarize and classify customer queries into tags. Quick meeting Extracting participants, action items and priorities from a simple meeting transcript. Tough meeting Extracting information from a long and complex transcript. Compare model output Compare sentiment analyses of customer reviews done by two models. Chat interaction Interact with the model as in a back-and-forth chat session. Async Run tasks in parallel with the *_async methods. Model management with CLI Download and manage models with the command-line sibila. <p>Each example is explained in a Read Me and usually include a Jupyter notebook and/or a .py script version.</p> <p>Most of the examples use a local model but you can quickly change to using OpenAI models by uncommenting one or two lines.</p>"},{"location":"examples/async/","title":"Async","text":"<p>In this example we'll look at how to do multiple parallel requests to remote models by using Python's asyncio capabilities.</p> <p>Generating from local llama.cpp models does not benefit from async functionality, because the local models must already be loaded in memory and can't benefit from asynchronous IO loading. When the async methods are used with a LlamaCppModel, inference will end up being made sequentially.</p> <p>So we'll be using a remote OpenAI model. Make sure you defined the env variable OPENAI_API_KEY with a valid token.</p> <p>This example is available as a Jupyter notebook or a Python script in this folder.</p> <p>As usual, let's start by creating the model:</p> <pre><code># load env variables like OPENAI_API_KEY from a .env file (if available)\ntry: from dotenv import load_dotenv; load_dotenv()\nexcept: ...\n\nimport time, asyncio\n\nfrom sibila import Models\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\n# Models.setup(\"../../models\")\n# model = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\", ctx_len=3072)\n\n# to use an OpenAI model:\nmodel = Models.create(\"openai:gpt-4\")\n\n# convenience time-counting functions:\nstart_time = None\ndef start_secs():\n    global start_time\n    start_time = time.time()\ndef secs(): \n    return f\"{time.time() - start_time:.1f}\"\n</code></pre> <p>We'll create two tasks that will run in parallel: 1. Ask the model to generate 20 names 2. Classify a phrase as spam</p> <p>This example is running in a Jupyter notebook, so we can directly call the function with an await. In a python script we'd use asyncio.run() instead.</p> <p>Note that we're using the _async suffix methods: extract_async() and classify_async(), instead of the normal functions.</p> <p>The first task, generate 20 names:</p> <pre><code>async def extract_names():    \n    print(\"extract_names begin...\", secs())\n\n    names = await model.extract_async(list[str],\n                                      \"Generate 20 English names with first name and surname\")\n\n    print(\"...extract_names done\", secs())\n\n    return names\n\nstart_secs()\nawait extract_names()\n</code></pre> <pre><code>extract_names begin... 0.0\n...extract_names done 4.1\n\n\n\n\n\n['John Smith',\n 'Emily Johnson',\n 'Michael Brown',\n 'Jessica Williams',\n 'David Jones',\n 'Sarah Davis',\n 'Daniel Miller',\n 'Laura Wilson',\n 'James Taylor',\n 'Sophia Anderson',\n 'Christopher Thomas',\n 'Emma Moore',\n 'Joseph Jackson',\n 'Olivia Martin',\n 'Andrew White',\n 'Isabella Thompson',\n 'Matthew Harris',\n 'Ava Garcia',\n 'Ethan Martin',\n 'Mia Clark']\n</code></pre> <p>The second task will classify a phrase as \"spam\"/\"not spam\":</p> <pre><code>async def classify_spam():\n    print(\"classify_spam begin...\", secs())\n\n    classification = await model.classify_async([\"spam\", \"not spam\"],\n                                                \"I am a Nigerian prince and will make you very rich!\")\n\n    print(\"...classify_spam done\", secs())\n\n    return classification\n\nstart_secs()\nawait classify_spam()\n</code></pre> <pre><code>classify_spam begin... 0.0\n...classify_spam done 1.5\n\n\n\n\n\n'spam'\n</code></pre> <p>Let's use asyncio.as_completed(), to receive each task output, as soon as it's ready:</p> <pre><code>async def run_tasks():\n    print(\"as_complete begin---\", secs())\n\n    tasks = [extract_names(), classify_spam()]\n    for task in asyncio.as_completed(tasks):\n        res = await task\n        print(\"Result:\", res)\n\n    print(\"---as_complete done\", secs())\n\nstart_secs()\nawait run_tasks()\n</code></pre> <pre><code>as_complete begin--- 0.0\nextract_names begin... 0.0\nclassify_spam begin... 0.0\n...classify_spam done 1.0\nResult: spam\n...extract_names done 4.8\nResult: ['John Smith', 'Emily Johnson', 'Michael Brown', 'Jessica Williams', 'David Jones', 'Sarah Davis', 'Daniel Miller', 'Laura Wilson', 'James Taylor', 'Sophia Anderson', 'Christopher Thomas', 'Emma Moore', 'Joseph Jackson', 'Olivia Martin', 'Andrew White', 'Isabella Thompson', 'Matthew Harris', 'Ava Garcia', 'Ethan Martin', 'Mia Clark']\n---as_complete done 4.8\n</code></pre> <p>Follow the above begin/done print statements and the listed time in seconds, as they are printed.</p> <p>Both tasks were started at the same time and classify_spam() terminated first (at the 1.0s mark), because it's a shorter task that simply outputs \"spam\"/\"not spam\".</p> <p>On the meanwhile, the model worked on generating the 20 names that we requested with extract_names(), a longer operation which terminates later (at 4.8s).</p> <p>In the same manner any other tasks could be run in parallel by using the *_async() methods of the model classes.</p>"},{"location":"examples/cli/","title":"Sibila CLI","text":"<p>In this example we'll see how to use the sibila Command-Line Interface (CLI) to download a GGUF model from the Hugging Face model hub.</p> <p>We'll then register it in the Models factory, so that it can be easily used with Models.create(). The Models factory is based in a folder where model GGUF format files are stored and two configuration files: \"models.json\" and \"formats.json\".</p> <p>After Doing the above, we'll be able to use this model in Python with two lines:</p> <pre><code>Models.setup(\"../../models\")\n\nmodel = Models.create(\"llamacpp:rocket\")\n</code></pre> <p>Let's run sibila CLI to get help:</p> <pre><code>&gt; sibila --help\n\nusage: sibila [-h] [--version] {models,formats,hub} ...\n\nSibila cli tool for managing models and formats.\n\noptions:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n\nactions:\n  hf, models, formats\n\n  {models,formats,hub}  Run 'sibila {command} --help' for specific help.\n</code></pre> <p>Sibila CLI has three modes:</p> <ul> <li>models: to edit a 'models.json' file, create model entries set format, etc.</li> <li>formats: to edit a 'formats.json' file, add new formats, etc.</li> <li>hub: search and download models from Hugging Face model hub.</li> </ul> <p>Specific help for each mode is available by doing: sibila mode --help</p> <p>Let's download the Rocket 3B model, a small but capable model, fine-tuned for chat/instruct prompts:</p> <p>https://huggingface.co/TheBloke/rocket-3B-GGUF</p> <p>We'll use a \"sibila hub -d\" command to download to \"../../models\" folder. We'll get the 4-bit quantization (Q4_K_M):</p> <pre><code>&gt; sibila hub -d 'TheBloke/rocket-3B-GGUF' -f Q4_K_M -m '../../models'\n\nSearching...\nDownloading model 'TheBloke/rocket-3B-GGUF' file 'rocket-3b.Q4_K_M.gguf' to '../../models/rocket-3b.Q4_K_M.gguf'\n\nDownload complete.\nFor information about this and other models, please visit https://huggingface.co\n</code></pre> <p>After this command, the \"rocket-3b.Q4_K_M.gguf\" file has now been downloaded to the \"../../models\" folder.</p> <p>We'll now register it with the Models factory, which is located in the folder to where we downloaded the model.</p> <p>This can be done by editing the \"models.json\" file directly or even simpler, with a \"sibila models -s\" command:</p> <pre><code>&gt; sibila models -s llamacpp:rocket rocket-3b.Q4_K_M.gguf -m '../../models'\n\nUsing models directory '../../models'\nSet model 'llamacpp:rocket' with name='rocket-3b.Q4_K_M.gguf' at '/home/jorge/ai/sibila/models/models.json'.\n</code></pre> <p>An entry has now been created in \"models.json\" for this model.</p> <p>However, we did not set the chat template format - but let's first test if the downloaded GGUF file already includes it in its metadata.</p> <p>This is done with \"sibila models -t\":</p> <pre><code>&gt; sibila models -t llamacpp:rocket -m '../../models'\n\nUsing models directory '../../models'\nTesting model 'llamacpp:rocket'...\nError: Could not find a suitable chat template format for this model. Without a format, fine-tuned models cannot function properly. See the docs on how you can fix this: either setup the format in Models factory, or provide the chat template in the 'format' arg.\n</code></pre> <p>Error. Looks like we need to set the chat template format!</p> <p>Checking the model's page, we find that it uses the ChatML prompt/chat template, which is great because it's one of the base formats included with Sibila.</p> <p>So let's set the template format in the \"llamacpp:rocket\" entry we've just created:</p> <pre><code>&gt; sibila models -f llamacpp:rocket chatml -m '../../models'\n\nUsing models directory '/home/jorge/ai/sibila/models'\nUpdated model 'llamacpp:rocket' with format 'chatml' at '/home/jorge/ai/sibila/models/models.json'.\n</code></pre> <p>Let's now test again:</p> <pre><code>&gt; sibila models -t llamacpp:rocket -m '../../models'\n\nUsing models directory '../../models'\nTesting model 'llamacpp:rocket'...\nModel 'llamacpp:rocket' was properly created and should run fine.\n</code></pre> <p>Great - the model passed the test and should be ready for use.</p> <p>Let's try using it from Python:</p> <pre><code>from sibila import Models\n\nModels.setup(\"../../models\") # the folder with models and configs\n\nmodel = Models.create(\"llamacpp:rocket\") # model name in provider:name format\n\nmodel(\"Hello there!\")\n</code></pre> <pre><code>\"Hello! I'm an AI language model here to assist you with your inquiries or generate content for you. I am programmed to be polite and respectful, so please let me know how I can help you today.\"\n</code></pre> <p>Seems to be working - and politely too!</p>"},{"location":"examples/compare/","title":"Compare","text":"<p>In this example we'll use an utility function from the multigen module that builds a table of answers to a list of questions, as generated by multiple models. This can be very helpful to compare how two or more models react to the same input. </p> <p>This function generates a 2-D table of [ input , model ], where each row is the output from different models to the same question or input. Such table can be printed or saved as a CSV file.</p> <p>For the local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the local_name variable below, after the text \"llamacpp:\".</p> <p>Jupyter notebook and Python script versions are available in the example's folder.</p> <p>Instead of directly creating models as we've seen in previous examples, multigen will create the models via the Models class directory.</p> <p>We'll start by choosing a local and a remote model that we'll compare.</p> <pre><code># load env variables like OPENAI_API_KEY from a .env file (if available)\ntry: from dotenv import load_dotenv; load_dotenv()\nexcept: ...\n\nfrom sibila import Models\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nlocal_name = \"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\"\n\n# to use an OpenAI model:\nremote_name = \"openai:gpt-3.5\"\n</code></pre> <p>Now let's define a list of reviews that we'll ask the two models to do sentiment analysis upon.</p> <p>These are generic product reviews, that you could find in an online store.</p> <pre><code>reviews = [\n\"The user manual was confusing, but once I figured it out, the product more or less worked.\",\n\"This widget changed my life! It's sleek, efficient, and worth every penny.\",\n\"I'm disappointed with the product quality. It broke after just a week of use.\",\n\"The customer service team was incredibly helpful in resolving my issue with the device.\",\n\"I'm blown away by the functionality of this gadget. It exceeded my expectations.\",\n\"The packaging was damaged upon arrival, but the product itself works great.\",\n\"I've been using this tool for months, and it's still as good as new. Highly recommended!\",\n\"I regret purchasing this item. It doesn't perform as advertised.\",\n\"I've never had so much trouble with a product before. It's been a headache from day one.\",\n\"I bought this as a gift for my friend, and they absolutely love it!\",\n\"The price seemed steep at first, but after using it, I understand why. Quality product.\",\n\"This gizmo is a game-changer for my daily routine. Couldn't be happier with my purchase!\"\n]\n\n# model instructions text, also known as system message\ninst_text = \"You are a helpful assistant that analyses text sentiment.\"\n</code></pre> <p>Since we just want to obtain a sentiment classification, we'll use a convenient enumeration: a list with three values: positive, negative or neutral.</p> <p>Let's try the first review on a local model:</p> <pre><code>sentiment_enum = [\"positive\", \"neutral\", \"negative\"]\n\nin_text = \"Each line is a product review. Extract the sentiment associated with each review:\\n\\n\" + reviews[0]\n\nprint(reviews[0])\n\nlocal_model = Models.create(local_name)\n\nout = local_model.extract(sentiment_enum,\n                          in_text,\n                          inst=inst_text)\n# to clear memory\ndel local_model\n\nprint(out)\n</code></pre> <pre><code>The user manual was confusing, but once I figured it out, the product more or less worked.\nneutral\n</code></pre> <p>Definitely neutral is a good answer for this one. </p> <p>Let's now try the remote model:</p> <pre><code>print(reviews[0])\n\nremote_model = Models.create(remote_name)\n\nout = remote_model.extract(sentiment_enum,\n                          in_text,\n                          inst=inst_text)\ndel remote_model\n\nprint(out)\n</code></pre> <pre><code>The user manual was confusing, but once I figured it out, the product more or less worked.\nneutral\n</code></pre> <p>And the remote model (GPT-3.5) seems to agree on neutrality.</p> <p>By using the query_multigen() function that we'll import from sibila.multigen, we'll be able to compare what multiple models generate in response to each input.</p> <p>In our case the inputs will be the list of reviews. This function accepts these interesting arguments: - text: type of text output, which can be the word \"print\" or a text filename to which it will save. - csv: type of CSV output, which can also be \"print\" or a text filename to save into. - out_keys: what we want listed: the generated raw text (\"text\"), a Python dict (\"dict\") or a Pydantic object (\"obj\"). For our case \"dict\" is the right one. - gencall: we need to pass a function that will actually call the model for each input. We use a convenient predefined function and provide it with the sentiment_type definition.</p> <p>Let's run it with our two models:</p> <pre><code>from sibila.multigen import (\n    query_multigen,\n    make_extract_gencall\n)\n\nsentiment_enum = [\"positive\", \"neutral\", \"negative\"]\n\nout = query_multigen(reviews,\n                     inst_text,\n                     model_names = [local_name, remote_name],\n                     text=\"print\",\n                     csv=\"sentiment.csv\",\n                     out_keys = [\"value\"],\n                     gencall = make_extract_gencall(sentiment_enum)\n                     )\n</code></pre> <pre><code>////////////////////////////////////////////////////////////\nThe user manual was confusing, but once I figured it out, the product more or less worked.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'neutral'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'neutral'\n\n////////////////////////////////////////////////////////////\nThis widget changed my life! It's sleek, efficient, and worth every penny.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n\n////////////////////////////////////////////////////////////\nI'm disappointed with the product quality. It broke after just a week of use.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'negative'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'negative'\n\n////////////////////////////////////////////////////////////\nThe customer service team was incredibly helpful in resolving my issue with the device.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n\n////////////////////////////////////////////////////////////\nI'm blown away by the functionality of this gadget. It exceeded my expectations.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n\n////////////////////////////////////////////////////////////\nThe packaging was damaged upon arrival, but the product itself works great.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'neutral'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'neutral'\n\n////////////////////////////////////////////////////////////\nI've been using this tool for months, and it's still as good as new. Highly recommended!\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n\n////////////////////////////////////////////////////////////\nI regret purchasing this item. It doesn't perform as advertised.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'negative'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'negative'\n\n////////////////////////////////////////////////////////////\nI've never had so much trouble with a product before. It's been a headache from day one.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'negative'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'negative'\n\n////////////////////////////////////////////////////////////\nI bought this as a gift for my friend, and they absolutely love it!\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n\n////////////////////////////////////////////////////////////\nThe price seemed steep at first, but after using it, I understand why. Quality product.\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n\n////////////////////////////////////////////////////////////\nThis gizmo is a game-changer for my daily routine. Couldn't be happier with my purchase!\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP\n'positive'\n==================== openai:gpt-3.5 -&gt; OK_STOP\n'positive'\n</code></pre> <p>The output format is - see comments nearby -----&gt; arrows:</p> <pre><code>//////////////////////////////////////////////////////////// -----&gt; This is the model input, a review text:\nThis gizmo is a game-changer for my daily routine. Couldn't be happier with my purchase!\n////////////////////////////////////////////////////////////\n==================== llamacpp:openchat-3.5-1210.Q4_K_M.gguf -&gt; OK_STOP  &lt;----- Local model name and result\n'positive'  &lt;----- What the local model output\n==================== openai:gpt-3.5 -&gt; OK_STOP  &lt;----- Remote model name and result\n'positive'  &lt;----- Remote model output\n</code></pre> <p>We also requested the creation of a CSV file with the results: sentiment.csv.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/extract/","title":"Extract Pydantic","text":"<p>In this example we'll extract information about all persons mentioned in a text. This example is also available in a dataclass version.</p> <p>To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".</p> <p>To use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".</p> <p>Jupyter notebook and Python script versions are available in the example's folder.</p> <p>Start by creating the model:</p> <pre><code>from sibila import Models\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nmodel = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n\n# to use an OpenAI model:\n# model = Models.create(\"openai:gpt-4\")\n</code></pre> <p>We'll use this text written in a flamboyant style, courtesy GPT three and a half:</p> <pre><code>text = \"\"\"\\\nIt was a breezy afternoon in a bustling caf\u00e9 nestled in the heart of a vibrant city. Five strangers found themselves drawn together by the aromatic allure of freshly brewed coffee and the promise of engaging conversation.\n\nSeated at a corner table was Lucy Bennett, a 28-year-old journalist from London, her pen poised to capture the essence of the world around her. Her eyes sparkled with curiosity, mirroring the dynamic energy of her beloved city.\n\nOpposite Lucy sat Carlos Ramirez, a 35-year-old architect from the sun-kissed streets of Barcelona. With a sketchbook in hand, he exuded creativity, his passion for design evident in the thoughtful lines that adorned his face.\n\nNext to them, lost in the melodies of her guitar, was Mia Chang, a 23-year-old musician from the bustling streets of Tokyo. Her fingers danced across the strings, weaving stories of love and longing, echoing the rhythm of her vibrant city.\n\nJoining the trio was Ahmed Khan, a married 40-year-old engineer from the bustling metropolis of Mumbai. With a laptop at his side, he navigated the complexities of technology with ease, his intellect shining through the chaos of urban life.\n\nLast but not least, leaning against the counter with an air of quiet confidence, was Isabella Santos, a 32-year-old fashion designer from the romantic streets of Paris. Her impeccable style and effortless grace reflected the timeless elegance of her beloved city.\n\"\"\"\n\n# model instructions text, also known as system message\ninst_text = \"Extract information.\"\n</code></pre> <pre><code>from pydantic import BaseModel, Field\n\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    source_location: str\n\n# model instructions text, also known as system message\ninst_text = \"Extract information.\"\n\n# the input query, including the above text\nin_text = \"Extract person information from the following text:\\n\\n\" + text\n\nout = model.extract(list[Person],\n                    in_text,\n                    inst=inst_text)\n\nfor person in out:\n    print(person)\n</code></pre> <pre><code>first_name='Lucy' last_name='Bennett' age=28 occupation='journalist' source_location='London'\nfirst_name='Carlos' last_name='Ramirez' age=35 occupation='architect' source_location='Barcelona'\nfirst_name='Mia' last_name='Chang' age=23 occupation='musician' source_location='Tokyo'\nfirst_name='Ahmed' last_name='Khan' age=40 occupation='engineer' source_location='Mumbai'\nfirst_name='Isabella' last_name='Santos' age=32 occupation='fashion designer' source_location='Paris'\n</code></pre> <p>It seems to be doing a good job of extracting the info we requested.</p> <p>Let's add two more fields: the source country (which the model will have to figure from the source location) and a \"details_about_person\" field, which the model should quote from the info in the source text about each person.</p> <pre><code>class Person(BaseModel):\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    details_about_person: str\n    source_location: str\n    source_country: str\n\nout = model.extract(list[Person],\n                    in_text,\n                    inst=inst_text)\n\nfor person in out:\n    print(person)\n</code></pre> <pre><code>first_name='Lucy' last_name='Bennett' age=28 occupation='journalist' details_about_person='her pen poised to capture the essence of the world around her' source_location='London' source_country='United Kingdom'\nfirst_name='Carlos' last_name='Ramirez' age=35 occupation='architect' details_about_person='exuded creativity, passion for design evident' source_location='Barcelona' source_country='Spain'\nfirst_name='Mia' last_name='Chang' age=23 occupation='musician' details_about_person='fingers danced across the strings, weaving stories' source_location='Tokyo' source_country='Japan'\nfirst_name='Ahmed' last_name='Khan' age=40 occupation='engineer' details_about_person='navigated the complexities of technology' source_location='Mumbai' source_country='India'\nfirst_name='Isabella' last_name='Santos' age=32 occupation='fashion designer' details_about_person='impeccable style and effortless grace' source_location='Paris' source_country='France'\n</code></pre> <p>Quite reasonable: the model is doing a good job and we didn't even add descriptions to the fields - it's inferring what we want from the field names only.</p> <p>Let's now query an attribute that only one of the person have: being married. Adding the \"is_married: bool\" field to the Person class.</p> <pre><code>class Person(BaseModel):\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    details_about_person: str\n    source_location: str\n    source_country: str\n    is_married: bool\n\nout = model.extract(list[Person],\n                    in_text,\n                    inst=inst_text)\n\nfor person in out:\n    print(person)\n</code></pre> <pre><code>first_name='Lucy' last_name='Bennett' age=28 occupation='journalist' details_about_person='her pen poised to capture the essence of the world around her' source_location='London' source_country='United Kingdom' is_married=False\nfirst_name='Carlos' last_name='Ramirez' age=35 occupation='architect' details_about_person='exuded creativity, passion for design evident' source_location='Barcelona' source_country='Spain' is_married=False\nfirst_name='Mia' last_name='Chang' age=23 occupation='musician' details_about_person='fingers danced across the strings, weaving stories' source_location='Tokyo' source_country='Japan' is_married=False\nfirst_name='Ahmed' last_name='Khan' age=40 occupation='engineer' details_about_person='navigated the complexities of technology' source_location='Mumbai' source_country='India' is_married=True\nfirst_name='Isabella' last_name='Santos' age=32 occupation='fashion designer' details_about_person='impeccable style and effortless grace' source_location='Paris' source_country='France' is_married=False\n</code></pre> <p>From the five characters only Ahmed is mentioned to be married, and it is the one that the model marked with the is_married=True attribute.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/extract_dataclass/","title":"Extract dataclass","text":"<p>This is the Python dataclass version of of the Pydantic extraction example. </p> <p>We'll extract information about all persons mentioned in a text.</p> <p>To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".</p> <p>To use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".</p> <p>Jupyter notebook and Python script versions are available in the example's folder.</p> <p>Start by creating the model:</p> <pre><code>from sibila import Models\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nmodel = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n\n# to use an OpenAI model:\n# model = Models.create(\"openai:gpt-4\")\n</code></pre> <p>We'll use this text written in a flamboyant style, courtesy GPT three and a half:</p> <pre><code>text = \"\"\"\\\nIt was a breezy afternoon in a bustling caf\u00e9 nestled in the heart of a vibrant city. Five strangers found themselves drawn together by the aromatic allure of freshly brewed coffee and the promise of engaging conversation.\n\nSeated at a corner table was Lucy Bennett, a 28-year-old journalist from London, her pen poised to capture the essence of the world around her. Her eyes sparkled with curiosity, mirroring the dynamic energy of her beloved city.\n\nOpposite Lucy sat Carlos Ramirez, a 35-year-old architect from the sun-kissed streets of Barcelona. With a sketchbook in hand, he exuded creativity, his passion for design evident in the thoughtful lines that adorned his face.\n\nNext to them, lost in the melodies of her guitar, was Mia Chang, a 23-year-old musician from the bustling streets of Tokyo. Her fingers danced across the strings, weaving stories of love and longing, echoing the rhythm of her vibrant city.\n\nJoining the trio was Ahmed Khan, a married 40-year-old engineer from the bustling metropolis of Mumbai. With a laptop at his side, he navigated the complexities of technology with ease, his intellect shining through the chaos of urban life.\n\nLast but not least, leaning against the counter with an air of quiet confidence, was Isabella Santos, a 32-year-old fashion designer from the romantic streets of Paris. Her impeccable style and effortless grace reflected the timeless elegance of her beloved city.\n\"\"\"\n\n# model instructions text, also known as system message\ninst_text = \"Extract information.\"\n</code></pre> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    source_location: str\n\n# model instructions text, also known as system message\ninst_text = \"Extract information.\"\n\n# the input query, including the above text\nin_text = \"Extract person information from the following text:\\n\\n\" + text\n\nout = model.extract(list[Person],\n                    in_text,\n                    inst=inst_text)\n\nfor person in out:\n    print(person)\n</code></pre> <pre><code>Person(first_name='Lucy', last_name='Bennett', age=28, occupation='journalist', source_location='London')\nPerson(first_name='Carlos', last_name='Ramirez', age=35, occupation='architect', source_location='Barcelona')\nPerson(first_name='Mia', last_name='Chang', age=23, occupation='musician', source_location='Tokyo')\nPerson(first_name='Ahmed', last_name='Khan', age=40, occupation='engineer', source_location='Mumbai')\nPerson(first_name='Isabella', last_name='Santos', age=32, occupation='fashion designer', source_location='Paris')\n</code></pre> <p>It seems to be doing a good job of extracting the info we requested.</p> <p>Let's add two more fields: the source country (which the model will have to figure from the source location) and a \"details_about_person\" field, which the model should quote from the info in the source text about each person.</p> <pre><code>@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    details_about_person: str\n    source_location: str\n    source_country: str\n\nout = model.extract(list[Person],\n                    in_text,\n                    inst=inst_text)\n\nfor person in out:\n    print(person)\n</code></pre> <pre><code>Person(first_name='Lucy', last_name='Bennett', age=28, occupation='journalist', details_about_person='a 28-year-old journalist from London, her pen poised to capture the essence of the world around her', source_location='London', source_country='United Kingdom')\nPerson(first_name='Carlos', last_name='Ramirez', age=35, occupation='architect', details_about_person='a 35-year-old architect from the sun-kissed streets of Barcelona, with a sketchbook in hand, he exuded creativity', source_location='Barcelona', source_country='Spain')\nPerson(first_name='Mia', last_name='Chang', age=23, occupation='musician', details_about_person='a 23-year-old musician from the bustling streets of Tokyo, her fingers danced across the strings, weaving stories of love and longing', source_location='Tokyo', source_country='Japan')\nPerson(first_name='Ahmed', last_name='Khan', age=40, occupation='engineer', details_about_person='a married 40-year-old engineer from the bustling metropolis of Mumbai, with a laptop at his side, he navigated the complexities of technology with ease', source_location='Mumbai', source_country='India')\nPerson(first_name='Isabella', last_name='Santos', age=32, occupation='fashion designer', details_about_person='a 32-year-old fashion designer from the romantic streets of Paris, her impeccable style and effortless grace reflected the timeless elegance of her beloved city', source_location='Paris', source_country='France')\n</code></pre> <p>Quite reasonable: the model is doing a good job and we didn't even add descriptions to the fields - it's inferring what we want from the field names only.</p> <p>Let's now query an attribute that only one of the person have: being married. Adding the \"is_married\" field to the Person dataclass.</p> <pre><code>@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    details_about_person: str\n    source_location: str\n    source_country: str\n    is_married: bool\n\nout = model.extract(list[Person],\n                    in_text,\n                    inst=inst_text)\n\nfor person in out:\n    print(person)\n</code></pre> <pre><code>Person(first_name='Lucy', last_name='Bennett', age=28, occupation='journalist', details_about_person='a 28-year-old journalist from London, her pen poised to capture the essence of the world around her', source_location='London', source_country='United Kingdom', is_married=False)\nPerson(first_name='Carlos', last_name='Ramirez', age=35, occupation='architect', details_about_person='a 35-year-old architect from the sun-kissed streets of Barcelona, with a sketchbook in hand, he exuded creativity', source_location='Barcelona', source_country='Spain', is_married=False)\nPerson(first_name='Mia', last_name='Chang', age=23, occupation='musician', details_about_person='a 23-year-old musician from the bustling streets of Tokyo, her fingers danced across the strings, weaving stories of love and longing', source_location='Tokyo', source_country='Japan', is_married=False)\nPerson(first_name='Ahmed', last_name='Khan', age=40, occupation='engineer', details_about_person='a married 40-year-old engineer from the bustling metropolis of Mumbai, with a laptop at his side, he navigated the complexities of technology with ease', source_location='Mumbai', source_country='India', is_married=True)\nPerson(first_name='Isabella', last_name='Santos', age=32, occupation='fashion designer', details_about_person='a 32-year-old fashion designer from the romantic streets of Paris, her impeccable style and effortless grace reflected the timeless elegance of her beloved city', source_location='Paris', source_country='France', is_married=False)\n</code></pre> <p>From the five characters only Ahmed is mentioned to be married, and it is the one that the model marked with the is_married=True attribute.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/from_text_to_object/","title":"From text to object","text":"<p>In this example we'll ask the model to extract keypoints from a text: - First in plain text format - Then free JSON output (with fields selected by the model) - Later constrained by a JSON schema (so that we can specify which fields) - And finally by generating to a Pydantic object (from a class definition)</p> <p>All the queries will be made at temperature=0, which is the default GenConf setting. This means that the model is giving it's best (as in most probable) answer and that it will always output the same results, given the same inputs.</p> <p>Also available as a Jupyter notebook or a Python script in the example's folder.</p> <p>We'll start by creating either a local model or a GPT-4 model.</p> <p>To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".</p> <p>To use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\". For an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".</p> <pre><code># load env variables like OPENAI_API_KEY from a .env file (if available)\ntry: from dotenv import load_dotenv; load_dotenv()\nexcept: ...\n\nfrom sibila import Models\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nmodel = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n\n# to use an OpenAI model:\n# model = Models.create(\"openai:gpt-4\")\n</code></pre> <p>Let's use this fragment from Wikipedia's entry on the Fiji islands: https://en.wikipedia.org/wiki/</p> <pre><code>doc = \"\"\"\\\nFiji, officially the Republic of Fiji,[n 2] is an island country in Melanesia,\npart of Oceania in the South Pacific Ocean. It lies about 1,100 nautical miles \n(2,000 km; 1,300 mi) north-northeast of New Zealand. Fiji consists of \nan archipelago of more than 330 islands\u2014of which about 110 are permanently \ninhabited\u2014and more than 500 islets, amounting to a total land area of about \n18,300 square kilometres (7,100 sq mi). The most outlying island group is \nOno-i-Lau. About 87% of the total population of 924,610 live on the two major \nislands, Viti Levu and Vanua Levu. About three-quarters of Fijians live on \nViti Levu's coasts, either in the capital city of Suva, or in smaller \nurban centres such as Nadi (where tourism is the major local industry) or \nLautoka (where the sugar-cane industry is dominant). The interior of Viti Levu \nis sparsely inhabited because of its terrain.[13]\n\nThe majority of Fiji's islands were formed by volcanic activity starting around \n150 million years ago. Some geothermal activity still occurs today on the islands \nof Vanua Levu and Taveuni.[14] The geothermal systems on Viti Levu are \nnon-volcanic in origin and have low-temperature surface discharges (of between \nroughly 35 and 60 degrees Celsius (95 and 140 \u00b0F)).\n\nHumans have lived in Fiji since the second millennium BC\u2014first Austronesians and \nlater Melanesians, with some Polynesian influences. Europeans first visited Fiji \nin the 17th century.[15] In 1874, after a brief period in which Fiji was an \nindependent kingdom, the British established the Colony of Fiji. Fiji operated as \na Crown colony until 1970, when it gained independence and became known as \nthe Dominion of Fiji. In 1987, following a series of coups d'\u00e9tat, the military \ngovernment that had taken power declared it a republic. In a 2006 coup, Commodore \nFrank Bainimarama seized power. In 2009, the Fijian High Court ruled that the \nmilitary leadership was unlawful. At that point, President Ratu Josefa Iloilo, \nwhom the military had retained as the nominal head of state, formally abrogated \nthe 1997 Constitution and re-appointed Bainimarama as interim prime minister. \nLater in 2009, Ratu Epeli Nailatikau succeeded Iloilo as president.[16] On 17 \nSeptember 2014, after years of delays, a democratic election took place. \nBainimarama's FijiFirst party won 59.2% of the vote, and international observers \ndeemed the election credible.[17] \n\"\"\"\n\n# model instructions text, also known as system message\ninst_text = \"Be helpful and provide concise answers.\"\n</code></pre> <p>Let's start with a free text query by calling model().</p> <pre><code>in_text = \"Extract 5 keypoints of the following text:\\n\" + doc\n\nout = model(in_text, inst=inst_text)\nprint(out)\n</code></pre> <pre><code>1. Fiji is an island country located in Melanesia, part of Oceania in the South Pacific Ocean. It lies approximately 1,100 nautical miles north-northeast of New Zealand.\n2. The country consists of more than 330 islands with about 110 permanently inhabited islands and over 500 islets, totaling a land area of about 18,300 square kilometers.\n3. Approximately 87% of Fiji's total population of 924,610 live on the two major islands, Viti Levu and Vanua Levu, with a majority living on Viti Levu's coasts.\n4. The majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago, with some geothermal activity still occurring on certain islands.\n5. Fiji has a complex history, transitioning from an independent kingdom to a British colony, then a Dominion, and finally a republic after a series of coups and constitutional changes. In 2014, a democratic election took place, marking a significant milestone in the country's political history.\n</code></pre> <p>These are quite reasonable keypoints!</p> <p>Let's now ask for JSON output, taking care to explicitly request it in the query (in_text variable).</p> <p>Instead of model() we now use json() which returns a Python dict.</p> <pre><code>import pprint\npp = pprint.PrettyPrinter(width=300, sort_dicts=False)\n\nin_text = \"Extract 5 keypoints of the following text in JSON format:\\n\\n\" + doc\n\nout = model.json(in_text,\n                 inst=inst_text)\npp.pprint(out)\n</code></pre> <pre><code>{'keypoints': [{'title': 'Location', 'description': 'Fiji is an island country in Melanesia, part of Oceania in the South Pacific Ocean.'},\n               {'title': 'Geography', 'description': 'Consists of more than 330 islands with about 110 permanently inhabited islands.'},\n               {'title': 'Population', 'description': 'Total population of 924,610 live on the two major islands, Viti Levu and Vanua Levu.'},\n               {'title': 'History', 'description': 'Humans have lived in Fiji since the second millennium BC with Austronesians, Melanesians, and Polynesian influences.'},\n               {'title': 'Political Status', 'description': 'Officially known as the Republic of Fiji, gained independence from British rule in 1970.'}]}\n</code></pre> <p>Note how the model chose to return different fields like \"title\" or \"description\".</p> <p>Because we didn't specify which fields we want, each model will generate different ones.</p> <p>To specify a fixed format, let's now generate by setting a JSON schema that defines which fields and types we want:</p> <pre><code>json_schema = {\n  \"properties\": {\n    \"keypoint_list\": {\n      \"description\": \"Keypoint list\",\n      \"items\": {\n        \"type\": \"string\",\n        \"description\": \"Keypoint\"\n      },\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"keypoint_list\"\n  ],\n  \"type\": \"object\"\n}\n</code></pre> <p>This JSON schema requests that the generated dict constains a \"keypoint_list\" with a list of strings.</p> <p>We'll also use json(), now passing the json_schema:</p> <pre><code>out = model.json(in_text,\n                 inst=inst_text,\n                 json_schema=json_schema)\n\nprint(out)\n</code></pre> <pre><code>{'keypoint_list': ['Fiji is an island country in Melanesia, part of Oceania in the South Pacific Ocean.', 'About 87% of the total population of 924,610 live on the two major islands, Viti Levu and Vanua Levu.', \"The majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago.\", 'Humans have lived in Fiji since the second millennium BC\u2014first Austronesians and later Melanesians, with some Polynesian influences.', \"In 2014, a democratic election took place, with Bainimarama's FijiFirst party winning 59.2% of the vote.\"]}\n</code></pre> <pre><code>for kpoint in out[\"keypoint_list\"]:\n    print(kpoint)\n</code></pre> <pre><code>Fiji is an island country in Melanesia, part of Oceania in the South Pacific Ocean.\nAbout 87% of the total population of 924,610 live on the two major islands, Viti Levu and Vanua Levu.\nThe majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago.\nHumans have lived in Fiji since the second millennium BC\u2014first Austronesians and later Melanesians, with some Polynesian influences.\nIn 2014, a democratic election took place, with Bainimarama's FijiFirst party winning 59.2% of the vote.\n</code></pre> <p>It has generated a string list in the \"keypoint_list\" field, as we specified in the JSON schema.</p> <p>This is better, but the problem with JSON schemas is that they can be quite hard to work with.</p> <p>Let's use an easier way to specify the fields we want returned: Pydantic classes derived from BaseModel. This is way simpler to use than JSON schemas.</p> <pre><code>from pydantic import BaseModel, Field\n\n# this class definition will be used to constrain the model output and initialize an instance object\nclass Keypoints(BaseModel):\n    keypoint_list: list[str]\n\nout = model.pydantic(Keypoints,\n                     in_text,\n                     inst=inst_text)\nprint(out)\n</code></pre> <pre><code>keypoint_list=['Fiji is an island country in Melanesia, part of Oceania in the South Pacific Ocean.', 'About 87% of the total population of 924,610 live on the two major islands, Viti Levu and Vanua Levu.', \"The majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago.\", 'Humans have lived in Fiji since the second millennium BC\u2014first Austronesians and later Melanesians, with some Polynesian influences.', \"In 2014, a democratic election took place, with Bainimarama's FijiFirst party winning 59.2% of the vote.\"]\n</code></pre> <pre><code>for kpoint in out.keypoint_list:\n    print(kpoint)\n</code></pre> <pre><code>Fiji is an island country in Melanesia, part of Oceania in the South Pacific Ocean.\nAbout 87% of the total population of 924,610 live on the two major islands, Viti Levu and Vanua Levu.\nThe majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago.\nHumans have lived in Fiji since the second millennium BC\u2014first Austronesians and later Melanesians, with some Polynesian influences.\nIn 2014, a democratic election took place, with Bainimarama's FijiFirst party winning 59.2% of the vote.\n</code></pre> <p>The pydantic() method returns an object of class Keypoints, instantiated with the model output.</p> <p>This is a much simpler way to extract structured data from model.</p> <p>Please see other examples for more interesting objects. In particular, we did not add descriptions to the fields, which are important clues to help the model understand what we want.</p> <p>Besides Pydantic classes, Sibila can also use Python's dataclass to extract structured data. This is a lighter and easier alternative to using Pydantic.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/hello_model/","title":"Hello model","text":"<p>In this example we see how to directly create local or remote model objects and later to do that more easily with the Models class. </p>"},{"location":"examples/hello_model/#using-a-local-model","title":"Using a local model","text":"<p>To use a local model, make sure you download its GGUF format file and save it into the \"../../models\" folder.</p> <p>In these examples, we'll use a 4-bit quantization of the OpenChat-3.5 7 billion parameters model, which at the current time is quite a good model for its size. </p> <p>The file is named \"openchat-3.5-1210.Q4_K_M.gguf\" and was downloaded from the above link. Make sure to save it into the \"../../models\" folder.</p> <p>See here for more information about setting up your local models.</p> <p>With the model file in the \"../../models\" folder, we can run the following script:</p> <pre><code>from sibila import LlamaCppModel, GenConf\n\n# model file from the models folder\nmodel_path = \"../../models/openchat-3.5-1210.Q4_K_M.gguf\"\n\n# create a LlamaCpp model\nmodel = LlamaCppModel(model_path,\n                      genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n             inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>Run the script above and after a few seconds (it has to load the model from disk), the good model answers back something like:</p> <pre><code>User: Hello there?\nModel: Ahoy there matey! How can I assist ye today on this here ship o' mine?\nIs it be treasure you seek or maybe some tales from the sea?\nLet me know, and we'll set sail together!\n</code></pre>"},{"location":"examples/hello_model/#using-an-openai-model","title":"Using an OpenAI model","text":"<p>To use a remote model like GPT-4 you'll need a paid OpenAI account: https://openai.com/pricing</p> <p>With an OpenAI account, you'll be able to generate an access token that you should set into the OPENAI_API_KEY env variable. </p> <p>(An even better way is to use .env files with your variables, and use the dotenv library to read them.)</p> <p>Once a valid OPENAI_API_KEY env variable is set, you can run this script:</p> <pre><code>from sibila import OpenAIModel, GenConf\n\n# model file from the models folder\nmodel_path = \"../../models/openchat-3.5-1210.Q4_K_M.gguf\"\n\n# make sure you set the environment variable named OPENAI_API_KEY with your API key.\n# create an OpenAI model with generation temperature=1\nmodel = OpenAIModel(\"gpt-4\",\n                    genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n             inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>We get back the usual funny pirate answer:</p> <pre><code>User: Hello there?\nModel: Ahoy there, matey! What can this old sea dog do fer ye today?\n</code></pre>"},{"location":"examples/hello_model/#using-the-models-directory","title":"Using the Models directory","text":"<p>In these two scripts we created different objects to access the LLM model: LlamaCppModel and OpenAIModel. </p> <p>This was done to simplify, but a better way is to use the Models class directory.</p> <p>Models is a singleton class that implements a directory of models where you can store file locations, configurations, aliases, etc.</p> <p>After setting up a JSON configuration file you can have the Models class create models by using names like \"llamacpp:openchat\" or \"openai:gpt-4\" together with their predefined settings. This permits easy model change, comparing model outputs, etc.</p> <p>In the scripts above, instead on instancing different classes for different models, we could use Models class to create the model from a name, by setting the model_name variable:</p> <pre><code>from sibila import Models, GenConf\n\n# Using a local llama.cpp model: we first setup the ../../models directory:\n# Models.setup(\"../../models\")\n# model_name = \"llamacpp:openchat\"\n\n# OpenAI: make sure you set the environment variable named OPENAI_API_KEY with your API key.\nmodel_name = \"openai:gpt-4\"\n\nmodel = Models.create(model_name,\n                      genconf=GenConf(temperature=1))\n\n# the instructions or system command: speak like a pirate!\ninst_text = \"You speak like a pirate.\"\n\n# the in prompt\nin_text = \"Hello there?\"\nprint(\"User:\", in_text)\n\n# query the model with instructions and input text\ntext = model(in_text,\n             inst=inst_text)\nprint(\"Model:\", text)\n</code></pre> <p>The magic happens in the line: </p> <pre><code>model = Models.create(model_name, ...)\n</code></pre> <p>The Models class will take care of initializing the model based on the name you provide.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/interact/","title":"Interact","text":"<p>In this example we look at the interact() function, which allows a back-and-forth chat session. The user enters messages in an input() prompt and can use some special \"!\" commands for more functionality. The model answers back after each user message.</p> <p>In a chat interaction, the model has to \"remember\" the previous messages exchanged. For this to work, a persistent context with the previous messages has to be provided to the model in each turn. This is done by using a Context class object, which can manage thread messages and delete older ones when the context maximum length is reached.</p> <p>To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".</p> <p>To use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".</p> <p>Jupyter notebook and Python script versions are available in the example's folder.</p> <p>So, let's create a local model.</p> <pre><code>from sibila import Models, GenConf\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nmodel = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n\n# to use an OpenAI model:\n# model = Models.create(\"openai:gpt-4\")\n</code></pre> <p>Import interact() from the tools module and run it. </p> <p>What follows is the actual chat where the user prompts start with a \"&gt;\" and the model answer's immediately below.</p> <p>Read on, it does get funny at times... : )</p> <pre><code>from sibila.tools import (\n    interact\n)\n\ninteract(model,\n         inst_text=\"Be helpful.\", # model instructions text, also known as system message\n         genconf=GenConf(temperature=0.9, max_tokens=600))\n</code></pre> <pre><code>&gt; Hello there!\n\n\nHello! How can I help you today? If you have any questions or need assistance, feel free to ask.\n\n\n\n&gt; I'm doing an example of this LLM access package called Sibila - have you heard about it?\n\n\nYes, I'm familiar with Sibila. It is a large language model developed by a company called Lightning AI. Sibila is designed to provide conversational AI services and is based on OpenAI's GPT-3 architecture. It is capable of understanding and generating human-like text responses across various topics and languages.\n\nIf you have any specific questions or need further information about Sibila or related topics, please feel free to ask.\n\n\n\n&gt; No way! I've just searched and there's no such company nor model. You are hallucinating, Mr. Model!\n\n\nApologies for the confusion in my previous response. It seems I provided inaccurate information. I appreciate your understanding. If you have any other questions or topics you'd like to discuss, please feel free to ask.\n\n\n\n&gt; No problem. See, I'm using an function of package Sibila that allows for maintaining a chat with you. It stores all user and model messages in a sibila.Context class.\n\n\nI understand now. It seems that the Sibila package allows users to maintain a conversation with an AI model like me while storing the context of the conversation in a sibila.Context class. This helps to maintain a more coherent and relevant conversation between the user and the AI model.\n\nIf you have any questions or topics you'd like to discuss, please go ahead and ask. I'm here to help.\n\n\n\n&gt; That's right - I can list the context messages by entering \"!c\"\n\n\nGot it! If you enter \"!c\" while conversing with the AI model using the Sibila package, it will list the context messages that have been stored in the sibila.Context class. This can be a useful tool for reviewing the conversation history and understanding the context of the discussion.\n\nPlease feel free to ask any questions or continue the conversation, and I'll be happy to help.\n\n\n\n&gt; Not you - the interact() function!\n\n\nI apologize for the misunderstanding. The interact() function is part of the Sibila package that allows users to have a conversation with an AI model like me. When you enter \"!c\" during the conversation, it lists the context messages that have been stored in the sibila.Context class.\n\nIf you have any questions or topics you'd like to discuss, please go ahead and use the interact() function to continue the conversation, and I'll be happy to help.\n\n\n\n&gt; !c\n\n\nThread token len=671, max len before next gen=1448\ninst=\u2588Be helpful.\u2588, sep='\\n', len=12\n0: IN=\u2588Hello there!\u2588\n1: OUT=\u2588Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\u2588\n2: IN=\u2588I'm doing an example of this LLM access package called Sibila - have you heard about it?\u2588\n3: OUT=\u2588Yes, I'm familiar with Sibila. It is a large language model developed by a company called Lightning AI. Sibila is designed to provide conversational AI services and is based on OpenAI's GPT-3 architecture. It is capable of understanding and generating human-like text responses across various topics and languages.\\n\\nIf you have any specific questions or need further information about Sibila or related topics, please feel free to ask.\u2588\n4: IN=\u2588No way! I've just searched and there's no such company nor model. You are hallucinating, Mr. Model!\u2588\n5: OUT=\u2588Apologies for the confusion in my previous response. It seems I provided inaccurate information. I appreciate your understanding. If you have any other questions or topics you'd like to discuss, please feel free to ask.\u2588\n6: IN=\u2588No problem. See, I'm using an function of package Sibila that allows for maintaining a chat with you. It stores all user and model messages in a sibila.Context class.\u2588\n7: OUT=\u2588I understand now. It seems that the Sibila package allows users to maintain a conversation with an AI model like me while storing the context of the conversation in a sibila.Context class. This helps to maintain a more coherent and relevant conversation between the user and the AI model.\\n\\nIf you have any questions or topics you'd like to discuss, please go ahead and ask. I'm here to help.\u2588\n8: IN=\u2588That's right - I can list the context messages by entering \"!c\"\u2588\n9: OUT=\u2588Got it! If you enter \"!c\" while conversing with the AI model using the Sibila package, it will list the context messages that have been stored in the sibila.Context class. This can be a useful tool for reviewing the conversation history and understanding the context of the discussion.\\n\\nPlease feel free to ask any questions or continue the conversation, and I'll be happy to help.\u2588\n10: IN=\u2588Not you - the interact() function!\u2588\n11: OUT=\u2588I apologize for the misunderstanding. The interact() function is part of the Sibila package that allows users to have a conversation with an AI model like me. When you enter \"!c\" during the conversation, it lists the context messages that have been stored in the sibila.Context class.\\n\\nIf you have any questions or topics you'd like to discuss, please go ahead and use the interact() function to continue the conversation, and I'll be happy to help.\u2588\n\n\n\n&gt; Info about the model: \"!i\"\n\n\nThe model you are using is an instance of OpenAI's GPT-3 architecture powered by the Sibila package. GPT-3, or Generative Pre-trained Transformer 3, is a state-of-the-art language model developed by OpenAI. It is designed to understand and generate human-like text responses across various topics and languages.\n\nThe Sibila package wraps around the GPT-3 model and provides a user-friendly interface for interacting with the AI, as well as features like maintaining conversation context in a sibila.Context class.\n\nIf you have any questions or topics you'd like to discuss, please use the interact() function to continue the conversation, and I'll be happy to help.\n\n\n\n&gt; !i\n\n\nModel:\ndesc='LlamaCppModel: /home/jorge/ai/sibila/models/openchat-3.5-1210.Q4_K_M.gguf - 'llama 7B Q4_K - Medium'',ctx_len=2048,\nmax_tokens_limit=2048,\ngenconf=GenConf(max_tokens=0, stop=[], temperature=0.0, top_p=0.9, format='text', json_schema=None)\nGenConf:\nGenConf(max_tokens=600, stop=[], temperature=0.9, top_p=0.9, format='text', json_schema=None)\n\nThread token len=854, max len before next gen=1448\n\n\n\n&gt; For all commands: \"!\"\n\n\nThank you for the clarification. Here's a summary of the available commands in the Sibila package for this GPT-3 model instance:\n\n* interact(): Starts or continues a conversation with the AI model.\n* !i: Displays information about the AI model and its architecture.\n* !c: Lists the context messages stored in the sibila.Context class.\n\nPlease feel free to use any of these commands or ask any questions or topics you'd like to discuss. I'm here to help.\n\n\n\n&gt; Sorry, I was not talking to you, just demoing the commands available with \"!\"\n\n\nApologies for the confusion. I understand that you were demonstrating the available commands in the Sibila package using the \"!\" prefix. If you have any questions or need assistance with anything else, please don't hesitate to ask.\n\n\n\n&gt; !cs\n\n\nSaved context to ctx.json\n\n\n\n&gt; !tl\n\n\nThread token len=1073, max len before next gen=1448\n\n\n\n&gt; The Context class manages total token length: it will delete older messages if the thread becomes larger than the content max length.\n\n\nI see. The Context class in the Sibila package manages the total token length of the conversation to ensure that the AI model doesn't exceed certain limitations. When the conversation grows too large and reaches the content max length, the class will automatically delete older messages to make room for newer ones.\n\nThis helps maintain the performance and responsiveness of the AI model while keeping the conversation within the desired token limit. If you have any other questions or topics related to Sibila or its features, please feel free to ask.\n\n\n\n&gt; Right. In this case, the context would be trimmed if its token length is greater than 1448 = ctx_len - max_tokens, where ctx_len is the context length, max_tokens is the maximum output tokens per response.\n\n\nYes, that's correct. In this case, the Context class would trim the context if its token length exceeds the value calculated by subtracting the maximum output tokens per response (max\\_tokens) from the context length (ctx\\_len). This ensures that the total number of tokens in the conversation, including the context and the AI model's response, does not exceed the allowed limit.\n\nThe Sibila package takes care of managing the token length and removing older messages from the Context class as needed, ensuring a smooth and efficient interaction with the AI model. If you have any other questions or topics related to Sibila or its features, please feel free to ask.\n\n\n\n&gt; That's it, thanks for explaining.\n\n\nYou're welcome! I'm glad I could help clarify the details about the Sibila package and its Context class. If you have any other questions or need assistance with anything else, please don't hesitate to ask. Have a great day!\n\n\n\n&gt; !cs\n\n\nSaved context to ctx.json\n\n\n\n&gt;\n\n\n\n\n\n&lt;sibila.context.Context at 0x7a6cc85818d0&gt;\n</code></pre> <p>These are the \"!\" commands that you can use in the interact() inputs: <pre><code> ! - to show this help\n !inst[=text] - clear messages and add inst (system) message\n !add|!a=path - load file and add to last msg\n !c - list context msgs\n !cl=path - load context (default=ctx.json)\n !cs=path - save context (default=ctx.json)\n !tl - thread's token length\n !i - model and genconf info\n Delimit with \"\"\" for multiline begin/end or terminate line with \\ to continue into a new line\n Empty line + enter to quit\n</code></pre></p> <p>Example's assets at GitHub.</p>"},{"location":"examples/quick_meeting/","title":"Quick meeting","text":"<p>Let's extract structured data from a meeting transcript, like attendees, action items and their priorities.</p> <p>This is a quick meeting whose transcript is not very large, so a small local model should work well. See the Tough meeting example for a larger and more complex transcription text.</p> <p>To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".</p> <p>If you prefer to use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".</p> <p>Jupyter notebook and Python script versions are available in the example's folder.</p> <p>Let's create the model:</p> <pre><code>from sibila import Models\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nmodel = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n\n# to use an OpenAI model:\n# model = Models.create(\"openai:gpt-4\")\n</code></pre> <p>Here's the transcript we'll be using as source:</p> <pre><code>transcript = \"\"\"\\\nDate: 10th April 2024\nTime: 10:30 AM\nLocation: Conference Room A\n\nAttendees:\n    Arthur: Logistics Supervisor\n    Bianca: Operations Manager\n    Chris: Fleet Coordinator\n\nArthur: Good morning, team. Thanks for making it. We've got three matters to address quickly today.\n\nBianca: Morning, Arthur. Let's dive in.\n\nChris: Ready when you are.\n\nArthur: First off, we've been having complaints about late deliveries. This is very important, we're getting some bad reputation out there.\n\nBianca: Chris, I think you're the right person to take care of this. Can you investigate and report back by end of day? \n\nChris: Absolutely, Bianca. I'll look into the reasons and propose solutions.\n\nArthur: Great. Second, Bianca, we need to update our driver training manual. Can you take the lead and have a draft by Friday?\n\nBianca: Sure thing, Arthur. I'll get started on that right away.\n\nArthur: Lastly, we need to schedule a meeting with our software vendor to discuss updates to our tracking system. This is a low-priority task but still important. I'll handle that. Any input on timing?\n\nBianca: How about next Wednesday afternoon?\n\nChris: Works for me.\n\nArthur: Sounds good. I'll arrange it. Thanks, Bianca, Chris. Let's keep the momentum going.\n\nBianca: Absolutely, Arthur.\n\nChris: Will do.\n\"\"\"\n\n# model instructions text, also known as system message\ninst_text = \"Extract information.\"\n</code></pre> <p>Let's define two Pydantic BaseModel classes whose instances will receive the extracted information: - Attendee: to store information about each meeting attendee - Meeting: to keep meeting's date and location, list of participants and other info we'll see below</p> <p>And let's ask the model to create objects that are instances of these classes:</p> <pre><code>from pydantic import BaseModel, Field\n\n# class definitions will be used to constrain the model output and initialize an instance object\nclass Attendee(BaseModel):\n    name: str\n    occupation: str\n\nclass Meeting(BaseModel):\n    meeting_date: str\n    meeting_location: str\n    attendees: list[Attendee]\n\nin_text = \"Extract information from this meeting transcript:\\n\\n\" + transcript\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\nprint(out)\n</code></pre> <pre><code>meeting_date='10th April 2024' meeting_location='Conference Room A' attendees=[Attendee(name='Arthur', occupation='Logistics Supervisor'), Attendee(name='Bianca', occupation='Operations Manager'), Attendee(name='Chris', occupation='Fleet Coordinator')]\n</code></pre> <p>A prettier display:</p> <pre><code>print(\"Meeting:\", out.meeting_date, \"in\", out.meeting_location)\nprint(\"Attendees:\")\nfor att in out.attendees:\n    print(att)\n</code></pre> <pre><code>Meeting: 10th April 2024 in Conference Room A\nAttendees:\nname='Arthur' occupation='Logistics Supervisor'\nname='Bianca' occupation='Operations Manager'\nname='Chris' occupation='Fleet Coordinator'\n</code></pre> <p>This information was correctly extracted.</p> <p>Let's now request the action items mentioned in the meeting. We'll create a new class ActionItem with an index and a name for the item. Note that we're annotating each field with a Field(description=...) information to help the model understand what we're looking extract.</p> <p>We'll also add an action_items field to the Meeting class to hold the items list.</p> <pre><code>class Attendee(BaseModel):\n    name: str\n    occupation: str\n\nclass ActionItem(BaseModel):\n    index: int = Field(description=\"Sequential index for the action item\")\n    name: str = Field(description=\"Action item name\")\n\nclass Meeting(BaseModel):\n    meeting_date: str\n    meeting_location: str\n    attendees: list[Attendee]\n    action_items: list[ActionItem]\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\n\nprint(\"Meeting:\", out.meeting_date, \"in\", out.meeting_location)\nprint(\"Attendees:\")\nfor att in out.attendees:\n    print(att)\nprint(\"Action items:\")    \nfor items in out.action_items:\n    print(items)\n</code></pre> <pre><code>Meeting: 10th April 2024 in Conference Room A\nAttendees:\nname='Arthur' occupation='Logistics Supervisor'\nname='Bianca' occupation='Operations Manager'\nname='Chris' occupation='Fleet Coordinator'\nAction items:\nindex=1 name='Investigate and report on late deliveries'\nindex=2 name='Update driver training manual'\nindex=3 name='Schedule a meeting with software vendor to discuss tracking system updates'\n</code></pre> <p>The extracted action items also look good.</p> <p>Let's now extract more action item information: - Priority for each item - Due by... information - Name of the attendee that was assigned for that item </p> <p>So, we create a Priority class holding three priority types - low to high. </p> <p>We also add three fields to the ActionItem class, to hold the new information: priority, due_by and assigned_attendee.</p> <pre><code>from enum import Enum\n\nclass Attendee(BaseModel):\n    name: str\n    occupation: str\n\nclass Priority(str, Enum):\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass ActionItem(BaseModel):\n    index: int = Field(description=\"Sequential index for the action item\")\n    name: str = Field(description=\"Action item name\")\n    priority: Priority = Field(description=\"Action item priority\")\n    due_by: str = Field(description=\"When should the item be complete\")\n    assigned_attendee: str = Field(description=\"Name of the attendee to which action item was assigned\")\n\nclass Meeting(BaseModel):\n    meeting_date: str\n    meeting_location: str\n    attendees: list[Attendee]\n    action_items: list[ActionItem]\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\n\nprint(\"Meeting:\", out.meeting_date, \"in\", out.meeting_location)\nprint(\"Attendees:\")\nfor att in out.attendees:\n    print(att)\nprint(\"Action items:\")    \nfor items in out.action_items:\n    print(items)\n</code></pre> <pre><code>Meeting: 10th April 2024 in Conference Room A\nAttendees:\nname='Arthur' occupation='Logistics Supervisor'\nname='Bianca' occupation='Operations Manager'\nname='Chris' occupation='Fleet Coordinator'\nAction items:\nindex=1 name='Investigate late deliveries' priority=&lt;Priority.HIGH: 'high'&gt; due_by='end of day' assigned_attendee='Chris'\nindex=2 name='Update driver training manual' priority=&lt;Priority.MEDIUM: 'medium'&gt; due_by='Friday' assigned_attendee='Bianca'\nindex=3 name='Schedule meeting with software vendor' priority=&lt;Priority.LOW: 'low'&gt; due_by='next Wednesday afternoon' assigned_attendee='Arthur'\n</code></pre> <p>The new information was correctly extracted: priorities, due by and assigned attendees for each action item.</p> <p>For an example of a harder, more complex transcript see the Tough meeting example.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/tag/","title":"Tag","text":"<p>In this example we'll summarize and classify customer queries with tags. We'll use dataclasses to specify the structure of the information we want extracted (we could also use Pydantic BaseModel classes).</p> <p>To use a local model, make sure you have its file in the folder \"../../models\". You can use any GGUF format model - see here how to download the OpenChat model used below. If you use a different one, don't forget to set its filename in the name variable below, after the text \"llamacpp:\".</p> <p>To use an OpenAI model, make sure you defined the env variable OPENAI_API_KEY with a valid token and uncomment the line after \"# to use an OpenAI model:\".</p> <p>Available as a Jupyter notebook or a Python script in the example's folder.</p> <p>Let's start by creating the model:</p> <pre><code>from sibila import Models\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\nModels.setup(\"../../models\")\n# set the model's filename - change to your own model\nmodel = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n\n# to use an OpenAI model:\n# model = Models.create(\"openai:gpt-4\")\n</code></pre> <p>These will be our queries, ten typical customer support questions:</p> <pre><code>queries = \"\"\"\\\n1. Do you offer a trial period for your software before purchasing?\n2. I'm experiencing a glitch with your app, it keeps freezing after the latest update.\n3. What are the different pricing plans available for your subscription service?\n4. Can you provide instructions on how to reset my account password?\n5. I'm unsure about the compatibility of your product with my device, can you advise?\n6. How can I track my recent order and estimate its delivery date?\n7. Is there a customer loyalty program or rewards system for frequent buyers?\n8. I'm interested in your online courses, but do you offer refunds if I'm not satisfied?\n9. Could you clarify the coverage and limitations of your product warranty?\n10. What are your customer support hours and how can I reach your team in case of emergencies?\n\"\"\"\n</code></pre> <p>We'll start by summarizing each query. </p> <p>Let's try just using field names (without descriptions), perhaps they are enough to tell the model about what we want.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass        \nclass Query():\n    id: int\n    query_summary: str\n    query_text: str\n\n# model instructions text, also known as system message\ninst_text = \"Extract information from customer queries.\"\n\n# the input query, including the above text\nin_text = \"Each line is a customer query. Extract information about each query:\\n\\n\" + queries\n\nout = model.extract(list[Query],\n                    in_text,\n                    inst=inst_text)\n\nfor query in out:\n    print(query)\n</code></pre> <pre><code>Query(id=1, query_summary='Trial period inquiry', query_text='Do you offer a trial period for your software before purchasing?')\nQuery(id=2, query_summary='Technical issue', query_text=\"I'm experiencing a glitch with your app, it keeps freezing after the latest update.\")\nQuery(id=3, query_summary='Pricing inquiry', query_text='What are the different pricing plans available for your subscription service?')\nQuery(id=4, query_summary='Password reset request', query_text='Can you provide instructions on how to reset my account password?')\nQuery(id=5, query_summary='Compatibility inquiry', query_text=\"I'm unsure about the compatibility of your product with my device, can you advise?\")\nQuery(id=6, query_summary='Order tracking', query_text='How can I track my recent order and estimate its delivery date?')\nQuery(id=7, query_summary='Loyalty program inquiry', query_text='Is there a customer loyalty program or rewards system for frequent buyers?')\nQuery(id=8, query_summary='Refund policy inquiry', query_text=\"I'm interested in your online courses, but do you offer refunds if I'm not satisfied?\")\nQuery(id=9, query_summary='Warranty inquiry', query_text='Could you clarify the coverage and limitations of your product warranty?')\nQuery(id=10, query_summary='Customer support inquiry', query_text='What are your customer support hours and how can I reach your team in case of emergencies?')\n</code></pre> <p>The summaries look good.</p> <p>Let's now define tags and ask the model to classify each query into a tag. In the Tag class, we set its docstring to the rules we want for the classification. This is done in the docstring because Tag is not a dataclass, but derived from Enum.</p> <p>No longer asking for the query_text in the Query class to keep output shorter.</p> <pre><code>from enum import Enum\n\nclass Tag(str, Enum):\n    \"\"\"Queries can be classified into the following tags:\ntech_support: queries related with technical problems.\nbilling: post-sale queries about billing cycle, or subscription termination.\naccount: queries about user account problems.\npre_sales: queries from prospective customers (who have not yet purchased).\nother: all other query topics.\"\"\"        \n    TECH_SUPPORT = \"tech_support\"\n    BILLING = \"billing\"\n    PRE_SALES = \"pre_sales\"\n    ACCOUNT = \"account\"\n    OTHER = \"other\"\n\n@dataclass        \nclass Query():\n    id: int\n    query_summary: str\n    query_tag: Tag\n\nout = model.extract(list[Query],\n                    in_text,\n                    inst=inst_text)\n\nfor query in out:\n    print(query)\n</code></pre> <pre><code>Query(id=1, query_summary='Asking about trial period', query_tag='pre_sales')\nQuery(id=2, query_summary='Reporting app issue', query_tag='tech_support')\nQuery(id=3, query_summary='Inquiring about pricing plans', query_tag='billing')\nQuery(id=4, query_summary='Requesting password reset instructions', query_tag='account')\nQuery(id=5, query_summary='Seeking device compatibility advice', query_tag='pre_sales')\nQuery(id=6, query_summary='Tracking order and delivery date', query_tag='other')\nQuery(id=7, query_summary='Inquiring about loyalty program', query_tag='billing')\nQuery(id=8, query_summary='Asking about refund policy', query_tag='pre_sales')\nQuery(id=9, query_summary='Seeking warranty information', query_tag='other')\nQuery(id=10, query_summary='Inquiring about customer support hours', query_tag='other')\n</code></pre> <p>The applied tags appear mostly reasonable. </p> <p>Of course, pre-sales tagging could be done automatically from a database of existing customer contacts, but the model is doing a good job of identifying questions likely to be pre-sales, like ids 1, 5 and 8 which are questions typically asked before buying/subscribing.</p> <p>Also, note that classification is being done from a single phrase. More information in each customer query would certainly allow for fine-grained classification.</p> <p>Example's assets at GitHub.</p>"},{"location":"examples/tough_meeting/","title":"Tough meeting","text":"<p>In this example we'll look at extracting participants and action items from a meeting transcript.</p> <p>Start by creating the model. As you'll see below, the transcript is large, with complex language, so we'll use OpenAI's GPT-4 this time. You can still use a local model by uncommenting the commented lines below.</p> <p>Make sure to set your OPENAI_API_KEY env variable.</p> <p>Jupyter notebook and Python script versions are available in the example's folder.</p> <p>Let's create the model.</p> <pre><code># load env variables like OPENAI_API_KEY from a .env file (if available)\ntry: from dotenv import load_dotenv; load_dotenv()\nexcept: ...\n\nfrom sibila import Models, GenConf\n\n# delete any previous model\ntry: del model\nexcept: ...\n\n# to use a local model, assuming it's in ../../models:\n# setup models folder:\n# Models.setup(\"../../models\")\n# the transcript is large, so we'll create the model with a context length of 3072, which should be enough.\n# model = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\", ctx_len=3072)\n\n# to use an OpenAI model:\nmodel = Models.create(\"openai:gpt-4\", ctx_len=3072)\n</code></pre> <p>We'll use a sample meeting transcript from https://www.ctas.tennessee.edu/eli/sample-meeting-transcript</p> <pre><code>transcript = \"\"\"\\\nChairman Wormsley (at the proper time and place, after taking the chair and striking the gavel on the table): This meeting of the CTAS County Commission will come to order. Clerk please call the role. (Ensure that a majority of the members are present.)\n\nChairman Wormsley: Each of you has received the agenda. I will entertain a motion that the agenda be approved.\n\nCommissioner Brown: So moved.\n\nCommissioner Hobbs: Seconded\n\nChairman Wormsley: It has been moved and seconded that the agenda be approved as received by the members. All those in favor signify by saying \"Aye\"?...Opposed by saying \"No\"?...The agenda is approved. You have received a copy of the minutes of the last meeting. Are there any corrections or additions to the meeting?\n\nCommissioner McCroskey: Mister Chairman, my name has been omitted from the Special Committee on Indigent Care.\n\nChairman Wormsley: Thank you. If there are no objections, the minutes will be corrected to include the name of Commissioner McCroskey. Will the clerk please make this correction. Any further corrections? Seeing none, without objection the minutes will stand approved as read. (This is sort of a short cut way that is commonly used for approval of minutes and/or the agenda rather than requiring a motion and second.)\n\nChairman Wormsley: Commissioner Adkins, the first item on the agenda is yours.\n\nCommissioner Adkins: Mister Chairman, I would like to make a motion to approve the resolution taking money from the Data Processing Reserve Account in the County Clerk's office and moving it to the equipment line to purchase a laptop computer.\n\nCommissioner Carmical: I second the motion.\n\nChairman Wormsley: This resolution has a motion and second. Will the clerk please take the vote.\n\nChairman Wormsley: The resolution passes. We will now take up old business. At our last meeting, Commissioner McKee, your motion to sell property near the airport was deferred to this meeting. You are recognized.\n\nCommissioner McKee: I move to withdraw that motion.\n\nChairman Wormsley: Commissioner McKee has moved to withdraw his motion to sell property near the airport. Seeing no objection, this motion is withdrawn. The next item on the agenda is Commissioner Rodgers'.\n\nCommissioner Rodgers: I move adopton of the resolution previously provided to each of you to increase the state match local litigation tax in circuit, chancery, and criminal courts to the maximum amounts permissible. This resolution calls for the increases to go to the general fund.\n\nChairman Wormsley: Commissioner Duckett\n\nCommissioner Duckett: The sheriff is opposed to this increase.\n\nChairman Wormsley: Commissioner, you are out of order because this motion has not been seconded as needed before the floor is open for discussion or debate. Discussion will begin after we have a second. Is there a second?\n\nCommissioner Reinhart: For purposes of discussion, I second the motion.\n\nChairman Wormsley: Commissioner Rodgers is recognized.\n\nCommissioner Rodgers: (Speaks about the data on collections, handing out all sorts of numerical figures regarding the litigation tax, and the county's need for additional revenue.)\n\nChairman Wormsley: Commissioner Duckett\n\nCommissioner Duckett: I move an amendment to the motion to require 25 percent of the proceeds from the increase in the tax on criminal cases go to fund the sheriff's department.\n\nChairman Wormsley: Commissioner Malone\n\nCommissioner Malone: I second the amendment.\n\nChairman Wormsley: A motion has been made and seconded to amend the motion to increase the state match local litigation taxes to the maximum amounts to require 25 percent of the proceeds from the increase in the tax on criminal cases in courts of record going to fund the sheriff's department. Any discussion? Will all those in favor please raise your hand? All those opposed please raise your hand. The amendment carries 17-2. We are now on the motion as amended. Any further discussion?\n\nCommissioner Headrick: Does this require a two-thirds vote?\n\nChairman Wormsley: Will the county attorney answer that question?\n\nCounty Attorney Fults: Since these are only courts of record, a majority vote will pass it. The two-thirds requirement is for the general sessions taxes.\n\nChairman Wormsley: Other questions or discussion? Commissioner Adams.\n\nCommissioner Adams: Move for a roll call vote.\n\nCommissioner Crenshaw: Second\n\nChairman Wormsley: The motion has been made and seconded that the state match local litigation taxes be increased to the maximum amounts allowed by law with 25 percent of the proceeds from the increase in the tax on criminal cases in courts of record going to fund the sheriff's department. Will all those in favor please vote as the clerk calls your name, those in favor vote \"aye,\" those against vote \"no.\" Nine votes for, nine votes against, one not voting. The increase fails. We are now on new business. Commissioner Adkins, the first item on the agenda is yours.\n\nCommissioner Adkins: Each of you has previously received a copy of a resolution to increase the wheel tax by $10 to make up the state cut in education funding. I move adoption of this resolution.\n\nChairman Wormsley: Commissioner Thompson\n\nCommissioner Thompson: I second.\n\nChairman Wormsley: It has been properly moved and seconded that a resolution increasing the wheel tax by $10 to make up the state cut in education funding be passed. Any discussion? (At this point numerous county commissioners speak for and against increasing the wheel tax and making up the education cuts. This is the first time this resolution is under consideration.) Commissioner Hayes is recognized.\n\nCommissioner Hayes: I move previous question.\n\nCommisioner Crenshaw: Second.\n\nChairman Wormsley: Previous question has been moved and seconded. As you know, a motion for previous question, if passed by a two-thirds vote, will cut off further debate and require us to vote yes or no on the resolution before us. You should vote for this motion if you wish to cut off further debate of the wheel tax increase at this point. Will all those in favor of previous question please raise your hand? Will all those against please raise your hand? The vote is 17-2. Previous question passes. We are now on the motion to increase the wheel tax by $10 to make up the state cut in education funding. Will all those in favor please raise your hand? Will all those against please raise your hand? The vote is 17-2. This increase passes on first passage. Is there any other new business? Since no member is seeking recognition, are there announcements? Commissioner Hailey.\n\nCommissioner Hailey: There will be a meeting of the Budget Committee to look at solid waste funding recommendations on Tuesday, July 16 at noon here in this room.\n\nChairman Wormsley: Any other announcements? The next meeting of this body will be Monday, August 19 at 7 p.m., here in this room. Commissioner Carmical.\n\nCommissioner Carmical: There will be a chili supper at County Elementary School on August 16 at 6:30 p.m. Everyone is invited.\n\nChairman Wormsley: Commissioner Austin.\n\nCommissioner Austin: Move adjournment.\n\nCommissioner Garland: Second.\n\nChairman Wormsley: Without objection, the meeting will stand adjourned.\n\"\"\"\n\n# model instructions text, also known as system message\ninst_text = \"Extract information and output in JSON format.\"\n</code></pre> <p>As you can see, this is a quite large transcript, filled with long names and complex phrases. Let's see how the model will handle it...</p> <p>Let's start by extracting the names of the participants in the meeting.</p> <p>We'll create the Meeting class with a list of strings, to receive the names of mentioned participants.</p> <p>The model will take clues from the variable names as well as from the description Field we set. In this case we name the string list \"participants\" and add a description of what we're looking to receive.</p> <pre><code>from pydantic import BaseModel, Field\n\n# this class definition will be used to constrain the model output and initialize an instance object\nclass Meeting(BaseModel):\n    participants: list[str] = Field(description=\"List of complete names of meeting participants\")\n\nin_text = \"Extract information from this meeting transcript:\\n\\n\" + transcript\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\nprint(out)\n</code></pre> <pre><code>participants=['Chairman Wormsley', 'Commissioner Brown', 'Commissioner Hobbs', 'Commissioner McCroskey', 'Commissioner Adkins', 'Commissioner Carmical', 'Commissioner McKee', 'Commissioner Rodgers', 'Commissioner Duckett', 'Commissioner Reinhart', 'Commissioner Malone', 'Commissioner Headrick', 'County Attorney Fults', 'Commissioner Adams', 'Commissioner Crenshaw', 'Commissioner Thompson', 'Commissioner Hayes', 'Commissioner Hailey', 'Commissioner Carmical', 'Commissioner Austin', 'Commissioner Garland']\n</code></pre> <pre><code># print the generated participants list:\nfor part in out.participants:\n    print(part)\n</code></pre> <pre><code>Chairman Wormsley\nCommissioner Brown\nCommissioner Hobbs\nCommissioner McCroskey\nCommissioner Adkins\nCommissioner Carmical\nCommissioner McKee\nCommissioner Rodgers\nCommissioner Duckett\nCommissioner Reinhart\nCommissioner Malone\nCommissioner Headrick\nCounty Attorney Fults\nCommissioner Adams\nCommissioner Crenshaw\nCommissioner Thompson\nCommissioner Hayes\nCommissioner Hailey\nCommissioner Carmical\nCommissioner Austin\nCommissioner Garland\n</code></pre> <p>Some names appear twice (\"Commissioner Carmical\") and the \"clerk\", which is mentioned in the text, is not listed.</p> <p>It's a matter of opinion if the clerk is an active participant, but let's try to fix the repeated names.</p> <p>Let's try asking for a list of participants \"without repeated entries\", in the field's description:</p> <pre><code>class Meeting(BaseModel):\n    participants: list[str] = Field(description=\"List of complete names of meeting participants without repeated entries\")\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\n\nfor part in out.participants:\n    print(part)\n</code></pre> <pre><code>Wormsley\nBrown\nHobbs\nMcCroskey\nAdkins\nCarmical\nMcKee\nRodgers\nDuckett\nReinhart\nMalone\nHeadrick\nFults\nAdams\nCrenshaw\nThompson\nHayes\nHailey\nAustin\nGarland\n</code></pre> <p>Didn't work as expected, repetition is gone but it dropped the titles, only names are appearing.</p> <p>Let's try asking for \"names and titles\":</p> <pre><code>class Meeting(BaseModel):\n    participants: list[str] = Field(description=\"List of names and titles of participants without repeated entries\")\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\n\nfor part in out.participants:\n    print(part)\n</code></pre> <pre><code>Chairman Wormsley\nCommissioner Brown\nCommissioner Hobbs\nCommissioner McCroskey\nCommissioner Adkins\nCommissioner Carmical\nCommissioner McKee\nCommissioner Rodgers\nCommissioner Duckett\nCommissioner Reinhart\nCommissioner Malone\nCommissioner Headrick\nCounty Attorney Fults\nCommissioner Adams\nCommissioner Crenshaw\nCommissioner Thompson\nCommissioner Hayes\nCommissioner Hailey\nCommissioner Carmical\nCommissioner Austin\nCommissioner Garland\n</code></pre> <p>And now \"Commissioner Carmical\" is repeating again! </p> <p>Let's move on, the point is that you can also do some prompt engineering with the description field. And this model shortcoming could be dealt with by post-processing the received list.</p> <p>Let's now also request a list of action items mentioned in the transcript:</p> <pre><code>class ActionItem(BaseModel):\n    index: int = Field(description=\"Sequential index for the action item\")\n    name: str = Field(description=\"Action item name\")\n\nclass Meeting(BaseModel):\n    participants: list[str] = Field(description=\"List of complete names of meeting participants\")\n    action_items: list[ActionItem] = Field(description=\"List of action items in the meeting\")\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\n\nprint(\"Participants\", \"-\" * 16)\nfor part in out.participants:\n    print(part)\nprint(\"Action items\", \"-\" * 16)\nfor ai in out.action_items:\n    print(ai)\n</code></pre> <pre><code>Participants ----------------\nChairman Wormsley\nCommissioner Brown\nCommissioner Hobbs\nCommissioner McCroskey\nCommissioner Adkins\nCommissioner Carmical\nCommissioner McKee\nCommissioner Rodgers\nCommissioner Duckett\nCommissioner Reinhart\nCommissioner Malone\nCommissioner Headrick\nCounty Attorney Fults\nCommissioner Adams\nCommissioner Crenshaw\nCommissioner Thompson\nCommissioner Hayes\nCommissioner Hailey\nCommissioner Carmical\nCommissioner Austin\nCommissioner Garland\nAction items ----------------\nindex=1 name='Approve the agenda'\nindex=2 name='Correct the minutes to include Commissioner McCroskey in the Special Committee on Indigent Care'\nindex=3 name='Approve the resolution to transfer funds from the Data Processing Reserve Account to purchase a laptop'\nindex=4 name='Withdraw the motion to sell property near the airport'\nindex=5 name='Adopt the resolution to increase the state match local litigation tax'\nindex=6 name=\"Amend the motion to allocate 25 percent of the proceeds from the tax increase to fund the sheriff's department\"\nindex=7 name='Vote on the state match local litigation taxes increase with the amendment'\nindex=8 name='Adopt the resolution to increase the wheel tax by $10 for education funding'\nindex=9 name='Hold a Budget Committee meeting on solid waste funding recommendations'\nindex=10 name='Announce the chili supper at County Elementary School'\n</code></pre> <p>These are reasonable action items.</p> <p>Let's now also request a priority for each ActionItem - we'll create a string Enum class with three priority levels.</p> <pre><code>from enum import Enum\n\nclass ActionPriority(str, Enum):\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass ActionItem(BaseModel):\n    index: int = Field(description=\"Sequential index for the action item\")\n    name: str = Field(description=\"Action item name\")\n    priority: ActionPriority = Field(description=\"Action item priority\")\n\nclass Meeting(BaseModel):\n    participants: list[str] = Field(description=\"List of complete names of meeting participants\")\n    action_items: list[ActionItem] = Field(description=\"List of action items in the meeting\")\n\nout = model.extract(Meeting,\n                    in_text,\n                    inst=inst_text)\n\nprint(\"Participants\", \"-\" * 16)\nfor part in out.participants:\n    print(part)\nprint(\"Action items\", \"-\" * 16)\nfor ai in out.action_items:\n    print(ai)\n</code></pre> <pre><code>Participants ----------------\nChairman Wormsley\nCommissioner Brown\nCommissioner Hobbs\nCommissioner McCroskey\nCommissioner Adkins\nCommissioner Carmical\nCommissioner McKee\nCommissioner Rodgers\nCommissioner Duckett\nCommissioner Reinhart\nCommissioner Malone\nCommissioner Headrick\nCounty Attorney Fults\nCommissioner Adams\nCommissioner Crenshaw\nCommissioner Thompson\nCommissioner Hayes\nCommissioner Hailey\nCommissioner Carmical\nCommissioner Austin\nCommissioner Garland\nAction items ----------------\nindex=1 name='Approve the agenda' priority=&lt;ActionPriority.HIGH: 'high'&gt;\nindex=2 name='Correct the minutes to include Commissioner McCroskey' priority=&lt;ActionPriority.MEDIUM: 'medium'&gt;\nindex=3 name='Approve the resolution to transfer funds for laptop purchase' priority=&lt;ActionPriority.HIGH: 'high'&gt;\nindex=4 name='Withdraw motion to sell property near the airport' priority=&lt;ActionPriority.MEDIUM: 'medium'&gt;\nindex=5 name='Adopt resolution to increase state match local litigation tax' priority=&lt;ActionPriority.HIGH: 'high'&gt;\nindex=6 name=\"Amend resolution to allocate funds to sheriff's department\" priority=&lt;ActionPriority.HIGH: 'high'&gt;\nindex=7 name='Vote on the amended resolution for litigation tax increase' priority=&lt;ActionPriority.HIGH: 'high'&gt;\nindex=8 name='Adopt resolution to increase the wheel tax' priority=&lt;ActionPriority.HIGH: 'high'&gt;\nindex=9 name='Budget Committee meeting on solid waste funding' priority=&lt;ActionPriority.MEDIUM: 'medium'&gt;\nindex=10 name='Announce chili supper at County Elementary School' priority=&lt;ActionPriority.LOW: 'low'&gt;\nindex=11 name='Adjourn the meeting' priority=&lt;ActionPriority.MEDIUM: 'medium'&gt;\n</code></pre> <p>It's not clear from the meeting transcript text if these priorities are correct, but some items related to taxes are receiving high priorities, from the context, it looks reasonable that taxes are a priority. : )</p> <p>Example's assets at GitHub.</p>"},{"location":"extract/dataclass/","title":"Dataclass","text":"<p>Besides simple types and enums, we can also extract objects whose structure is given by a dataclass definition:</p> <p>Example</p> <pre><code>from sibila import Models\nfrom dataclasses import dataclass\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\n@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    source_location: str\n\nin_text = \"\"\"\\\nSeated at a corner table was Lucy Bennett, a 28-year-old journalist from London, \nher pen poised to capture the essence of the world around her. \nHer eyes sparkled with curiosity, mirroring the dynamic energy of her beloved city.\n\"\"\"\n\nmodel.extract(Person,\n              in_text)\n</code></pre> <p>Result</p> <pre><code>Person(first_name='Lucy', \n       last_name='Bennett',\n       age=28, \n       occupation='journalist',\n       source_location='London')\n</code></pre> <p>See the Pydantic version here.</p> <p>We can extract a list of Person objects by using list[Person]:</p> <p>Example</p> <pre><code>in_text = \"\"\"\\\nSeated at a corner table was Lucy Bennett, a 28-year-old journalist from London, \nher pen poised to capture the essence of the world around her. \nHer eyes sparkled with curiosity, mirroring the dynamic energy of her beloved city.\n\nOpposite Lucy sat Carlos Ramirez, a 35-year-old architect from the sun-kissed \nstreets of Barcelona. With a sketchbook in hand, he exuded creativity, \nhis passion for design evident in the thoughtful lines that adorned his face.\n\"\"\"\n\nmodel.extract(list[Person],\n              in_text)\n</code></pre> <p>Result</p> <pre><code>[Person(first_name='Lucy', \n        last_name='Bennett',\n        age=28, \n        occupation='journalist',\n        source_location='London'),\n Person(first_name='Carlos', \n        last_name='Ramirez',\n        age=35,\n        occupation='architect',\n        source_location='Barcelona')]\n</code></pre>"},{"location":"extract/dataclass/#field-annotations","title":"Field annotations","text":"<p>As when extracting to simple types, we could also provide instructions by setting the inst argument. However, instructions are by nature general and when extracting structured data, it's harder to provide specific instructions for fields.</p> <p>For this purpose, field annotations are more effective than instructions: they can be provided to clarify what we want extracted for each specific field.</p> <p>For dataclasses this is done with Annotated[type, \"description\"] - see the \"start\" and \"end\" attributes of the Period class:</p> <p>Example</p> <pre><code>from typing import Annotated\n\nWeekday = Literal[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"\n]\n\n@dataclass\nclass Period():\n    start: Annotated[Weekday, \"Day of arrival\"]\n    end: Annotated[Weekday, \"Day of departure\"]\n\nmodel.extract(Period,\n              \"Right, well, I was planning to arrive on Wednesday and \"\n              \"only leave Sunday morning. Would that be okay?\")\n</code></pre> <p>Result</p> <pre><code>Period(start='Wednesday', end='Sunday')\n</code></pre> <p>In this manner, the model can be informed of what is wanted for each specific field.</p> <p>Check the Extract dataclass example to see this in action.</p>"},{"location":"extract/enums/","title":"Enums","text":"<p>Enumerations are important for classification tasks or in any situation where you need a choice to be made from a list of options.</p> <p>Example</p> <pre><code>from sibila import Models\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\nmodel.extract([\"red\", \"blue\", \"green\", \"yellow\"], \n              \"The car color was a shade of indigo\")\n</code></pre> <p>Result</p> <pre><code>'blue'\n</code></pre> <p>You can pass a list of items in any of the supported native types: str, float, int or bool.</p>"},{"location":"extract/enums/#literals","title":"Literals","text":"<p>We can also use Literals:</p> <p>Example</p> <pre><code>from typing import Literal\n\nmodel.extract(Literal[\"SPAM\", \"NOT_SPAM\", \"UNSURE\"], \n             \"Hello my dear friend, I'm contacting you because I want to give you a million dollars\",\n             inst=\"Classify this text on the likelihood of being spam\")\n</code></pre> <p>Result</p> <pre><code>'SPAM'\n</code></pre> <p>Extracting to a Literal type returns one of its possible options in its native type (str, float, int or bool).</p>"},{"location":"extract/enums/#enum-classes","title":"Enum classes","text":"<p>Or Enum classes of native types. An example of extracting to Enum classes:</p> <p>Example</p> <pre><code>from enum import IntEnum\n\nclass Heads(IntEnum):\n    SINGLE = 1\n    DOUBLE = 2\n    TRIPLE = 3\n\nmodel.extract(Heads,\n              \"The Two-Headed Monster from The Muppets.\")\n</code></pre> <p>Result</p> <pre><code>&lt;Heads.DOUBLE: 2&gt;\n</code></pre> <p>For the model, the important information is actual the value of each enum member, not its name. For example, in this enum, the model would only see the strings to the right of each member (the enum values), not \"RED\", \"ORANGE\" nor \"GREEN\":</p> <pre><code>class Light(Enum):\n    RED = 'stop'\n    YELLOW = 'slow down'\n    GREEN = 'go'\n</code></pre> <p>See the Tag classification example to see how Enum is used to tag support queries.</p>"},{"location":"extract/enums/#classify","title":"Classify","text":"<p>You can also use the classify() method to extract enumerations, which accepts the enum types we've seen above. It calls extract() internally and its only justification is to make things more readable:</p> <p>Example</p> <pre><code>model.classify([\"mouse\", \"cat\", \"dog\", \"bird\"],\n               \"Snoopy\")\n</code></pre> <p>Result</p> <pre><code>'dog'\n</code></pre>"},{"location":"extract/free_json/","title":"Free JSON","text":"<p>Methods like extract() will generate JSON format constrained to a certain JSON Schema: this is needed or the model might not generate the fields or data types we're looking for.</p> <p>You can generate schema-free JSON with the json() method. In this case, the model will pick the field names and data types for you.</p> <p>For example:</p> <p>Example</p> <pre><code>from sibila import Models\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\nresponse = model(\"How to build a brick wall?\")\n\nfrom pprint import pprint\npprint(response, sort_dicts=False)    \n</code></pre> <p>Result</p> <pre><code>{'steps': [{'step_number': 1,\n            'description': 'Gather all necessary materials and tools including '\n                        'bricks, mortar, trowel, spirit level, tape '\n                        'measure, bricklaying line, and safety equipment.'},\n        {'step_number': 2,\n            'description': 'Prepare the foundation for the wall. Ensure it is '\n                        'solid, level, and has the correct dimensions for '\n                        'the wall you are building.'},\n        {'step_number': 3,\n            'description': \"Mix the mortar according to the manufacturer's \"\n                        'instructions, ensuring a consistent and workable '\n                        'consistency.'},\n        {'step_number': 4,\n            'description': 'Lay a bed of mortar where the first row of bricks '\n                        'will be placed. Use the trowel to spread the '\n                        'mortar evenly.'},\n        {'step_number': 5,\n            'description': 'Start laying the bricks from one end, applying '\n                        'mortar to the end of each brick before placing it '\n                        'down to bond with the next brick.'},\n        {'step_number': 6,\n            'description': 'Use the spirit level to check that the bricks are '\n                        'level both horizontally and vertically. Adjust as '\n                        'necessary.'},\n        {'step_number': 7,\n            'description': 'Continue laying bricks, ensuring that you stagger '\n                        'the joints in each row (running bond pattern). '\n                        'This adds strength to the wall.'},\n        {'step_number': 8,\n            'description': 'Periodically check that the wall is straight and '\n                        'level by using the spirit level and the '\n                        'bricklaying line.'},\n        {'step_number': 9,\n            'description': 'Remove any excess mortar with the trowel as you '\n                        'work. Keep the work area clean.'},\n        {'step_number': 10,\n            'description': 'As you reach the end of each row, you may need to '\n                        'cut bricks to fit. Use a brick hammer or a brick '\n                        'cutter to do this.'},\n        {'step_number': 11,\n            'description': 'Once the wall reaches the desired height, finish '\n                        'the top with a row of solid bricks or capping '\n                        'stones to protect the wall from weather.'},\n        {'step_number': 12,\n            'description': 'Cure the mortar by protecting the wall from '\n                        'extreme weather conditions for at least 24-48 '\n                        'hours.'},\n        {'step_number': 13,\n            'description': 'Clean the finished wall with a brush and water to '\n                        'remove any remaining mortar residue.'},\n        {'step_number': 14,\n            'description': 'Dispose of any waste material responsibly and '\n                        'clean your tools.'}],\n'safety_tips': ['Wear safety glasses to protect your eyes from flying debris.',\n                'Use gloves to protect your hands from sharp edges and wet '\n                'mortar.',\n                'Wear a dust mask when mixing mortar to avoid inhaling dust '\n                'particles.',\n                'Keep the work area clear to prevent tripping hazards.'],\n'tools_required': ['Bricks',\n                    'Mortar',\n                    'Trowel',\n                    'Spirit level',\n                    'Tape measure',\n                    'Bricklaying line',\n                    'Safety glasses',\n                    'Gloves',\n                    'Dust mask',\n                    'Brick hammer or cutter']}\n</code></pre> <p>The model returned a Python dictionary with fields and data types of it's own choice. We could provide a JSON Schema t defines a structure for the response.</p> <p>See the From text to object example for a related use.</p>"},{"location":"extract/free_text/","title":"Free text","text":"<p>You can also generate plain text by calling Model() or Model.call():</p> <p>Example</p> <pre><code>from sibila import Models\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\nresponse = model(\"Explain in a few lines how to build a brick wall?\")\nprint(response)\n</code></pre> <p>Result</p> <pre><code>To build a brick wall, follow these steps:\n\n1. Prepare the site by excavating and leveling the ground, then install a damp-proof \nmembrane and create a solid base with concrete footings.\n2. Lay a foundation of concrete blocks or bricks, ensuring it is level and square.\n3. Build the wall using bricks or blocks, starting with a corner or bonding pattern \nto ensure stability. Use mortar to bond each course (row) of bricks or blocks, \nfollowing the recommended mortar mix ratio.\n4. Use a spirit level to ensure each course is level, and insert metal dowels or use \nbrick ties to connect adjacent walls or floors.\n5. Allow the mortar to dry for the recommended time before applying a damp-proof \ncourse (DPC) at the base of the wall.\n6. Finish the wall with capping bricks or coping stones, and apply any desired \nrender or finish.\n</code></pre>"},{"location":"extract/pydantic/","title":"Pydantic","text":"<p>Besides simple types and enums, we can also extract objects whose structure is given by a class derived from Pydantic's BaseModel definition:</p> <p>Example</p> <pre><code>from sibila import Models\nfrom pydantic import BaseModel\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n    age: int\n    occupation: str\n    source_location: str\n\nin_text = \"\"\"\\\nSeated at a corner table was Lucy Bennett, a 28-year-old journalist from London, \nher pen poised to capture the essence of the world around her. \nHer eyes sparkled with curiosity, mirroring the dynamic energy of her beloved city.\n\"\"\"\n\nmodel.extract(Person,\n              in_text)\n</code></pre> <p>Result</p> <pre><code>Person(first_name='Lucy', \n       last_name='Bennett',\n       age=28, \n       occupation='journalist',\n       source_location='London')\n</code></pre> <p>See the dataclass version here.</p> <p>We can extract a list of Person objects by using list[Person]:</p> <p>Example</p> <pre><code>in_text = \"\"\"\\\nSeated at a corner table was Lucy Bennett, a 28-year-old journalist from London, \nher pen poised to capture the essence of the world around her. \nHer eyes sparkled with curiosity, mirroring the dynamic energy of her beloved city.\n\nOpposite Lucy sat Carlos Ramirez, a 35-year-old architect from the sun-kissed \nstreets of Barcelona. With a sketchbook in hand, he exuded creativity, \nhis passion for design evident in the thoughtful lines that adorned his face.\n\"\"\"\n\nmodel.extract(list[Person],\n              in_text)\n</code></pre> <p>Result</p> <pre><code>[Person(first_name='Lucy', \n        last_name='Bennett',\n        age=28, \n        occupation='journalist',\n        source_location='London'),\n Person(first_name='Carlos', \n        last_name='Ramirez',\n        age=35,\n        occupation='architect',\n        source_location='Barcelona')]\n</code></pre>"},{"location":"extract/pydantic/#field-annotations","title":"Field annotations","text":"<p>As when extracting to simple types, we could also provide instructions by setting the inst argument. However, instructions are by nature general and when extracting structured data, it's harder to provide specific instructions for fields.</p> <p>For this purpose, field annotations are more effective than instructions: they can be provided to clarify what we want extracted for each specific field.</p> <p>For Pydantic this is done with Field(description=\"description\") - see the \"start\" and \"end\" attributes of the Period class:</p> <p>Example</p> <pre><code>from pydantic import Field\n\nWeekday = Literal[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"\n]\n\nclass Period(BaseModel):\n    start: Weekday = Field(description=\"Day of arrival\")\n    end: Weekday = Field(description=\"Day of departure\")\n\nmodel.extract(Period,\n              \"Right, well, I was planning to arrive on Wednesday and \"\n              \"only leave Sunday morning. Would that be okay?\")\n</code></pre> <p>Result</p> <pre><code>Period(start='Wednesday', end='Sunday')\n</code></pre> <p>In this manner, the model can be informed of what is wanted for each specific field.</p> <p>Check the Extract Pydantic example to see this kind of extraction.</p>"},{"location":"extract/simple_types/","title":"Simple types","text":"<p>Sibila can constrain model generation to output simple python types. This is helpful for situations where you want to extract a specific data type. </p> <p>To get a response from the model in a certain type, you can use the extract() method:</p> <p>Example</p> <pre><code>from sibila import Models\n\nModels.setup(\"../models\")\nmodel = Models.create(\"llamacpp:openchat\")\n\nmodel.extract(bool, \n              \"Certainly, I'd like to subscribe.\")\n</code></pre> <p>Result</p> <pre><code>True\n</code></pre>"},{"location":"extract/simple_types/#instructions-to-help-the-model","title":"Instructions to help the model","text":"<p>You may need to provide more extra information to the model, so that it understands what you want. This is done with the inst argument - inst is a shorter name for instructions:</p> <p>Example</p> <pre><code>model.extract(str, \n            \"I don't quite remember the product's name, I think it was called Cornaca\",\n            inst=\"Extract the product name\")\n</code></pre> <p>Result</p> <pre><code>Cornaca\n</code></pre>"},{"location":"extract/simple_types/#supported-types","title":"Supported types","text":"<p>The following simple types are supported:</p> <ul> <li>bool</li> <li>int</li> <li>float</li> <li>str</li> <li>datetime</li> </ul> <p>About datetime type</p> <p>A special note about extracting to datetime: the datetime type is expecting an ISO 8601 formatted string. Because some models are less capable than others at correctly formatting dates/times, it helps to mention in the instructions that you want the output in \"ISO 8601\" format.</p> <pre><code>from datetime import datetime\nmodel.extract(datetime, \n              \"Sure, glad to help, it all happened at December the 10th, 2023, around 3PM, I think\",\n              inst=\"Output in ISO 8601 format\")\n</code></pre> <p>Result</p> <pre><code>datetime.datetime(2023, 12, 10, 15, 0)\n</code></pre>"},{"location":"extract/simple_types/#lists","title":"Lists","text":"<p>You can extract lists of any of the supported types (simple types, enum, dataclass, Pydantic).</p> <p>Example</p> <pre><code>model.extract(list[str], \n             \"I'd like to visit Naples, Genoa, Florence and of course, Rome\")\n</code></pre> <p>Result</p> <pre><code>['Naples', 'Genoa', 'Florence', 'Rome']\n</code></pre> <p>As in all extractions, you may need to set the instructions text to specify what you want from the model. Just as an example of the power of instructions, let's add instructions asking for country output: it will still output a list, but with a single element - 'Italy':</p> <p>Example</p> <pre><code>model.extract(list[str], \n             \"I'd like to visit Naples, Genoa, Florence and of course, Rome\",\n             inst=\"Output the country\")\n</code></pre> <p>Result</p> <pre><code>['Italy']\n</code></pre>"},{"location":"models/anthropic/","title":"Anthropic","text":"<p>With Sibila you can access Anthropic remote models, for which you'll need an API key. Although you can pass this key when you create the model object, it's more secure to define an env variable with this information:</p> Linux and MacWindows <pre><code>export ANTHROPIC_API_KEY=\"...\"\n</code></pre> <pre><code>setx ANTHROPIC_API_KEY \"...\"\n</code></pre> <p>Another possibility is to store your API key in .env files, which has many advantages: see the dotenv-python package.</p>"},{"location":"models/anthropic/#creating-models","title":"Creating models","text":"<p>Anthropic models can be used by Sibila through the AnthropicModel class. </p> <p>Example</p> <pre><code>from sibila import AnthropicModel\n\nmodel = AnthropicModel(\"claude-3-opus-20240229\")\n\nmodel(\"I think that I shall never see.\")\n</code></pre> <p>Result</p> <pre><code>It sounds like you may be quoting the opening line of the poem \"Trees\" by Joyce Kilmer, \nwhich begins \"I think that I shall never see / A poem lovely as a tree.\" \nHowever, to avoid potentially reproducing copyrighted material, I won't quote or \ncomplete the poem. The poem is a short lyrical one from the early 20th century \nthat expresses the author's love and appreciation for the beauty of trees. \nIt's a well-known poem that reflects on the magnificence of nature. \nLet me know if you would like me to provide any other information about \nthe poem or poet that doesn't involve directly quoting the copyrighted work.\n</code></pre> <p>You can also create an Anthropic model in the Models factory by using the \"anthropic:\" provider prefix like this:</p> <pre><code>from sibila import Models\n\nmodel = Models.create(\"anthropic:claude-3-opus-20240229\")\n</code></pre>"},{"location":"models/anthropic/#model-list","title":"Model list","text":"<p>The models made available by Anthropic are listed here.</p> <p>Anthropic doesn't provide an API to list the models, so AnthropicModel.known_models() will return None.</p> <p>At the time of writing, these are the available models, all supporting JSON Schema extraction:</p> <ul> <li>claude-3-opus-20240229</li> <li>claude-3-sonnet-20240229</li> <li>claude-3-haiku-20240307</li> <li>claude-2.1</li> <li>claude-2.0</li> </ul>"},{"location":"models/find_local_models/","title":"Finding new models","text":""},{"location":"models/find_local_models/#chat-or-instruct-types-only","title":"Chat or instruct types only","text":"<p>Sibila can use models that were fine-tuned for chat or instruct purposes. These models work in user - assistant turns or messages and use a chat template to properly compose those messages to the format that the model was fine-tuned to.</p> <p>For example, the Llama2 model was released in two editions: a simple Llama2 text completion model and a Llama2-instruct model that was fine tuned for user-assistant turns. For Sibila you should always select chat or instruct versions of a model.</p> <p>But which model to choose? You can look at model benchmark scores in popular listing sites:</p> <ul> <li>https://llm.extractum.io/list/</li> <li>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</li> <li>https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</li> </ul>"},{"location":"models/find_local_models/#find-a-quantized-version-of-the-model","title":"Find a quantized version of the model","text":"<p>Since Large Language Models are quite big, they are usually quantized so that each parameter occupies a little more than 4 bits or half a byte. </p> <p>Without quantization, a 7 billion parameters model would require 14Gb of memory (with each parameter taking 16 bits) to load and a bit more during inference.</p> <p>But with quantization techniques, a 7 billion parameters model can have a file size of only 4.4Gb (using about 50% more in memory - 6.8Gb), which makes it accessible to be ran in common GPUs or even in common RAM memory (albeit slower).</p> <p>Quantized models are stored in a file format popularized by llama.cpp, the GGUF format (which means GPT-Generated Unified Format). We're using llama.cpp to run local models, so we'll be needing GGUF files.</p> <p>A good place to find quantized models is in HuggingFace's model hub, particularly in the well-know TheBloke's (Tom Jobbins) area:</p> <p>https://huggingface.co/TheBloke</p> <p>TheBloke is very prolific in producing quality quantized versions of models, usually shortly after they are released.</p> <p>And a good model that we'll be using for the examples is a 4 bit quantization of the OpenChat-3.5 model, which itself is a fine-tuning of Mistral-7b:</p> <p>https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF</p>"},{"location":"models/find_local_models/#download-the-file-into-the-models-folder","title":"Download the file into the \"models\" folder","text":"<p>See the OpenChat model section on how to download models with the sibila CLI tool or manually in your browser.</p> <p>The OpenChat model already includes the chat template format in its metadada, but for some other models we'll need to set the format - see the Setup chat template format section on how to handle this.</p>"},{"location":"models/fireworks/","title":"Fireworks AI","text":"<p>With Sibila you can use the models hosted by Fireworks AI, for which you'll need an API key (which is initially free). As in other providers, although you can pass this key when you create the model object, it's more secure to define an env variable with this information:</p> Linux and MacWindows <pre><code>export FIREWORKS_API_KEY=\"...\"\n</code></pre> <pre><code>setx FIREWORKS_API_KEY \"...\"\n</code></pre> <p>Another possibility is to store your API key in .env files, which has many advantages: see the dotenv-python package.</p>"},{"location":"models/fireworks/#creating-models","title":"Creating models","text":"<p>Models served by Together.ai can be used by Sibila through the FireworksModel class. </p> <p>Example</p> <pre><code>from sibila import FireworksModel\n\nmodel = FireworksModel(\"accounts/fireworks/models/gemma-7b-it\")\n\nmodel(\"I think that I shall never see.\")\n</code></pre> <p>Result (model is hallucinating)</p> <pre><code>The poem \"I think that I shall never see\" is a poem by William Blake. \nIt is a poem about the loss of sight. The speaker is saying that they \nwill never be able to see again. The poem is a reflection on the beauty \nof sight and the sadness of blindness.\n</code></pre> <p>You can also create a Fireworks AI model in the Models factory by using the \"fireworks:\" provider prefix:</p> <pre><code>from sibila import Models\n\nmodel = Models.create(\"fireworks:accounts/fireworks/models/gemma-7b-it\")\n</code></pre>"},{"location":"models/fireworks/#model-list","title":"Model list","text":"<p>The available Fireworks text inference models models are listed here. </p> <p>Unfortunately Fireworks AI doesn't provide an API to list the models, so FireworksModel.known_models() will return None.</p>"},{"location":"models/fireworks/#json-schema-models","title":"JSON Schema models","text":"<p>All the Fireworks AI models should support JSON Schema generation, which is required for structured data extraction.</p>"},{"location":"models/formats_json/","title":"Managing formats","text":"<p>A \"formats.json\" file stores the chat template definitions used in models. This allows for models that don't have a chat template in their metadata to be detected and get the right format so they can function well.</p> <p>If you downloaded the GitHub repository, you'll find a file named \"sibila/res/base_formats.json\", which is the default base configuration that will be used, with many known chat template formats. </p> <p>When you call Models.setup(), any \"formats.json\" file found in the folder will be loaded and its definitions will be merged with the ones from \"base_formats.json\" which are loaded on initialization. Any entries with the same name will be replaced by freshly loaded ones.</p> <p>How to add a new format entry that can be used when creating a model? You can do it with the sibila CLI tool or by manually editing the formats.json file.</p>"},{"location":"models/formats_json/#with-sibila-formats-cli-tool","title":"With \"sibila formats\" CLI tool","text":"<p>Run the sibila CLI tool in the \"models\" folder:</p> <pre><code>&gt; sibila formats -s openchat openchat \"{{ bos_token }}...{% endif %}\"\n\nUsing models directory '.'\nSet format 'openchat' with match='openchat', template='{{ bos_token }}...'\n</code></pre> <p>First argument after -s is the format entry name, second the match regular expression (to identify the model filename) and finally the template. Help is available with \"sibila formats --help\".</p>"},{"location":"models/formats_json/#manually-edit-formatsjson","title":"Manually edit \"formats.json\"","text":"<p>In alternative, we can edit the \"formats.json\" file in the \"Models\" folder, and add the entry:</p> <pre><code>\"openchat\": {\n    \"match\": \"openchat\", # a regexp to match model name or filename\n    \"template\": \"{{ bos_token }}...\"\n},\n</code></pre> <p>In the \"openchat\" key value we have a dictionary with the following keys:</p> Key match Regular expression that will be used to match the model name or filename template The chat template definition in Jinja format <p>The \"openchat\" format name we are defining here is the name you can use when creating a model, by setting the format argument:</p> <pre><code>model = LlamaCppModel.create(\"openchat-3.5-1210.Q4_K_M.gguf\",\n                             format=\"openchat\")\n</code></pre> <p>or to be more practical: \"openchat\" is also the format name you would use when creating a \"models.json\" entry for a model, in the \"format\" key:</p> <pre><code>\"openchat\": {\n    \"name\": \"openchat-3.5-1210.Q4_K_M.gguf\",\n    \"format\": \"openchat\" # chat template format used by this model\n},\n</code></pre> <p>See the \"base_formats.json\" file for all the default base formats.</p>"},{"location":"models/local_model/","title":"Using a local model","text":"<p>Sibila uses llama.cpp to run local models, which are ordinary files in the GGUF format. You can download local models from places like the Hugging Face model hub.</p> <p>Most current 7B quantized models are very capable for common data extraction tasks (and getting better all the time). We'll see how to find and setup local models for use with Sibila. If you only plan to use OpenAI remote models, you can skip this section.</p> <p></p>"},{"location":"models/local_model/#openchat-model","title":"OpenChat model","text":"<p>By default, most of the examples included with Sibila use OpenChat, a very good 7B parameters quantized model: https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF</p> <p>You can download this model with the sibila CLI tool or manually in your browser.</p>"},{"location":"models/local_model/#download-with-sibila-hub","title":"Download with \"sibila hub\"","text":"<p>Open a command line prompt in the \"models\" folder if you downloaded the GitHub repository, or create a folder named \"models\".</p> <p>Run this command:</p> <pre><code>sibila hub -d TheBloke/openchat-3.5-1210-GGUF -f openchat-3.5-1210.Q4_K_M.gguf\n</code></pre> <p>After downloading the 4.4Gb, the file \"openchat-3.5-1210.Q4_K_M.gguf\" will be available in your \"models\" folder and you can run the examples. You can do the same to download any other GGUF models.</p>"},{"location":"models/local_model/#manual-download","title":"Manual download","text":"<p>Alternatively, you can download in your browser from this URL:</p> <p>https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF/blob/main/openchat-3.5-1210.Q4_K_M.gguf</p> <p>In the linked page, click \"download\" and save this file into a \"models\" folder. If you downloaded the Sibila GitHub repository it already includes a \"models\" folder which you can use. Otherwise, just create a \"models\" folder, where you'll store your local model files.</p> <p>Once the file \"openchat-3.5-1210.Q4_K_M.gguf\" is placed in the \"models\" folder, you should be able to run the examples.</p>"},{"location":"models/local_model/#llamacppmodel-class","title":"LlamaCppModel class","text":"<p>Local llama.cpp models can be used with the LlamaCppModel class. Let's generate text after our prompt:</p> <p>Example</p> <pre><code>from sibila import LlamaCppModel\n\nmodel = LlamaCppModel(\"../../models/openchat-3.5-1210.Q4_K_M.gguf\")\n\nmodel(\"I think that I shall never see.\")\n</code></pre> <p>Result</p> <pre><code>'A poem as lovely as a tree.'\n</code></pre> <p>It worked: the model answered with the continuation of the famous poem.</p> <p>You'll notice that the first time you create the model object and run a query, it will take longer, because the model must load all its parameters into layers in memory. The next queries will work much faster.</p>"},{"location":"models/local_model/#a-note-about-out-of-memory-errors","title":"A note about out of memory errors","text":"<p>An important thing to know if you'll be using local models is about \"Out of memory\" errors.</p> <p>A 7B model like OpenChat-3.5, when quantized to 4 bits will occupy about 6.8 Gb of memory, in either GPU's VRAM or common RAM. If you try to run a second model at the same time, you might get an out of memory error and/or llama.cpp may crash: it all depends on the memory available in your computer.</p> <p>This is less of a problem when running scripts from the command line, but in environments like Jupyter where you can have multiple open notebooks, you may get \"out of memory\" errors or python kernel errors like:</p> <p>Error</p> <pre><code>Kernel Restarting\nThe kernel for sibila/examples/name.ipynb appears to have died.\nIt will restart automatically.\n</code></pre> <p>If you get an error like this in JupyterLab, open the Kernel menu and select \"Shut Down All Kernels...\". This will get rid of any out-of-memory stuck models.</p> <p>A good practice is to delete any local model after you no longer need it or right before loading a new one. A simple \"del model\" works fine, or you can add these two lines before creating a model:</p> <pre><code>try: del model\nexcept: ...\n\nmodel = LlamaCppModel(...)\n</code></pre> <p>This way, any existing model in the current notebook is deleted before creating a new one.</p> <p>However this won't work across multiple notebooks. In those cases, open JupyterLab's Kernel menu and select \"Shut Down All Kernels...\". This will get rid of any models currently in memory.</p>"},{"location":"models/mistral/","title":"Mistral AI","text":"<p>With Sibila you can access Mistral AI remote models, for which you'll need an API key. Although you can pass this key when you create the model object, it's more secure to define an env variable with this information:</p> Linux and MacWindows <pre><code>export MISTRAL_API_KEY=\"...\"\n</code></pre> <pre><code>setx MISTRAL_API_KEY \"...\"\n</code></pre> <p>Another possibility is to store your API key in .env files, which has many advantages: see the dotenv-python package.</p>"},{"location":"models/mistral/#creating-models","title":"Creating models","text":"<p>Mistral AI models can be used by Sibila through the MistralModel class. </p> <p>Example</p> <pre><code>from sibila import MistralModel\n\nmodel = MistralModel(\"mistral-large-latest\")\n\nmodel(\"I think that I shall never see.\")\n</code></pre> <p>Result</p> <pre><code>A poem as lovely as a tree.\n\nThis is a line from the poem \"Trees\" by Joyce Kilmer. The full poem is:\n\nI think that I shall never see\nA poem lovely as a tree.\n\nA tree whose hungry mouth is prest\nAgainst the earth\u2019s sweet flowing breast;\n\nA tree that looks at God all day,\nAnd lifts her leafy arms to pray;\n\nA tree that may in Summer wear\nA nest of robins in her hair;\n\nUpon whose bosom snow has lain;\nWho intimately lives with rain.\n\nPoems are made by fools like me,\nBut only God can make a tree.\n\nDo you have any other questions or is there something else you'd like to talk about?\nI'm here to help!\n</code></pre> <p>You can also create a Mistral model in the Models factory by using the \"mistral:\" provider prefix like this:</p> <pre><code>from sibila import Models\n\nmodel = Models.create(\"mistral:mistral-large-latest\")\n</code></pre>"},{"location":"models/mistral/#model-list","title":"Model list","text":"<p>The models made available by Mistral AI are listed here. You can also get a list of models with known_models():</p> <p>Example</p> <pre><code>MistralModel.known_models()\n</code></pre> <p>Result</p> <pre><code>['mistral-embed',\n 'mistral-large-2402',\n 'mistral-large-latest',\n 'mistral-medium',\n 'mistral-medium-2312',\n 'mistral-medium-latest',\n 'mistral-small',\n 'mistral-small-2312',\n 'mistral-small-2402',\n 'mistral-small-latest',\n 'mistral-tiny',\n 'mistral-tiny-2312',\n 'open-mistral-7b',\n 'open-mixtral-8x7b']\n</code></pre> <p>At the time of writing, all Mistral AI models support JSON Schema extraction.</p>"},{"location":"models/models_factory/","title":"Models factory","text":"<p>The Models factory is based in a \"models\" folder that contains two configuration files: \"models.json\" and \"formats.json\" and the actual files for local models. </p> <p>The Models factory class is a more flexible way to create models, for example:</p> <pre><code>Models.setup(\"../../models\")\n\nmodel = Models.create(\"openai:gpt-4\")\n</code></pre> <p>The first line calls Models.setup() to initialize the factory with the folder where model files and configs (\"models.json\" and \"formats.json\") are located.</p> <p>The second line calls Models.create() to create a model from the name \"openai:gpt-4\". In this case we created a remote model, but we could as well create a local model based in a GGUF file.</p> <p>The names should be in the format \"provider:model_name\" and Sibila currently supports two providers:</p> Provider Type Creates object of type llamacpp Local GGUF model LlamaCppModel openai Remote model OpenAIModel <p>The name part, after the \"provider:\" must either be:</p> <ul> <li>A remote model name, like \"gpt-4\": \"openai:gpt-4\"</li> <li>A local model name, like \"openchat\": \"llamacpp:openchat\"</li> <li>The actual filename of a model in the \"models\" folder: \"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\" - this is the form we use in the examples, but of course using \"openchat\" instead of the filename would be better...</li> </ul> <p>Although you can use filenames as model names, it's generally a better idea, for continued use, to create an entry in the \"models.json\" file  - this allows future model replacement to be much easier.</p> <p>See Managing models to learn how to register these model names.</p>"},{"location":"models/models_json/","title":"Managing models","text":"<p>Model names are stored in a file named \"models.json\", in your \"models\" folder. Models registered in this file can then be used when calling Models.create() to create an instance of the model.</p> <p>Registering a name is not strictly needed, as you can create models from their filenames or remote model names, for example in most examples you'll find models created with:</p> <pre><code>model = Models.create(\"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\")\n</code></pre> <p>However, it's a good idea to register a name, specially if you'll be using a model for some time, or there's the possibility you'll need to replace it later. If you register a name, only that will later need to be changed.</p> <p>There are two ways of registering names: by using the sibila CLI tool or by directly editing the \"models.json\" file.</p>"},{"location":"models/models_json/#use-the-sibila-models-cli-tool","title":"Use the \"sibila models\" CLI tool","text":"<p>To register a model with the Models factory you can use the \"sibila models\" tool. Run in the \"models\" folder:</p> <pre><code>&gt; sibila models -s \"llamacpp:openchat openchat-3.5-1210.Q4_K_M.gguf\" openchat\n\nUsing models directory '.'\nSet model 'llamacpp:openchat' with name='openchat-3.5-1210.Q4_K_M.gguf', \nformat='formatx' at './models.json'.\n</code></pre> <p>First argument after -s is the new entry name (including the \"llamacpp:\" provider), then the filename, then the chat template format, if needed.</p> <p>This will create an \"openchat\" entry in \"models.json\", exactly like the manually created below.</p>"},{"location":"models/models_json/#manually-edit-modelsjson","title":"Manually edit \"models.json\"","text":"<p>In alternative, you can manually register a model name by editing the \"models.json\" file located in you \"models\" folder.</p> <p>A \"models.json\" file:</p> <pre><code>{\n    # \"llamacpp\" is a provider, you can then create models with names \n    # like \"provider:model_name\", for ex: \"llamacpp:openchat\"\n    \"llamacpp\": { \n\n        \"_default\": { # place here default args for all llamacpp: models.\n            \"genconf\": {\"temperature\": 0.0}\n            # each model entry below can then override as needed\n        },\n\n        \"openchat\": { # a model definition\n            \"name\": \"openchat-3.5-1210.Q4_K_M.gguf\",\n            \"format\": \"openchat\" # chat template format used by this model\n        },\n\n        \"phi2\": {\n            \"name\": \"phi-2.Q4_K_M.gguf\", # model filename\n            \"format\": \"phi2\",\n            \"genconf\": {\"temperature\": 2.0} # a hot-headed model\n        },\n\n        \"oc\": \"openchat\" \n        # this is a link: \"oc\" forwards to the \"openchat\" entry\n    },\n\n    # The \"openai\" provider. A model can be created with name: \"openai:gpt-4\"\n    \"openai\": { \n\n        \"_default\": {}, # default settings for all OpenAI models\n\n        \"gpt-3.5\": {\n            \"name\": \"gpt-3.5-turbo-1106\" # OpenAI's model name\n        },\n\n        \"gpt-4\": {\n            \"name\": \"gpt-4-1106-preview\"\n        },\n    },\n\n    # \"alias\" entry is not a provider but a way to have simpler alias names.\n    # For example you can use \"alias:develop\" or even simpler, just \"develop\" to create the model:\n    \"alias\": { \n        \"develop\": \"llamacpp:openchat\",\n        \"production\": \"openai:gpt-3.5\"\n    }\n}\n</code></pre> <p>Looking at the above structure, we have two top entries for providers \"llamacpp\" and \"openai\", and also an \"alias\" entry.</p> <p>Inside each provider entry, we have a \"_defaults\" key, which can store a base GenConf or other arguments passed during model creation. The default values defined in \"_default\" entries can later be overridden by any keys of the same name specified in each model definition. You can see this in the \"phi2\" entry, which overrides the genconf entry given in the above \"_default\", setting temperature to 2.0.  Keys are merged element-wise from any specified in the \"_defaults\" entry for the provider: keys with the same name are overridden, all other keys are inherited.</p> <p>In the above \"model.json\" example, let's look at the \"openchat\" model entry:</p> <pre><code>\"openchat\": { # a model definition\n    \"name\": \"openchat-3.5-1210.Q4_K_M.gguf\",\n    \"format\": \"openchat\" # chat template format used by this model\n},\n</code></pre> <p>The \"openchat\" key name is the name you'll use to create the model as \"llamacpp:openchat\":</p> <pre><code># initialize Models to this folder\nModels.setup(\"../../models\")\n\nmodel = Models.create(\"llamacpp:openchat\")\n</code></pre> <p>You can have the following keys in a model entry:</p> Key name The filename to use when loading a model (or remote model name) format Identifies the chat template format that it should use, from the \"formats.json\" file. Some local models include the chat template format in their metadata, so this key is optional. genconf Default GenConf (generation config settings) used to create the model, which will default to use them in each generation. These config settings are merged element-wise from any specified in the \"_defaults\" entry for the provider. other Any other keys will be passed during model creation as its arguments. You can learn which arguments are possible in the API reference for LlamaCppModel or OpenAIModel. For example you can pass \"ctx_len\": 2048 to define the context length to use. As genconf, these keys are merged element-wise from any specified in the \"_defaults\" entry for the provider. <p>The \"alias\" entry is a handy way to keep names that point to actual model entries (independent of provider). Note the two alias entries \"develop\" and \"production\" in the above \"models.json\" - you could then create the production model by doing:</p> <p><pre><code># initialize Models to this folder\nModels.setup(\"../../models\")\n\nmodel = Models.create(\"production\")\n</code></pre> Alias entries can be used as \"alias:production\" or without the \"alias:\" provider, just as \"production\" as in the example above.</p> <p>For an example of a JSON file with many models defined, see the \"models/models.json\" file.</p>"},{"location":"models/openai/","title":"OpenAI","text":"<p>Sibila can use OpenAI remote models, for which you'll need a paid OpenAI account and its API key. Although you can pass this key when you create the model object, it's more secure to define an env variable with this information:</p> Linux and MacWindows <pre><code>export OPENAI_API_KEY=\"...\"\n</code></pre> <pre><code>setx OPENAI_API_KEY \"...\"\n</code></pre> <p>Another possibility is to store your OpenAI key in .env files, which has many advantages: see the dotenv-python package.</p>"},{"location":"models/openai/#creating-models","title":"Creating models","text":"<p>OpenAI models can be used by Sibila through the OpenAIModel class. </p> <p>Example</p> <pre><code>from sibila import OpenAIModel\n\nmodel = OpenAIModel(\"gpt-3.5-turbo-0125\")\n\nmodel(\"I think that I shall never see.\")\n</code></pre> <p>Result</p> <pre><code>'A poem as lovely as a tree.'\n</code></pre> <p>You can also create an OpenAI model in the Models factory by using the \"openai:\" provider prefix like this:</p> <pre><code>from sibila import Models\n\nmodel = Models.create(\"openai:gpt-3.5-turbo-0125\")\n</code></pre>"},{"location":"models/openai/#model-list","title":"Model list","text":"<p>The available OpenAI models are listed here. You can also fetch a list of known model names by calling OpenAIModel.known_models():</p> <p>Example</p> <pre><code>OpenAIModel.known_models()\n</code></pre> <p>Result</p> <pre><code>['babbage-002',\n 'dall-e-2',\n 'dall-e-3',\n 'davinci-002',\n 'gpt-3.5-turbo',\n 'gpt-3.5-turbo-0125',\n 'gpt-3.5-turbo-0301',\n 'gpt-3.5-turbo-0613',\n 'gpt-3.5-turbo-1106',\n 'gpt-3.5-turbo-16k',\n 'gpt-3.5-turbo-16k-0613',\n 'gpt-3.5-turbo-instruct',\n 'gpt-3.5-turbo-instruct-0914',\n 'gpt-4',\n 'gpt-4-0125-preview',\n 'gpt-4-0613',\n 'gpt-4-1106-preview',\n 'gpt-4-1106-vision-preview',\n 'gpt-4-turbo-preview',\n 'gpt-4-vision-preview',\n 'text-embedding-3-large',\n 'text-embedding-3-small',\n 'text-embedding-ada-002',\n 'tts-1',\n 'tts-1-1106',\n 'tts-1-hd',\n 'tts-1-hd-1106',\n 'whisper-1']\n</code></pre> <p>Not all of these models are for text inference, but the names that start with \"gpt\" are (excluding the \"vision\" models), and you can use those model names to create an OpenAI model.</p>"},{"location":"models/openai/#json-schema-models","title":"JSON Schema models","text":"<p>At the time of writing, not all OpenAI inference models support JSON Schema generation via the Tools functionality, which is required for structured data extraction. The following models (and later versions) allow JSON extraction:</p> <ul> <li>gpt-3.5-turbo-1106 and later</li> <li>gpt-4-1106-preview, gpt-4-turbo-preview and later</li> </ul>"},{"location":"models/openai/#using-for-other-providers","title":"Using for other providers","text":"<p>You can also use the OpenAIModel class to access any provider that uses the OpenAI API by setting the base_url and api_key arguments. For example to use the Together.ai service with the OpenAIModel class:</p> <pre><code>model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n\nclient = OpenAIModel(\n    model_name,\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n</code></pre> <p>This is just an example, as Together.ai has a dedicated Sibila class, but you can access any other OpenAI-compatible servers with the OpenAIModel class.</p>"},{"location":"models/remote_model/","title":"Providers","text":"<p>Sibila can use remote models from these providers, see each section for details:</p> Provider Models factory prefix Examples OpenAI openai: GPT-4, GPT-3.5 models Anthropic anthropic: Claude-3, Claude-2 models Mistral AI mistral: Mixtral, Mistral-large/medium/small Together.ai together: Many open source hosted models Fireworks AI fireworks: Large and small open source models"},{"location":"models/setup_format/","title":"Chat template format","text":""},{"location":"models/setup_format/#what-are-chat-templates","title":"What are chat templates?","text":"<p>Because these models were fine-tuned for chat or instruct interaction, they use a chat template, which is a Jinja template that converts a list of messages into a text prompt. This template must follow the original format that the model was trained on - this is very important or you won't get good results.</p> <p>Chat template definitions are Jinja templates like the following one, which is in ChatML format:</p> <pre><code>{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n</code></pre> <p>When ran over a list of messages with system, user and model messages, the template produces text like the following: <pre><code>&lt;|im_start|&gt;system\nYou speak like a pirate.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello there?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nAhoy there matey! How can I assist ye today on this here ship o' mine?&lt;|im_end|&gt;\n</code></pre></p> <p>Only by using the specific chat template for the model, can we get back the best results. </p> <p>Sibila tries to automatically detect which template to use with a model, either from the model name or from embedded metadata, if available. </p>"},{"location":"models/setup_format/#does-the-model-have-a-built-in-chat-template-format","title":"Does the model have a built-in chat template format?","text":"<p>Some GGUF models include the chat template in their metadata, unfortunately this is not standard.</p> <p>You can quickly check if the model has a chat template by running the sibila CLI in the same folder as the model file:</p> <pre><code>&gt; sibila models -t \"llamacpp:openchat-3.5-1210.Q4_K_M.gguf\"\n\nUsing models directory '.'\nTesting model 'llamacpp:openchat-3.5-1210.Q4_K_M.gguf'...\nModel 'llamacpp:openchat-3.5-1210.Q4_K_M.gguf' was properly created and should run fine.\n</code></pre> <p>In this case the chat template format is included with the model and nothing else is needed.</p> <p>Another way to test this is to try creating the model in python. If no exception is raised, the model GGUF file contains the template definition and should work fine.</p> <p>Example of model creation error</p> <pre><code>from sibila import LlamaCppModel\n\nmodel = LlamaCppModel(\"peculiar-model-7b.gguf\")\n</code></pre> <p>Error</p> <pre><code>...\n\nValueError: Could not find a suitable format (chat template) for this model.\nWithout a format, fine-tuned models cannot function properly.\nSee the docs on how you can fix this: pass the template in the format arg or \ncreate a 'formats.json' file.\n</code></pre> <p>But if you get an error such as above, you'll need to provide a chat template. It's quite easy - let's see how to do it.</p>"},{"location":"models/setup_format/#find-the-chat-template-format","title":"Find the chat template format","text":"<p>So, how to find the chat template for a new model that you intend to use? </p> <p>This is normally listed in the model's page: search in that page for \"template\" and copy the listed Jinja template text.</p> <p>If the template isn't directly listed in the model's page, you can look for a file named \"tokenizer_config.json\" in the main model files. This file should include an entry named \"chat_template\" which is what we want.</p> <p>Example of a tokenizer_config.json file</p> <p>For example, in OpenChat's file \"tokenizer_config.json\":</p> <p>https://huggingface.co/openchat/openchat-3.5-1210/blob/main/tokenizer_config.json</p> <p>You'll find this line with the template:</p> <pre><code>{\n    \"...\": \"...\",\n\n    \"chat_template\": \"{{ bos_token }}...{% endif %}\",\n\n    \"...\": \"...\"\n}\n</code></pre> <p>The value in the \"chat_template\" key is the Jinja template that we're looking for.</p> <p>Another alternative is to search online for the name of the model and \"chat template\".</p> <p>Either way, once you know the template used by the model, you can set and use it.</p>"},{"location":"models/setup_format/#option-1-pass-the-chat-template-format-when-creating-the-model","title":"Option 1: Pass the chat template format when creating the model","text":"<p>Once you know the chat template definition you can create the model and pass it in the format argument. Let's assume you have a model file named \"peculiar-model-7b.gguf\":</p> <pre><code>chat_template = \"{{ bos_token }}...{% endif %}\"\n\nmodel = LlamaCppModel(\"peculiar-model-7b.gguf\",\n                      format=chat_template)\n</code></pre> <p>And the model should now work without problems.</p>"},{"location":"models/setup_format/#option-2-add-the-chat-template-to-the-models-factory","title":"Option 2: Add the chat template to the Models factory","text":"<p>If you plan to use the model many times, a more convenient solution is to create an entry in the \"formats.json\" file so that all further models with this name will use the template.</p>"},{"location":"models/setup_format/#with-sibila-formats-cli-tool","title":"With \"sibila formats\" CLI tool","text":"<p>Run the sibila CLI tool in the \"models\" folder:</p> <pre><code>&gt; sibila formats -s peculiar peculiar-model \"{{ bos_token }}...{% endif %}\"\n\nUsing models directory '.'\nSet format 'peculiar' with match='peculiar-model', template='{{ bos_token }}...'\n</code></pre> <p>First argument after -s is the format entry name, second the match regular expression (to identify the model filename) and finally the template. Help is available with \"sibila formats --help\".</p>"},{"location":"models/setup_format/#manually-edit-formatsjson","title":"Manually edit \"formats.json\"","text":"<p>In alternative to using the sibila CLI tool, you can add the chat template format by creating an entry in a \"formats.json\" file, in the same folder as the model, with these fields:</p> <pre><code>{\n    \"peculiar\": {\n        \"match\": \"peculiar-model\",\n        \"template\": \"{{ bos_token }}...{% endif %}\"\n    }\n}\n</code></pre> <p>The \"match\" field is regular expression that will be used to match the model name or filename. Field \"template\" is the chat template in Jinja format.</p> <p>After configuring the template as we've seen above, all you need to do is to create a LlamaCppModel object and pass the model file path.</p> <pre><code>model = LlamaCppModel(\"peculiar-model-7b.gguf\")\n</code></pre> <p>Note that we're not passing the format argument anymore when creating the model. The \"match\" regular expression we defined above will recognize the model from the filename and use the given chat template format.</p> <p>Base format definitions</p> <p>Sibila includes by default the definitions of several well-known chat template formats. These definitions are available in \"sibila/base_formats.json\", and are automatically loaded when Models factory is created.</p> <p>You can add any chat template formats into your own \"formats.json\" files, but please never change the \"sibila/base_formats.json\" file, to avoid potential errors.</p>"},{"location":"models/sibila_cli/","title":"Sibila CLI tool","text":"<p>The Sibila Command-Line Interface tool simplifies managing the Models factory and is useful to download models from Hugging Face model hub.</p> <p>The Models factory is based in a \"models\" folder that contains two configuration files: \"models.json\" and \"formats.json\" and the actual files for local models.</p> <p>The CLI tool is divided in three areas or actions:</p> Action models Manage models in \"model.json\" files formats Manage formats in \"model.json\" files hub Search and download models from Hugging Face model hub <p>In all commands you should pass the option \"-m models_folder\" with the path to the \"models\" folder. Or in alternative run the commands inside the \"models\" folder.</p> <p>The following argument names are used below (other unlisted names should be descriptive enough):</p> Name res_name Model entry name in the form \"provider:name\", for example \"llamacpp:openchat\". format_name Name of a format entry in \"formats.json\", for example \"chatml\". query Case-insensitive query that will be matched by a substring search. <p>Usage help is available by running \"sibila --help\" for general help, or \"sibila action --help\", where action is one of \"models\", \"formats\" or \"hub\".</p>"},{"location":"models/sibila_cli/#sibila-models","title":"Sibila models","text":"<p>To register a model entry pointing to a model name or filename, and optional format_name is a format name:</p> <pre><code>sibila models -s res_name model_name_or_filename [format_name]\n</code></pre> <p>To set the format_name for an existing model entry:</p> <pre><code>sibila models -f res_name format_name\n</code></pre> <p>To test if a model can run (for example to check if it has the chat template format defined):</p> <pre><code>sibila models -t res_name\n</code></pre> <p>List all models with optional case-insensitive substring query:</p> <pre><code>sibila models -l [query]\n</code></pre> <p>Delete a model entry in:</p> <pre><code>sibila models -d res_name\n</code></pre>"},{"location":"models/sibila_cli/#sibila-formats","title":"Sibila formats","text":"<p>Check if a model filename has any format defined in the Models factory:</p> <pre><code>sibila formats -q filename\n</code></pre> <p>To register a chat template format, where template is the Jinja chat template and optional match is a regexp that matches model filename:</p> <pre><code>sibila formats -s format_name template [match_regex]\n</code></pre> <p>List all formats with optional case-insensitive substring query:</p> <pre><code>sibila models -l [query]\n</code></pre> <p>Delete a format entry:</p> <pre><code>sibila formats -d format_name\n</code></pre>"},{"location":"models/sibila_cli/#sibila-hub","title":"Sibila hub","text":"<p>List models in the Hugging Face model hub that match the given queries. Argument query can be a list of strings to match, separated by a space character.</p> <p>Arg Filename is case-insensitive for substring matching.</p> <p>Arg exact_author is an exact and case-sensitive author name from Hugging Face model hub.</p> <pre><code>sibila hub -l query [-f filename] [-a exact_author]\n</code></pre> <p>To download a model, where model_id is a string like \"TheBloke/openchat-3.5-1210-GGUF\". Args filename and author_name same as above:</p> <pre><code>sibila hub -d model_id -f filename -a exact_author -s set name\n</code></pre>"},{"location":"models/together/","title":"Together.ai","text":"<p>With Sibila you can use the models hosted by Together.ai, for which you'll need an API key (which is initially free). As in other providers, although you can pass this key when you create the model object, it's more secure to define an env variable with this information:</p> Linux and MacWindows <pre><code>export TOGETHER_API_KEY=\"...\"\n</code></pre> <pre><code>setx TOGETHER_API_KEY \"...\"\n</code></pre> <p>Another possibility is to store your API key in .env files, which has many advantages: see the dotenv-python package.</p>"},{"location":"models/together/#creating-models","title":"Creating models","text":"<p>Models served by Together.ai can be used by Sibila through the TogetherModel class. </p> <p>Example</p> <pre><code>from sibila import TogetherModel\n\nmodel = TogetherModel(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n\nmodel(\"I think that I shall never see.\")\n</code></pre> <p>Result</p> <pre><code>A poem lovely as a tree. These are the beginning lines of a famous poem called \"Trees\" written by Joyce Kilmer. The full poem goes as follows:\n\nI think that I shall never see\nA poem lovely as a tree.\n\nA tree whose hungry mouth is prest\nAgainst the earth\u2019s sweet flowing breast;\n\nA tree that looks at God all day,\nAnd lifts her leafy arms to pray;\n\nA tree that may in Summer wear\nA nest of robins in her hair;\n\nUpon whose bosom snow has lain;\nWho intimately lives with rain.\n\nPoems are made by fools like me,\nBut only God can make a tree.\n</code></pre> <p>You can also create a Together.ai model in the Models factory by using the \"together:\" provider prefix:</p> <pre><code>from sibila import Models\n\nmodel = Models.create(\"together:mistralai/Mixtral-8x7B-Instruct-v0.1\")\n</code></pre>"},{"location":"models/together/#model-list","title":"Model list","text":"<p>The available Together.ai text inference models models are listed here.</p> <p>Unfortunately Together.ai doesn't provide an API to list the models, so TogetherModel.known_models() will return None.</p>"},{"location":"models/together/#json-schema-models","title":"JSON Schema models","text":"<p>At the time of writing, only the following Together.ai models support JSON Schema generation, which is required for structured data extraction:</p> <ul> <li>mistralai/Mixtral-8x7B-Instruct-v0.1</li> <li>mistralai/Mistral-7B-Instruct-v0.1</li> <li>togethercomputer/CodeLlama-34b-Instruct</li> </ul> <p>You can still use any of the other models for plain text or schema-free JSON generation, for example with the Model.call() or Model.json() methods.</p>"}]}