
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../api-reference/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.6">
    
    
      
        <title>Getting Started - Sibila</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#getting-started" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Sibila" class="md-header__button md-logo" aria-label="Sibila" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sibila
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Getting Started
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Sibila" class="md-nav__button md-logo" aria-label="Sibila" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sibila
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Getting Started
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Getting Started
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-open-ai-models" class="md-nav__link">
    <span class="md-ellipsis">
      Using OPEN AI models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-local-models-in-llamacpp" class="md-nav__link">
    <span class="md-ellipsis">
      Using local models in llama.cpp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#out-of-memory-running-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Out of memory running local models
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-reference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-open-ai-models" class="md-nav__link">
    <span class="md-ellipsis">
      Using OPEN AI models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-local-models-in-llamacpp" class="md-nav__link">
    <span class="md-ellipsis">
      Using local models in llama.cpp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#out-of-memory-running-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Out of memory running local models
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="getting-started">Getting Started</h1>
<h2 id="installation">Installation</h2>
<p>Sibilia requires Python 3.9+ and uses the llama-cpp-python package for local models and OpenAI's API to access remote models like GPT-4.</p>
<p>You can run it in a plain CPU, CUDA GPU or other accelerator supported by llama.cpp.</p>
<p>For accelerated inference with local models, to take advantage of CUDA, Metal, etc, make sure you install llamacpp-python with the right settings - <a href="https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#installation">see more info here</a>.</p>
<p>To use Sibila, download the repository and from the base directory (which has a setup.py script) do:</p>
<pre><code>pip install -e .
</code></pre>
<p>You should now be able to use Sibila to get structured information from local or remote models.</p>
<h2 id="using-open-ai-models">Using OPEN AI models</h2>
<p>To use an OpenAI remote model, you'll need a paid OpenAI account and its API key. You can explicitely pass this key when creating an OpenAIModel object but this is not a good security practice. A better way is to define an environment variable which the OpenAI API will use when needed.</p>
<p>In Linux/Mac you can define this key by running:</p>
<pre><code>export OPENAI_API_KEY=&quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
<p>And in Windows command prompt:</p>
<pre><code>setx OPENAI_API_KEY &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
<p>Having set this variable with your OpenAI API key, you can run an <a href="https://github.com/sibila/tree/master/examples/hellomodel/hello_openai.py">"Hello Model" example</a> :</p>
<pre><code class="language-python">import os
from sibila import (OpenAIModel, GenConf)

# make sure you set an environment variable named OPENAI_API_KEY with your API key.
model = OpenAIModel(&quot;gpt-3.5&quot;,
                    genconf=GenConf(temperature=1))

sys_text = &quot;You are a helpful model that speaks like a pirate.&quot;
in_text = &quot;Hello there?&quot;

print(in_text)

text = model.query_gen(sys_text, in_text)

print(text)
</code></pre>
<h2 id="using-local-models-in-llamacpp">Using local models in llama.cpp</h2>
<p>Sibila can use llama.cpp (via the llamacpp-python package) to load models from local GGUF format files. Since model files are quite big, they are usually quantized so that each parameter occupies a little more than 4 bits or half a byte. </p>
<p>This means that a 7 billion parameter model can have a file size of only 4.4Gb (and about 50% more in memory - 6.8Gb), which makes it accessible to be ran in common GPUs or even in common RAM memory (albeit slower).</p>
<p>A great place to find quantized model is in HuggingFace's model hub, particularly in TheBloke's (Tom Jobbins) account:</p>
<p><a href="https://huggingface.co/TheBloke">https://huggingface.co/TheBloke</a></p>
<p>Sibila can use models that were fine-tuned for chat or instruct purposes. These models work in user/assistant turns or messages and use a chat template to properly compose those messages to the format that the model was fine-tuned to. For example the Llama2 model was released in two editions: a simple Llama2 text completion model and a Llama2-instruct model that was fine tuned for user-assistant turns. For using Sibila you should always select chat or instruct versions of the model.</p>
<p>A good model that we can use for our examples is the 4 bit quantization of the OpenChat-3.5 model, which itself is a fine-tuning of Mistral-7b:</p>
<p><a href="https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF">https://huggingface.co/TheBloke/openchat-3.5-1210-GGUF</a></p>
<p>You can download the GGUF file (in this and other quantized models by TheBloke) by scrolling down to the "Provided files" section and clicking one of the links. Usually the files ending in "Q4_K_M" are reasonable 4-bit quantizations.</p>
<p>In this case you'll download file openchat-3.5-1210.Q4_K_M.gguf - save it into the models folder inside Sibila.</p>
<p>You can now run <a href="https://github.com/sibila/tree/master/examples/hellomodel/hello_llamacpp.py">hello-llammacpp.py</a>:</p>
<pre><code class="language-python">import os
from sibila import (LlamaCppModel, GenConf)

model_path = &quot;../../models/openchat-3.5-1210.Q4_K_M.gguf&quot;

model = LlamaCppModel(model_path,
                      genconf=GenConf(temperature=1))

sys_text = &quot;You are a helpful model that speaks like a pirate.&quot;
in_text = &quot;Hello there?&quot;

print(in_text)

text = model.query_gen(sys_text, in_text)

print(text)
</code></pre>
<p>After running the above and/or OpenAI's script you'll receive the model's answer to your "Hello there?" - in arrr-style:</p>
<pre><code>Hello there?
Ahoy, me hearty! How be it goin'? Me name's Captain Chatbot, and I be here to assist thee with whatever ye need! So, what can me crew and I do fer yer today? Arrr!
</code></pre>
<h2 id="out-of-memory-running-local-models">Out of memory running local models</h2>
<p>A 7B model like OpenChat-3.5, when quantized to 4 bits will occupy about 6.8 Gb of memory, in either GPU's VRAM or common RAM. If you try to run a second model, you might get an out of memory error, or llama.cpp may crash.</p>
<p>This is less of a problem when running scripts from the command line, but in environments like Jupyter where you can have multiple open notebooks, you may get python kernel errors like:</p>
<pre><code>Kernel Restarting
The kernel for sibila/examples/name.ipynb appears to have died. It will restart automatically.
</code></pre>
<p>If you get an error like this in JupyterLab, open the Kernel menu and select "Shut Down All Kernels...". This will get rid of any out-of-memory stuck models.</p>
<p>A good practice is to delete any model after you no longer need it or right before loading a new one. A simple "del model" works fine, or you can add these two lines before creating a model:</p>
<pre><code class="language-python">try: del model
except: pass

model = LlamaCppModel(...)
</code></pre>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.e1c3ead8.min.js"></script>
      
    
  </body>
</html>